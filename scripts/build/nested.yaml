module:
  aurora:
    count: '.deploy_aurora ? 1 : 0'
    vpc_name: .vpc_name
    cluster_identifier: .cluster_identifier
    cluster_instance_identifier: .cluster_instance_identifier
    cluster_instance_class: .cluster_instance_class
    cluster_engine: .cluster_engine
    cluster_engine_version: .cluster_engine_version
    master_username: .master_username
    storage_encrypted: .storage_encrypted
    apply_immediate: .apply_immediate
    engine_mode: .engine_mode
    serverlessv2_scaling_min_capacity: .serverlessv2_scaling_min_capacity
    serverlessv2_scaling_max_capacity: .serverlessv2_scaling_max_capacity
    skip_final_snapshot: .skip_final_snapshot
    final_snapshot_identifier: .final_snapshot_identifier
    backup_retention_period: .backup_retention_period
    preferred_backup_window: .preferred_backup_window
    password_length: .password_length
    secrets_manager_enabled: .secrets_manager_enabled
    db_kms_key_id: .db_kms_key_id
    depends_on: module.cdis_vpc.vpc_id module.cdis_vpc.vpc_peering_id
    data:
      aws_caller_identity:
        current: 
      aws_vpcs:
        vpcs:
          tags:
            Name: .vpc_name
      aws_security_group:
        private:
          vpc_id: data.aws_vpc.the_vpc.id
          name: local
      aws_vpc:
        the_vpc:
          id: data.aws_vpcs.vpcs.ids[0]
    locals:
      aurora-creds-template: "\n    \"aurora\": \n        \"db_host\": \"aws_rds_cluster.postgresql.endpoint\"\
        ,\n        \"db_username\": \"aws_rds_cluster.postgresql.master_username\"\
        ,\n        \"db_password\": \"aws_rds_cluster.postgresql.master_password\"\
        ,\n    \n"
      pg_family_version: replace(var.cluster_engine_version, "/\\.[0-9]/", "")
    output:
      aurora_cluster_writer_endpoint:
        description: Aurora cluster writer instance endpoint
        value: aws_rds_cluster.postgresql.endpoint
      aurora_cluster_reader_endpoint:
        description: Aurora cluster reader endpoint
        value: aws_rds_cluster.postgresql.reader_endpoint
      aurora_cluster_master_username:
        description: Aurora cluster master username
        value: aws_rds_cluster.postgresql.master_username
      aurora_cluster_master_password:
        description: Aurora cluster master user's password
        value: aws_rds_cluster.postgresql.master_password
        sensitive: true
    resource:
      random_password:
        password:
          length: .password_length
          special: false
      aws_rds_cluster:
        postgresql:
          cluster_identifier: .vpc_name-.cluster_identifier
          engine: .cluster_engine
          engine_version: .cluster_engine_version
          db_subnet_group_name: .vpc_name_private_group
          vpc_security_group_ids: data.aws_security_group.private.id
          master_username: .master_username
          master_password: random_password.password.result
          storage_encrypted: .storage_encrypted
          apply_immediately: .apply_immediate
          engine_mode: .engine_mode
          skip_final_snapshot: .skip_final_snapshot
          final_snapshot_identifier: .vpc_name-.final_snapshot_identifier
          backup_retention_period: .backup_retention_period
          preferred_backup_window: .preferred_backup_window
          db_cluster_parameter_group_name: aws_rds_cluster_parameter_group.aurora_cdis_pg.name
          kms_key_id: .db_kms_key_id
          serverlessv2_scaling_configuration:
            max_capacity: .serverlessv2_scaling_max_capacity
            min_capacity: .serverlessv2_scaling_min_capacity
          lifecycle:
            ignore_changes: kms_key_id engine_version
      aws_rds_cluster_instance:
        postgresql:
          db_subnet_group_name: aws_rds_cluster.postgresql.db_subnet_group_name
          identifier: .vpc_name-.cluster_instance_identifier
          cluster_identifier: aws_rds_cluster.postgresql.id
          instance_class: .cluster_instance_class
          engine: aws_rds_cluster.postgresql.engine
          engine_version: aws_rds_cluster.postgresql.engine_version
          lifecycle:
            ignore_changes: engine_version
      local_sensitive_file:
        aurora_creds:
          count: '.secrets_manager_enabled ? 0 : 1'
          content: local.aurora-creds-template
          filename: path.cwd/.vpc_name_output/aurora-creds.json
      aws_rds_cluster_parameter_group:
        aurora_cdis_pg:
          name: .vpc_name-aurora-cdis-pg
          family: aurora-postgresqllocal.pg_family_version
          parameter:
            name:
            - cpu_index_tuple_cost
            - cpu_tuple_cost
            - log_duration
            - log_min_duration_statement
            - random_page_cost
            - password_encryption
            value:
            - '0.000005'
            - '0.7'
            - '1'
            - '0'
            - '0.7'
            - scram-sha-256
          lifecycle:
            ignore_changes: all
    module:
      secrets_manager:
        count: '.secrets_manager_enabled ? 1 : 0'
        vpc_name: .vpc_name
        secret: aws_rds_cluster.postgresql.master_password
        secret_name: aurora-master-password
        data:
          aws_caller_identity:
            current: 
          aws_vpcs:
            vpcs:
              tags:
                Name: .vpc_name
          aws_security_group:
            private:
              vpc_id: data.aws_vpc.the_vpc.id
              name: local
          aws_vpc:
            the_vpc:
              id: data.aws_vpcs.vpcs.ids[0]
        output:
          secret-arn:
            value: aws_secretsmanager_secret.secret.arn
        resource:
          aws_secretsmanager_secret:
            secret:
              name: .vpc_name_.secret_name
          aws_secretsmanager_secret_version:
            secret:
              secret_id: aws_secretsmanager_secret.secret.id
              secret_string: .secret
        variable:
          vpc_name: 
          secret: 
          secret_name: 
    variable:
      vpc_name: 
      secrets_manager_enabled:
        default: false
      cluster_identifier:
        description: Cluster Identifier
        type: string
        default: aurora-cluster
      cluster_instance_identifier:
        description: Cluster Instance Identifier
        type: string
        default: aurora-cluster-instance
      cluster_instance_class:
        description: Cluster Instance Class
        type: string
        default: db.serverless
      cluster_engine:
        description: Aurora database engine type
        type: string
        default: aurora-postgresql
      cluster_engine_version:
        description: Aurora database engine version.
        type: string
        default: '13.7'
      master_username:
        description: Master DB username
        type: string
        default: postgres
      storage_encrypted:
        description: Specifies whether storage encryption is enabled
        type: bool
        default: true
      apply_immediate:
        description: Instruct the service to apply the change immediately. This can
          result in a brief downtime as the server reboots. See the AWS Docs on RDS
          Maintenance for more information
        type: bool
        default: true
      engine_mode:
        type: string
        description: use provisioned for Serverless v2 RDS cluster
        default: provisioned
      serverlessv2_scaling_min_capacity:
        type: string
        description: Serverless v2 RDS cluster minimum scaling capacity in ACUs
        default: '0.5'
      serverlessv2_scaling_max_capacity:
        type: string
        description: Serverless v2 RDS cluster maximum scaling capacity in ACUs
        default: '10.0'
      skip_final_snapshot:
        description: Determines whether a final DB snapshot is created before the
          DB cluster is deleted
        type: bool
        default: false
      final_snapshot_identifier:
        type: string
        description: The name of your final DB snapshot when this DB cluster is deleted
        default: aurora-cluster-snapshot-final
      backup_retention_period:
        type: number
        description: The days to retain backups for
        default: 10
      preferred_backup_window:
        description: The daily time range during which automated backups are created
          if automated backups are enabled using the BackupRetentionPeriod parameter
        type: string
        default: 02:00-03:00
      password_length:
        type: number
        description: The length of the password string
        default: 16
      db_kms_key_id:
        default: ''
  aws_waf:
    count: '.deploy_waf ? 1 : 0'
    vpc_name: .vpc_name
    base_rules: .base_rules
    additional_rules: .additional_rules
    depends_on: module.cdis_vpc.vpc_id module.cdis_vpc.vpc_peering_id
    output:
      waf_arn:
        description: WAF arn - annotate the cluster ingress
        value: aws_wafv2_web_acl.waf.arn
    resource:
      aws_wafv2_web_acl:
        waf:
          name: .vpc_name-waf
          description: WAF per environment for tailored security.
          scope: REGIONAL
          default_action:
            allow: 
          dynamic:
            rule:
              for_each: concat(var.base_rules, var.additional_rules)
              content:
                name: AWS-rule.value.managed_rule_group_name
                priority: rule.value.priority
                override_action:
                  none: 
                statement:
                  managed_rule_group_statement:
                    vendor_name: AWS
                    name: rule.value.managed_rule_group_name
                    dynamic:
                      rule_action_override:
                        for_each: 'length(rule.value.override_to_count) > 0 ? rule.value.override_to_count
                          : []'
                        content:
                          action_to_use:
                            count: 
                          name: rule_action_override.value
                visibility_config:
                  sampled_requests_enabled: true
                  cloudwatch_metrics_enabled: true
                  metric_name: AWS-rule.value.managed_rule_group_name
          tags:
            Environment: .vpc_name
          visibility_config:
            cloudwatch_metrics_enabled: false
            metric_name: WebAclMetrics
            sampled_requests_enabled: false
    variable:
      vpc_name: 
      base_rules:
        description: Base AWS Managed Rules
        type: 'list(object("managed_rule_group_name": "string", "priority": "number",
          "override_to_count": "list(string)"))'
        default:
          managed_rule_group_name:
          - AWSManagedRulesAmazonIpReputationList
          - AWSManagedRulesPHPRuleSet
          - AWSManagedRulesWordPressRuleSet
          priority:
          - 0
          - 1
          - 2
          override_to_count:
          - AWSManagedReconnaissanceList
          - PHPHighRiskMethodsVariables_HEADER PHPHighRiskMethodsVariables_QUERYSTRING
            PHPHighRiskMethodsVariables_BODY
          - WordPressExploitableCommands_QUERYSTRING WordPressExploitablePaths_URIPATH
      additional_rules:
        description: Additional AWS Managed Rules
        type: 'list(object("managed_rule_group_name": "string", "priority": "number",
          "override_to_count": "list(string)"))'
        default: []
  cdis_vpc:
    ami_account_id: .ami_account_id
    squid_image_search_criteria: .squid_image_search_criteria
    vpc_cidr_block: .vpc_cidr_block
    secondary_cidr_block: .secondary_cidr_block
    vpc_name: .vpc_name
    ssh_key_name: aws_key_pair.automation_dev.key_name
    peering_cidr: .peering_cidr
    csoc_account_id: .csoc_account_id
    organization_name: .organization_name
    csoc_managed: .csoc_managed
    peering_vpc_id: .peering_vpc_id
    vpc_flow_logs: .vpc_flow_logs
    vpc_flow_traffic: .vpc_flow_traffic
    branch: .branch
    fence-bot_bucket_access_arns: .fence-bot_bucket_access_arns
    deploy_ha_squid: .deploy_ha_squid
    deploy_single_proxy: .deploy_single_proxy
    squid_cluster_desired_capasity: .ha-squid_cluster_desired_capasity
    squid_cluster_min_size: .ha-squid_cluster_min_size
    squid_cluster_max_size: .ha-squid_cluster_max_size
    squid_instance_type: .ha-squid_instance_type
    squid_instance_drive_size: .ha-squid_instance_drive_size
    squid_bootstrap_script: .ha-squid_bootstrap_script
    squid_extra_vars: .ha-squid_extra_vars
    single_squid_instance_type: .single_squid_instance_type
    fips: .fips
    network_expansion: .network_expansion
    activation_id: .activation_id
    customer_id: .customer_id
    slack_webhook: .slack_webhook
    deploy_cloud_trail: .deploy_cloud_trail
    send_logs_to_csoc: .send_logs_to_csoc
    commons_log_retention: .commons_log_retention
    module:
      data-bucket:
        vpc_name: .vpc_name
        cloudwatchlogs_group: aws_cloudwatch_log_group.main_log_group.arn
        environment: .vpc_name
        deploy_cloud_trail: .deploy_cloud_trail
        module:
          cloud-trail:
            count: '.deploy_cloud_trail ? 1 : 0'
            vpc_name: .vpc_name
            environment: .environment
            cloudwatchlogs_group: .cloudwatchlogs_group
            bucket_arn: aws_s3_bucket.data_bucket.arn
            bucket_id: aws_s3_bucket.log_bucket.id
            data:
              aws_iam_policy_document:
                trail_policy:
                  statement:
                    effect: Allow
                    actions: logs:CreateLogStream logs:PutLogEvents
                    resources: .cloudwatchlogs_group:*
            resource:
              aws_cloudtrail:
                logger_trail:
                  name: .vpc_name-data-bucket-trail
                  s3_bucket_name: .bucket_id
                  s3_key_prefix: trail-logs
                  include_global_service_events: false
                  cloud_watch_logs_role_arn: aws_iam_role.cloudtrail_to_cloudwatch_writer.arn
                  cloud_watch_logs_group_arn: .cloudwatchlogs_group:*
                  event_selector:
                    read_write_type: All
                    include_management_events: false
                    data_resource:
                      type: AWS::S3::Object
                      values: .bucket_arn/
                  lifecycle:
                    ignore_changes: all
                  tags:
                    Name: .vpc_name_data-bucket
                    Environment: .environment
                    Purpose: trail_for_.vpc_name_data_bucket
              aws_iam_role:
                cloudtrail_to_cloudwatch_writer:
                  name: .vpc_name_data-bucket_ct_to_cwl_writer
                  path: /
                  assume_role_policy: "\n    \"Version\": \"2012-10-17\",\n    \"\
                    Statement\": [\n        \n            \"Action\": \"sts:AssumeRole\"\
                    ,\n            \"Principal\": \n               \"Service\": \"\
                    cloudtrail.amazonaws.com\"\n            ,\n            \"Effect\"\
                    : \"Allow\",\n            \"Sid\": \"\"\n        \n    ]\n"
              aws_iam_policy:
                trail_writer:
                  name: trail_write_to_cwl_.environment
                  description: Put logs in CWL .environment
                  policy: data.aws_iam_policy_document.trail_policy.json
              aws_iam_role_policy_attachment:
                trail_writer_role:
                  role: aws_iam_role.cloudtrail_to_cloudwatch_writer.name
                  policy_arn: aws_iam_policy.trail_writer.arn
            variable:
              vpc_name: 
              environment: 
              cloudwatchlogs_group: 
              bucket_arn: 
              bucket_id: 
          data-bucket-queue:
            bucket_name: aws_s3_bucket.data_bucket.id
            configure_bucket_notifications: false
            data:
              aws_s3_bucket:
                selected:
                  bucket: .bucket_name
              aws_iam_policy_document:
                sns-topic-policy:
                  policy_id: __default_policy_ID
                  statement:
                    actions: SNS:Subscribe SNS:Receive SNS:Publish SNS:ListSubscriptionsByTopic
                      SNS:GetTopicAttributes
                    condition:
                      test: ArnLike
                      variable: aws:SourceArn
                      values: arn:aws:s3:*:*:.bucket_name
                    effect: Allow
                    principals:
                      type: AWS
                      identifiers: '*'
                    resources: aws_sns_topic.user_updates.arn
                    sid: __default_statement_ID
            output:
              data-bucket_name:
                value: aws_sns_topic.user_updates.arn
              sns-topic-arn:
                value: aws_sns_topic.user_updates.arn
              sqs-url:
                value: aws_sqs_queue.user_updates_queue.id
            resource:
              aws_sns_topic:
                user_updates:
                  name: .bucket_name_sns_topic
              aws_sqs_queue:
                user_updates_queue:
                  name: .bucket_name_data_upload
                  visibility_timeout_seconds: 300
              aws_sns_topic_subscription:
                user_updates_sqs_target:
                  topic_arn: aws_sns_topic.user_updates.arn
                  protocol: sqs
                  endpoint: aws_sqs_queue.user_updates_queue.arn
              aws_s3_bucket_notification:
                bucket_notification:
                  count: '.configure_bucket_notifications ? 1 : 0'
                  bucket: .bucket_name
                  topic:
                    topic_arn: aws_sns_topic.user_updates.arn
                    events: s3:ObjectCreated:Put s3:ObjectCreated:Post s3:ObjectCreated:Copy
                      s3:ObjectCreated:CompleteMultipartUpload
                  lifecycle:
                    ignore_changes: topic
              aws_sqs_queue_policy:
                subscribe_sns:
                  queue_url: aws_sqs_queue.user_updates_queue.id
                  policy: "\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"sqspolicy\"\
                    ,\n  \"Statement\": [\n    \n      \"Sid\": \"100\",\n      \"\
                    Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\"\
                    : \"sqs:SendMessage\",\n      \"Resource\": \"aws_sqs_queue.user_updates_queue.arn\"\
                    ,\n      \"Condition\": \n        \"ArnEquals\": \n        \
                    \  \"aws:SourceArn\": \"aws_sns_topic.user_updates.arn\"\n\
                    \        \n      \n    \n  ]\n"
              aws_sns_topic_policy:
                default:
                  arn: aws_sns_topic.user_updates.arn
                  policy: data.aws_iam_policy_document.sns-topic-policy.json
            variable:
              bucket_name: 
              configure_bucket_notifications:
                default: true
        data:
          aws_iam_policy_document:
          - data_bucket_reader:
              statement:
                actions: s3:Get* s3:List*
                effect: Allow
                resources: aws_s3_bucket.data_bucket.arn aws_s3_bucket.data_bucket.arn/*
          - data_bucket_writer:
              statement:
                actions: s3:PutObject
                effect: Allow
                resources: aws_s3_bucket.data_bucket.arn aws_s3_bucket.data_bucket.arn/*
          - log_bucket_writer:
              statement:
                actions:
                - s3:Get* s3:List*
                - s3:PutObject s3:GetObject s3:DeleteObject
                effect:
                - Allow
                - Allow
                resources:
                - aws_s3_bucket.log_bucket.arn aws_s3_bucket.log_bucket.arn/*
                - aws_s3_bucket.log_bucket.arn/*
        output:
          data-bucket_name:
            value: aws_s3_bucket.data_bucket.id
          log_bucket_name:
            value: aws_s3_bucket.log_bucket.id
        resource:
          aws_iam_role:
            data_bucket:
              name: .vpc_name-data-bucket-access
              path: /
              assume_role_policy: "\n    \"Version\": \"2012-10-17\",\n    \"Statement\"\
                : [\n        \n            \"Action\": \"sts:AssumeRole\",\n    \
                \        \"Principal\": \n               \"Service\": \"ec2.amazonaws.com\"\
                \n            ,\n            \"Effect\": \"Allow\",\n           \
                \ \"Sid\": \"\"\n        \n    ]\n"
          aws_iam_policy:
          - data_bucket_reader:
              name: data_bucket_read_.vpc_name
              description: Data Bucket access for .vpc_name
              policy: data.aws_iam_policy_document.data_bucket_reader.json
          - data_bucket_writer:
              name: data_bucket_write_.vpc_name
              description: Data Bucket access for .vpc_name
              policy: data.aws_iam_policy_document.data_bucket_reader.json
          - log_bucket_writer:
              name: bucket_writer_aws_s3_bucket.log_bucket.id
              description: Read or write aws_s3_bucket.log_bucket.id
              policy: data.aws_iam_policy_document.log_bucket_writer.json
          aws_iam_role_policy_attachment:
          - data_bucket_reader:
              role: aws_iam_role.data_bucket.name
              policy_arn: aws_iam_policy.data_bucket_reader.arn
          - data_bucket_writer:
              role: aws_iam_role.data_bucket.name
              policy_arn: aws_iam_policy.data_bucket_writer.arn
          aws_s3_bucket:
          - data_bucket:
              bucket: .vpc_name-data-bucket
              tags:
                Name: .vpc_name-data-bucket
                Environment: .environment
                Purpose: data bucket
          - log_bucket:
              bucket: .vpc_name-data-bucket-logs
              tags:
                Name: .vpc_name
                Environment: .environment
                Purpose: logs bucket
          aws_s3_bucket_server_side_encryption_configuration:
          - data_bucket:
              bucket: aws_s3_bucket.data_bucket.bucket
              lifecycle:
                ignore_changes: all
              rule:
                apply_server_side_encryption_by_default:
                  sse_algorithm: aws:kms
          - log_bucket:
              bucket: aws_s3_bucket.log_bucket.bucket
              rule:
                apply_server_side_encryption_by_default:
                  sse_algorithm: aws:kms
          aws_s3_bucket_logging:
            data_bucket:
              bucket: aws_s3_bucket.data_bucket.id
              target_bucket: aws_s3_bucket.log_bucket.id
              target_prefix: log/.vpc_name-data-bucket/
          aws_s3_bucket_public_access_block:
          - data_bucket_privacy:
              bucket: aws_s3_bucket.data_bucket.id
              block_public_acls: true
              block_public_policy: true
              ignore_public_acls: true
              restrict_public_buckets: true
          - data_bucket_logs_privacy:
              bucket: aws_s3_bucket.log_bucket.id
              block_public_acls: true
              block_public_policy: true
              ignore_public_acls: true
              restrict_public_buckets: true
          aws_s3_bucket_notification:
            bucket_notification:
              bucket: aws_s3_bucket.data_bucket.id
              topic:
                topic_arn: module.data-bucket-queue.data-bucket_name
                events: s3:ObjectCreated:Put s3:ObjectCreated:Post s3:ObjectCreated:Copy
                  s3:ObjectCreated:CompleteMultipartUpload
          aws_s3_bucket_lifecycle_configuration:
            log_bucket:
              bucket: aws_s3_bucket.log_bucket.bucket
              rule:
                status: Enabled
                id: log
                filter:
                  and:
                    prefix: /
                    tags:
                      rule: log
                      autoclean: 'true'
                expiration:
                  days: 120
          aws_s3_bucket_policy:
            log_bucket_writer_by_ct:
              bucket: aws_s3_bucket.log_bucket.id
              policy: "\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n   \
                \ \n      \"Sid\": \"AWSCloudTrailAclCheck20150319\",\n      \"Effect\"\
                : \"Allow\",\n      \"Principal\": \n         \"Service\": \"cloudtrail.amazonaws.com\"\
                \n      ,\n      \"Action\": \"s3:GetBucketAcl\",\n      \"Resource\"\
                : \"aws_s3_bucket.log_bucket.arn\"\n    ,\n\n    \n      \"Sid\"\
                : \"AWSCloudTrailWrite20150319\",\n     \"Effect\": \"Allow\",\n \
                \     \"Principal\": \n        \"Service\": \"cloudtrail.amazonaws.com\"\
                \n      ,\n      \"Action\": \"s3:PutObject\",\n      \"Resource\"\
                : \"aws_s3_bucket.log_bucket.arn/*\",\n      \"Condition\": \n\
                \         \"StringEquals\": \n         \"s3:x-amz-acl\": \"bucket-owner-full-control\"\
                \n         \n      \n    \n  ]\n"
        variable:
          vpc_name: 
          environment: 
          cloudwatchlogs_group: 
          deploy_cloud_trail:
            default: true
      fence-bot-user:
        vpc_name: .vpc_name
        bucket_name: module.data-bucket.data-bucket_name
        bucket_access_arns: .fence-bot_bucket_access_arns
        data:
          aws_s3_bucket:
            data-bucket:
              bucket: .bucket_name
        output:
          fence-bot_secret:
            value: aws_iam_access_key.fence-bot_user_key.secret
          fence-bot_id:
            value: aws_iam_access_key.fence-bot_user_key.id
        resource:
          aws_iam_user:
            fence-bot:
              name: .vpc_name_fence-bot
          aws_iam_access_key:
            fence-bot_user_key:
              user: aws_iam_user.fence-bot.name
          aws_iam_user_policy:
          - fence-bot_policy:
              name: .vpc_name_fence-bot_policy
              user: aws_iam_user.fence-bot.name
              policy: "\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n   \
                \ \n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:GetObject\"\
                ,\n        \"s3:DeleteObject\"\n      ],\n      \"Effect\": \"Allow\"\
                ,\n      \"Resource\": [\"data.aws_s3_bucket.data-bucket.arn/*\"\
                ]\n    ,\n    \n       \"Action\": [\n         \"s3:List*\",\n \
                \        \"s3:Get*\"\n       ],\n      \"Effect\": \"Allow\",\n  \
                \    \"Resource\": [\"data.aws_s3_bucket.data-bucket.arn/*\", \"\
                data.aws_s3_bucket.data-bucket.arn\"]\n    \n  ]\n"
              lifecycle:
                ignore_changes: policy
          - fence-bot_extra_policy:
              count: length(var.bucket_access_arns)
              name: .vpc_name_fence-bot_policy_count.index
              user: aws_iam_user.fence-bot.name
              policy: "\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n   \
                \ \n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:GetObject\"\
                ,\n        \"s3:DeleteObject\"\n      ],\n      \"Effect\": \"Allow\"\
                ,\n      \"Resource\": [\".bucket_access_arns[count.index]/*\"\
                ]\n    ,\n    \n       \"Action\": [\n         \"s3:List*\",\n \
                \        \"s3:Get*\"\n       ],\n      \"Effect\": \"Allow\",\n  \
                \    \"Resource\": [\".bucket_access_arns[count.index]/*\",\
                \ \".bucket_access_arns[count.index]\"]\n    \n  ]\n"
        variable:
          vpc_name: 
          bucket_name: 
          bucket_access_arns:
            description: When fence bot has to access another bucket that wasn't created
              by the VPC module
            default: []
      squid-auto:
        peering_cidr: .peering_cidr
        secondary_cidr_block: .secondary_cidr_block
        env_vpc_name: .vpc_name
        env_vpc_cidr: aws_vpc.main.cidr_block
        env_vpc_id: aws_vpc.main.id
        env_log_group: aws_cloudwatch_log_group.main_log_group.name
        env_squid_name: squid-auto-.vpc_name
        squid_proxy_subnet: '.network_expansion ? cidrsubnet(var.vpc_cidr_block,
          5, 3) : cidrsubnet(var.vpc_cidr_block, 4, 1)'
        organization_name: .organization_name
        ssh_key_name: .ssh_key_name
        ami_account_id: .ami_account_id
        image_name_search_criteria: .squid_image_search_criteria
        squid_instance_drive_size: .squid_instance_drive_size
        squid_availability_zones: .availability_zones
        main_public_route: aws_route_table.public.id
        route_53_zone_id: aws_route53_zone.main.id
        squid_instance_type: .squid_instance_type
        bootstrap_script: .squid_bootstrap_script
        extra_vars: .squid_extra_vars
        branch: .branch
        cluster_max_size: .squid_cluster_max_size
        cluster_min_size: .squid_cluster_min_size
        cluster_desired_capasity: .squid_cluster_desired_capasity
        network_expansion: .network_expansion
        squid_depends_on: aws_nat_gateway.nat_gw.id
        activation_id: .activation_id
        customer_id: .customer_id
        slack_webhook: .slack_webhook
        fips: .fips
        data:
          aws_caller_identity:
            current: 
          aws_region:
            current: 
          aws_ami:
            public_squid_ami:
              most_recent: true
              filter:
                name:
                - name
                - virtualization-type
                - root-device-type
                values:
                - .image_name_search_criteria
                - hvm
                - ebs
              owners: .ami_account_id
          aws_iam_policy_document:
            squid_policy_document:
              statement:
                actions:
                - ec2:* route53:* autoscaling:* sts:AssumeRole logs:CreateLogGroup
                  logs:CreateLogStream logs:GetLogEvents logs:PutLogEvents logs:DescribeLogGroups
                  logs:DescribeLogStreams logs:PutRetentionPolicy
                - s3:Get* s3:List*
                effect:
                - Allow
                - Allow
                resources:
                - '*'
                - arn:aws:s3:::qualys-agentpackage arn:aws:s3:::qualys-agentpackage/*
        locals:
          cidrs: '.secondary_cidr_block != "" ? [''.env_vpc_cidr'', ''.peering_cidr'',
            ''.secondary_cidr_block''] : [''.env_vpc_cidr'', ''.peering_cidr'']'
          cidrs2: '.secondary_cidr_block != "" ? [''.env_vpc_cidr'', ''.secondary_cidr_block'']
            : [''.env_vpc_cidr'']'
        output:
          squid_auto-name:
            value: aws_autoscaling_group.squid_auto.name
        resource:
          aws_subnet:
            squid_pub0:
              count: length(var.squid_availability_zones)
              vpc_id: .env_vpc_id
              cidr_block: '.network_expansion ? cidrsubnet(var.squid_proxy_subnet,
                2, count.index) : cidrsubnet(var.squid_proxy_subnet, 3, count.index)'
              availability_zone: .squid_availability_zones[count.index]
              tags: 'tomap("Name": ".env_squid_name_pubcount.index", "Organization":
                ".organization_name", "Environment": ".env_squid_name")'
          aws_iam_role:
            squid-auto_role:
              name: .env_squid_name_role
              path: /
              assume_role_policy: "\n    \"Version\": \"2012-10-17\",\n    \"Statement\"\
                : [\n        \n            \"Action\": \"sts:AssumeRole\",\n    \
                \        \"Principal\": \n               \"Service\": \"ec2.amazonaws.com\"\
                \n            ,\n            \"Effect\": \"Allow\",\n           \
                \ \"Sid\": \"\"\n        \n    ]\n"
          aws_iam_role_policy:
            squid_policy:
              name: .env_squid_name_policy
              policy: data.aws_iam_policy_document.squid_policy_document.json
              role: aws_iam_role.squid-auto_role.id
          aws_iam_role_policy_attachment:
            eks-policy-AmazonSSMManagedInstanceCore:
              policy_arn: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
              role: aws_iam_role.squid-auto_role.id
          aws_iam_instance_profile:
            squid-auto_role_profile:
              name: .env_vpc_name_squid-auto_role_profile
              role: aws_iam_role.squid-auto_role.id
          aws_route_table_association:
            squid_auto0:
              count: length(var.squid_availability_zones)
              subnet_id: aws_subnet.squid_pub0.*.id[count.index]
              route_table_id: .main_public_route
          aws_launch_template:
            squid_auto:
              name_prefix: .env_squid_name-lt
              instance_type: .squid_instance_type
              image_id: data.aws_ami.public_squid_ami.id
              key_name: .ssh_key_name
              iam_instance_profile:
                name: aws_iam_instance_profile.squid-auto_role_profile.name
              network_interfaces:
                associate_public_ip_address: true
                security_groups: aws_security_group.squidauto_in.id aws_security_group.squidauto_out.id
              user_data: "sensitive(base64encode(\"MIME-Version: 1.0\nContent-Type:\
                \ multipart/mixed; boundary=\"BOUNDARY\"\n\n--BOUNDARY\nContent-Type:\
                \ text/x-shellscript; charset=\"us-ascii\"\n\n#!/bin/bash\nDISTRO=$(awk\
                \ -F '[=\"]*' '/^NAME/  print $2 ' < /etc/os-release)\nUSER=\"ubuntu\"\
                \nif [[ $DISTRO == \"Amazon Linux\" ]]; then\n  USER=\"ec2-user\"\n\
                \  if [[ $(awk -F '[=\"]*' '/^VERSION_ID/  print $2 ' < /etc/os-release)\
                \ == \"2023\" ]]; then\n    DISTRO=\"al2023\"\n  fi\nfi\n(\n  if [[\
                \ $DISTRO == \"Amazon Linux\" ]]; then\n    sudo yum update -y\n \
                \   sudo yum install git lsof dracut-fips openssl rsync -y\n    sudo\
                \ /sbin/grubby --update-kernel=ALL --args=\"fips=1\"\n    echo \"\
                0 3 * * * root yum update --security -y\" | sudo tee /etc/cron.d/security-updates\n\
                \  elif [[ $DISTRO == \"al2023\" ]]; then\n    sudo dnf update -y\n\
                \    sudo dnf install git rsync lsof docker crypto-policies crypto-policies-scripts\
                \ -y\n    sudo fips-mode-setup --enable\n  fi\n) > /var/log/bootstrapping_script.log\n\
                --BOUNDARY\nContent-Type: text/cloud-config; charset=\"us-ascii\"\n\
                \npower_state:\n    delay: now\n    mode: reboot\n    message: Powering\
                \ off\n    timeout: 2\n    condition: true\n\n--BOUNDARY\nContent-Type:\
                \ text/x-shellscript; charset=\"us-ascii\"\n\n#!/bin/bash\nDISTRO=$(awk\
                \ -F '[=\"]*' '/^NAME/  print $2 ' < /etc/os-release)\nUSER=\"ubuntu\"\
                \nif [[ $DISTRO == \"Amazon Linux\" ]]; then\n  USER=\"ec2-user\"\n\
                \  if [[ $(awk -F '[=\"]*' '/^VERSION_ID/  print $2 ' < /etc/os-release)\
                \ == \"2023\" ]]; then\n    DISTRO=\"al2023\"\n  fi\nfi\nUSER_HOME=\"\
                /home/$USER\"\nCLOUD_AUTOMATION=\"$USER_HOME/cloud-automation\"\n\
                (\n  cd $USER_HOME\n  if [[ ! -z \".slack_webhook\" ]]; then\n\
                \    echo \".slack_webhook\" > /slackWebhook\n  fi\n  git clone\
                \ https://github.com/uc-cdis/cloud-automation.git\n  cd $CLOUD_AUTOMATION\n\
                \  git pull\n\n  # This is needed temporarily for testing purposes\
                \ ; before merging the code to master\n  if [ \".branch\" !=\
                \ \"master\" ];\n  then\n    git checkout \".branch\"\n    git\
                \ pull\n  fi\n  chown -R $USER. $CLOUD_AUTOMATION\n\n  echo \"127.0.1.1\
                \ .env_squid_name\" | tee --append /etc/hosts\n  hostnamectl\
                \ set-hostname .env_squid_name\n  if [[ $DISTRO == \"Ubuntu\"\
                \ ]]; then\n    apt -y update\n    DEBIAN_FRONTEND='noninteractive'\
                \ apt-get -y -o Dpkg::Options::='--force-confdef' -o Dpkg::Options::='--force-confold'\
                \ upgrade\n\n    apt autoremove -y\n    apt clean\n    apt autoclean\n\
                \  fi\n  cd $USER_HOME\n\n  bash \".bootstrap_path.bootstrap_script\"\
                \ \"cwl_group=.env_log_group;join(\";\",var.extra_vars)\"\
                \ 2>&1\n  cd $CLOUD_AUTOMATION\n  git checkout master\n  # Install\
                \ qualys agent if the activtion and customer id provided\n  # Amazon\
                \ Linux does not support qualys agent (?)\n  # https://success.qualys.com/discussions/s/question/0D52L00004TnwvgSAB/installing-qualys-cloud-agent-on-amazon-linux-2-instances\n\
                \  if [[ $DISTRO == \"Ubuntu\" ]]; then\n    if [[ ! -z \".activation_id\"\
                \ ]] || [[ ! -z \".customer_id\" ]]; then\n      apt install\
                \ awscli jq -y\n      aws s3 cp s3://qualys-agentpackage/QualysCloudAgent.deb\
                \ ./qualys-cloud-agent.x86_64.deb\n      dpkg -i ./qualys-cloud-agent.x86_64.deb\n\
                \      # Clean up deb package after install\n      rm qualys-cloud-agent.x86_64.deb\n\
                \      sudo /usr/local/qualys/cloud-agent/bin/qualys-cloud-agent.sh\
                \ ActivationId=.activation_id CustomerId=.customer_id\n\
                \    fi\n  fi\n) > /var/log/bootstrapping_script_part2.log\n--BOUNDARY--\"\
                ))"
              block_device_mappings:
                device_name: /dev/xvda
                ebs:
                  volume_size: .squid_instance_drive_size
              tag_specifications:
                resource_type: instance
                tags:
                  Name: .env_squid_name
              lifecycle:
                create_before_destroy: true
          null_resource:
            service_depends_on:
              triggers:
                deps: jsonencode(var.squid_depends_on)
          aws_iam_service_linked_role:
            squidautoscaling:
              aws_service_name: autoscaling.amazonaws.com
              custom_suffix: .env_vpc_name_squid
              lifecycle:
                ignore_changes: custom_suffix
          aws_kms_grant:
            kms:
              count: '.fips ? 1 : 0'
              name: kms-cmk-eks
              key_id: .fips_ami_kms
              grantee_principal: aws_iam_service_linked_role.squidautoscaling.arn
              operations: Encrypt Decrypt ReEncryptFrom ReEncryptTo GenerateDataKey
                GenerateDataKeyWithoutPlaintext DescribeKey CreateGrant
          aws_autoscaling_group:
            squid_auto:
              name: .env_squid_name
              service_linked_role_arn: aws_iam_service_linked_role.squidautoscaling.arn
              desired_capacity: .cluster_desired_capasity
              max_size: .cluster_max_size
              min_size: .cluster_min_size
              vpc_zone_identifier: aws_subnet.squid_pub0.*.id
              depends_on: null_resource.service_depends_on aws_route_table_association.squid_auto0
              launch_template:
                id: aws_launch_template.squid_auto.id
                version: $Latest
              tag:
                key:
                - Name
                - Environment
                value:
                - .env_squid_name-grp-member
                - .organization_name
                propagate_at_launch:
                - true
                - true
          aws_security_group:
          - squidauto_in:
              name: .env_squid_name-squidauto_in
              description: security group that only enables ssh from VPC nodes and
                CSOC
              vpc_id: .env_vpc_id
              ingress:
                from_port:
                - 22
                - 3128
                - 80
                - 443
                to_port:
                - 22
                - 3128
                - 80
                - 443
                protocol:
                - TCP
                - TCP
                - TCP
                - TCP
                cidr_blocks:
                - local.cidrs
                - local.cidrs
                - local.cidrs2
                - local.cidrs2
              tags:
                Environment: .env_squid_name
                Organization: .organization_name
              lifecycle:
                ignore_changes: description
          - squidauto_out:
              name: .env_squid_name-squidauto_out
              description: security group that allow outbound traffics
              vpc_id: .env_vpc_id
              egress:
                from_port: 0
                to_port: 0
                protocol: '-1'
                cidr_blocks: 0.0.0.0/0
              tags:
                Environment: .env_squid_name
                Organization: .organization_name
        variable:
          env_vpc_cidr:
            description: CIDR of the VPC where this cluster will reside
          squid_proxy_subnet: 
          env_vpc_name: 
          env_squid_name: 
          ami_account_id:
            default: '137112412989'
          image_name_search_criteria:
            default: al2023-ami-*
          peering_cidr:
            default: 10.128.0.0/20
          secondary_cidr_block:
            default: ''
          bootstrap_path:
            default: cloud-automation/flavors/squid_auto/
          bootstrap_script:
            default: squid_running_on_docker.sh
          squid_instance_type:
            description: instance type that replicas of squid will be deployed into
            default: t3.medium
          organization_name:
            description: basically for tagging porpuses
            default: Basic Services
          env_log_group:
            description: log group in which to send logs from the instance
          env_vpc_id:
            description: the vpc id where the proxy cluster will reside
          ssh_key_name:
            description: ssh key name that instances in the cluster will use
          squid_instance_drive_size:
            description: Size of the root volume for the instance
            default: 8
          squid_availability_zones:
            description: AZs on wich to associate the routes for the squid proxies
          main_public_route:
            description: The route table that allows public access
          route_53_zone_id:
            description: DNS zone for .internal.io
          branch:
            description: branch to use in bootstrap script
            default: master
          extra_vars:
            description: additional variables to pass along with the bootstrapscript
            default: squid_image=master
          deploy_ha_squid:
            description: Should this module be deployed
            default: true
          cluster_desired_capasity:
            description: Desired capasity for the ha squid proxy
            default: 2
          cluster_max_size:
            description: Max size of the autoscaling group
            default: 3
          cluster_min_size:
            description: Min size of the autoscaling group
            default: 1
          network_expansion:
            description: let k8s workers run on a /22 subnet
            default: false
          squid_depends_on:
            default: ''
          activation_id:
            default: ''
          customer_id:
            default: ''
          slack_webhook:
            default: ''
          fips_ami_kms:
            default: arn:aws:kms:us-east-1:707767160287:key/mrk-697897f040ef45b0aa3cebf38a916f99
          fips:
            default: false
    data:
      aws_region:
        current: 
      aws_availability_zones:
        available: 
      aws_caller_identity:
        current: 
      aws_route_tables:
        control_routing_table:
          count: '.csoc_managed ? 0 : 1'
          vpc_id: .peering_vpc_id
    locals:
      cidrs: '.secondary_cidr_block != "" ? [''.vpc_cidr_block'', ''.peering_cidr'',
        ''.secondary_cidr_block''] : [''.vpc_cidr_block'', ''.peering_cidr'']'
      cidrs_no_peering: '.secondary_cidr_block != "" ? [''.vpc_cidr_block'',
        ''.secondary_cidr_block''] : [''.vpc_cidr_block'']'
    output:
      zone_zid:
        value: aws_route53_zone.main.zone_id
      zone_id:
        value: aws_route53_zone.main.id
      zone_name:
        value: aws_route53_zone.main.name
      vpc_id:
        value: aws_vpc.main.id
      vpc_cidr_block:
        value: aws_vpc.main.cidr_block
      public_route_table_id:
        value: aws_route_table.public.id
      gateway_id:
        value: aws_internet_gateway.gw.id
      public_subnet_id:
        value: aws_subnet.public.id
      security_group_local_id:
        value: aws_security_group.local.id
      nat_gw_id:
        value: aws_nat_gateway.nat_gw.id
      ssh_key_name:
        value: .ssh_key_name
      vpc_peering_id:
        value: aws_vpc_peering_connection.vpcpeering.id
      es_user_key:
        value: aws_iam_access_key.es_user_key.secret
      es_user_key_id:
        value: aws_iam_access_key.es_user_key.id
      cwlogs:
        value: aws_cloudwatch_log_group.main_log_group.arn
      fence-bot_id:
        value: module.fence-bot-user.fence-bot_id
      fence-bot_secret:
        value: module.fence-bot-user.fence-bot_secret
      data-bucket_name:
        value: module.data-bucket.data-bucket_name
      squid_auto:
        value: module.squid-auto.squid_auto-name
    resource:
      aws_vpc:
        main:
          cidr_block: .vpc_cidr_block
          enable_dns_hostnames: true
          tags:
            Name: .vpc_name
            Environment: .vpc_name
            Organization: .organization_name
          lifecycle:
            ignore_changes: tags
      aws_flow_log:
        main:
          count: '.vpc_flow_logs ? 1 : 0'
          iam_role_arn: aws_iam_role.flow_logs[count.index].arn
          log_destination: aws_cloudwatch_log_group.main_log_group.arn
          traffic_type: .vpc_flow_traffic
          vpc_id: aws_vpc.main.id
      aws_iam_role:
        flow_logs:
          count: '.vpc_flow_logs ? 1 : 0'
          name: .vpc_name_flow_logs_role
          assume_role_policy: "\n  \"Version\": \"2012-10-17\",\n  \"Statement\"\
            : [\n    \n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n     \
            \ \"Principal\": \n        \"Service\": \"vpc-flow-logs.amazonaws.com\"\
            \n      ,\n      \"Action\": \"sts:AssumeRole\"\n    \n  ]\n"
      aws_iam_role_policy:
        example:
          count: '.vpc_flow_logs ? 1 : 0'
          name: .vpc_name_flow_logs_policy
          role: aws_iam_role.flow_logs[count.index].id
          policy: "\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    \n\
            \      \"Action\": [\n        \"logs:CreateLogGroup\",\n        \"logs:CreateLogStream\"\
            ,\n        \"logs:PutLogEvents\",\n        \"logs:DescribeLogGroups\"\
            ,\n        \"logs:DescribeLogStreams\"\n      ],\n      \"Effect\": \"\
            Allow\",\n      \"Resource\": \"*\"\n    \n  ]\n"
      aws_vpc_ipv4_cidr_block_association:
        secondary_cidr:
          count: '.secondary_cidr_block != "" ? 1 : 0'
          vpc_id: aws_vpc.main.id
          cidr_block: .secondary_cidr_block
      aws_internet_gateway:
        gw:
          vpc_id: aws_vpc.main.id
          tags:
            Name: .vpc_name-igw
            Environment: .vpc_name
            Organization: .organization_name
      aws_nat_gateway:
        nat_gw:
          allocation_id: aws_eip.nat_gw.id
          subnet_id: aws_subnet.public.id
          tags:
            Name: .vpc_name-ngw
            Environment: .vpc_name
            Organization: .organization_name
      aws_route_table:
        public:
          vpc_id: aws_vpc.main.id
          route:
            cidr_block:
            - 0.0.0.0/0
            - .peering_cidr
            gateway_id: aws_internet_gateway.gw.id
            vpc_peering_connection_id: aws_vpc_peering_connection.vpcpeering.id
          tags:
            Name: main
            Environment: .vpc_name
            Organization: .organization_name
          lifecycle:
            ignore_changes: all
      aws_eip:
        nat_gw:
          vpc: true
          tags:
            Name: .vpc_name-ngw-eip
            Environment: .vpc_name
            Organization: .organization_name
      aws_default_route_table:
        default:
          default_route_table_id: aws_vpc.main.default_route_table_id
          route:
            cidr_block: .peering_cidr
            vpc_peering_connection_id: aws_vpc_peering_connection.vpcpeering.id
          tags:
            Name: default table
            Environment: .vpc_name
            Organization: .organization_name
      aws_main_route_table_association:
        default:
          vpc_id: aws_vpc.main.id
          route_table_id: aws_default_route_table.default.id
      aws_route_table_association:
        public:
          subnet_id: aws_subnet.public.id
          route_table_id: aws_route_table.public.id
      aws_subnet:
        public:
          vpc_id: aws_vpc.main.id
          cidr_block: '.network_expansion ? cidrsubnet(var.vpc_cidr_block, 5,
            2) : cidrsubnet(var.vpc_cidr_block, 4, 0)'
          map_public_ip_on_launch: true
          availability_zone: data.aws_availability_zones.available.names[1]
          tags: 'tomap("Name": "public", "Organization": ".organization_name",
            "Environment": ".vpc_name")'
          lifecycle:
            ignore_changes: tags availability_zone
      aws_cloudwatch_log_group:
        main_log_group:
          name: .vpc_name
          retention_in_days: .commons_log_retention
          tags:
            Environment: .vpc_name
            Organization: .organization_name
      aws_cloudwatch_log_subscription_filter:
        csoc_subscription:
          count: '.csoc_managed && var.send_logs_to_csoc ? 1 : 0'
          name: .vpc_name_subscription
          destination_arn: 'arn:aws:logs:data.aws_region.current.name:.csoc_managed
            ? var.csoc_account_id : data.aws_caller_identity.current.account_id:destination:.vpc_name_logs_destination'
          log_group_name: .vpc_name
          filter_pattern: ''
          lifecycle:
            ignore_changes: distribution
      aws_route53_zone:
        main:
          name: internal.io
          comment: internal dns server for .vpc_name
          vpc:
            vpc_id: aws_vpc.main.id
          tags:
            Environment: .vpc_name
            Organization: .organization_name
      aws_vpc_peering_connection:
        vpcpeering:
          peer_owner_id: '.csoc_managed ? var.csoc_account_id : data.aws_caller_identity.current.account_id'
          peer_vpc_id: .peering_vpc_id
          vpc_id: aws_vpc.main.id
          auto_accept: false
          tags:
            Name: VPC Peering between .vpc_name and adminVM vpc
            Environment: .vpc_name
            Organization: .organization_name
          lifecycle:
            ignore_changes: all
      aws_route:
        default_csoc:
          count: '.csoc_managed ? 0 : 1'
          route_table_id: data.aws_route_tables.control_routing_table[count.index].id
          destination_cidr_block: .vpc_cidr_block
          vpc_peering_connection_id: aws_vpc_peering_connection.vpcpeering.id
      aws_iam_user:
        es_user:
          name: .vpc_name_es_user
          tags:
            Environment: .vpc_name
            Organization: .organization_name
      aws_iam_access_key:
        es_user_key:
          user: aws_iam_user.es_user.name
      aws_security_group:
      - local:
          name: local
          description: security group that only allow internal tcp traffics
          vpc_id: aws_vpc.main.id
          ingress:
            from_port: 0
            to_port: 0
            protocol: '-1'
            cidr_blocks: local.cidrs
          egress:
            from_port: 0
            to_port: 0
            protocol: '-1'
            cidr_blocks: local.cidrs_no_peering
          tags:
            Environment: .vpc_name
            Organization: .organization_name
            Name: .vpc_name-local-sec-group
      - out:
          name: out
          description: security group that allow outbound traffics
          vpc_id: aws_vpc.main.id
          egress:
            from_port: 0
            to_port: 0
            protocol: '-1'
            cidr_blocks: 0.0.0.0/0
          tags:
            Environment: .vpc_name
            Organization: .organization_name
            Name: .vpc_name-outbound-traffic
      - proxy:
          count: '.deploy_single_proxy ? 1 : 0 '
          name: squid-proxy
          description: allow inbound tcp at 3128
          vpc_id: aws_vpc.main.id
          ingress:
            from_port: 0
            to_port: 3128
            protocol: TCP
            cidr_blocks: local.cidrs
          tags:
            Environment: .vpc_name
            Organization: Basic Service
    variable:
      ami_account_id:
        default: 099720109477
      vpc_name: 
      vpc_cidr_block:
        default: 172.24.17.0/20
      secondary_cidr_block:
        default: ''
      vpc_flow_logs:
        default: false
      vpc_flow_traffic:
        default: ALL
      ssh_key_name: 
      csoc_account_id:
        default: '433568766270'
      peering_cidr:
        default: 10.128.0.0/20
      peering_vpc_id:
        default: vpc-e2b51d99
      csoc_managed:
        default: true
      organization_name:
        description: for tagging purposes
        default: Basic Service
      availability_zones:
        description: AZ to be used by EKS nodes
        default: us-east-1a us-east-1c us-east-1d
      squid_image_search_criteria:
        description: Search criteria for squid AMI look up
        default: ubuntu/images/hvm-ssd/ubuntu-bionic-18.04-amd64-server-*
      squid_instance_drive_size:
        description: Volume size for the squid instance
        default: 8
      squid_instance_type:
        description: Instance type for HA squid instances
        default: t3.medium
      squid_bootstrap_script:
        description: Script to run on deployment for the HA squid instances
        default: squid_running_on_docker.sh
      deploy_single_proxy:
        description: Single instance plus HA
        default: false
      squid_extra_vars:
        description: additional variables to pass along with the bootstrapscript
      branch:
        description: For testing purposes, when something else than the master
        default: master
      fence-bot_bucket_access_arns:
        description: When fence bot has to access another bucket that wasn't created
          by the VPC module
      deploy_ha_squid:
        description: should you want to deploy HA-squid
        default: false
      squid_cluster_desired_capasity:
        description: If ha squid is enabled and you want to set your own capasity
        default: 2
      squid_cluster_min_size:
        description: If ha squid is enabled and you want to set your own min size
        default: 1
      squid_cluster_max_size:
        description: If ha squid is enabled and you want to set your own max size
        default: 3
      single_squid_instance_type:
        description: Single squid instance type
      network_expansion:
        description: Let k8s wokers use /22 subnets per AZ
        default: false
      activation_id:
        default: ''
      customer_id:
        default: ''
      slack_webhook:
        default: ''
      fips:
        default: false
      deploy_cloud_trail:
        default: true
      send_logs_to_csoc:
        default: true
      commons_log_retention:
        description: value in days for the cloudwatch log retention period
        default: '3650'
  commons_vpc_es:
    count: '.deploy_es ? 1 : 0'
    vpc_name: .vpc_name
    vpc_id: module.cdis_vpc.vpc_id
    instance_type: .es_instance_type
    ebs_volume_size_gb: .ebs_volume_size_gb
    encryption: .encryption
    instance_count: .es_instance_count
    organization_name: .organization_name
    es_version: .es_version
    es_linked_role: .es_linked_role
    es_name: .es_name
    role_arn: '.deploy_es_role ? aws_iam_role.esproxy-role[0].arn : ""'
    depends_on: module.cdis_vpc.vpc_id module.cdis_vpc.vpc_peering_id
    data:
      aws_availability_zones:
        available:
          state: available
      aws_vpcs:
        vpcs:
          tags:
            Name: .vpc_name
      aws_vpc:
        the_vpc:
          id: data.aws_vpcs.vpcs.ids[0]
      aws_iam_user:
        es_user:
          user_name: .vpc_name_es_user
      aws_cloudwatch_log_group:
        logs_group:
          name: .vpc_name
      aws_subnets:
        private:
          filter:
            name: vpc-id
            values: data.aws_vpc.the_vpc.id
          tags:
            Name: private_db_alt
    locals:
      vpc_id: '.vpc_id != "" ? var.vpc_id : data.aws_vpc.the_vpc.id'
      es_policy: '.role_arn == "" ? local.policy1 : local.policy2'
      policy1: "\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n      \
        \  \n            \"Action\": \"es:*\",\n            \"Principal\": \n  \
        \            \"AWS\": [\n                \"data.aws_iam_user.es_user.arn\"\
        \n              ]\n            ,\n            \"Effect\": \"Allow\",\n  \
        \          \"Resource\": \"*\"\n        \n    ]\n"
      policy2: "\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n      \
        \  \n            \"Action\": \"es:*\",\n            \"Principal\": \n  \
        \            \"AWS\": [\n                \"data.aws_iam_user.es_user.arn\"\
        ,\n                \".role_arn\"\n              ]\n            ,\n\
        \            \"Effect\": \"Allow\",\n            \"Resource\": \"*\"\n   \
        \     \n    ]\n"
      all_cidr_blocks: '[for assoc in data.aws_vpc.the_vpc.cidr_block_associations
        : assoc.cidr_block]'
    output:
      kibana_endpoint:
        value: aws_elasticsearch_domain.gen3_metadata.kibana_endpoint
      es_endpoint:
        value: aws_elasticsearch_domain.gen3_metadata.endpoint
      es_arn:
        value: aws_elasticsearch_domain.gen3_metadata.arn
    resource:
      aws_iam_service_linked_role:
        es:
          count: '.es_linked_role ? 1 : 0'
          aws_service_name: es.amazonaws.com
      aws_security_group:
        private_es:
          name: private_es
          description: security group that allow es port out
          vpc_id: local.vpc_id
          ingress:
            from_port: 0
            to_port: 0
            protocol: '-1'
            cidr_blocks: local.all_cidr_blocks
          egress:
            from_port: 0
            to_port: 0
            protocol: '-1'
            cidr_blocks: local.all_cidr_blocks
          tags:
            Environment: .vpc_name
            Organization: .organization_name
      aws_cloudwatch_log_resource_policy:
        es_logs:
          policy_name: es_logs_for_.vpc_name
          policy_document: "\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n\
            \    \n      \"Effect\": \"Allow\",\n      \"Principal\": \n       \
            \ \"Service\": \"es.amazonaws.com\"\n      ,\n      \"Action\": [\n \
            \       \"logs:PutLogEvents\",\n        \"logs:PutLogEventsBatch\",\n\
            \        \"logs:CreateLogStream\"\n      ],\n      \"Resource\": \"data.aws_cloudwatch_log_group.logs_group.arn:*\"\
            \n    \n  ]\n"
      aws_elasticsearch_domain:
        gen3_metadata:
          domain_name: '.es_name != "" ? var.es_name : ".vpc_name-gen3-metadata"'
          elasticsearch_version: .es_version
          access_policies: local.es_policy
          encrypt_at_rest:
            enabled: .encryption
          node_to_node_encryption:
            enabled: .encryption
          vpc_options:
            security_group_ids: aws_security_group.private_es.id
            subnet_ids: data.aws_subnets.private.ids
          cluster_config:
            instance_type: .instance_type
            instance_count: .instance_count
          ebs_options:
            ebs_enabled: 'true'
            volume_size: .ebs_volume_size_gb
          log_publishing_options:
            log_type: ES_APPLICATION_LOGS
            cloudwatch_log_group_arn: data.aws_cloudwatch_log_group.logs_group.arn:*
            enabled: 'true'
          advanced_options:
            rest.action.multi.allow_explicit_index: 'true'
          snapshot_options:
            automated_snapshot_start_hour: 23
          lifecycle:
            ignore_changes: elasticsearch_version
          tags:
            Name: gen3_metadata
            Environment: .vpc_name
            Organization: .organization_name
          depends_on: aws_cloudwatch_log_resource_policy.es_logs aws_iam_service_linked_role.es
    variable:
      vpc_name: 
      es_name:
        default: ''
      vpc_id:
        default: ''
      instance_type:
        default: m4.large.elasticsearch
      ebs_volume_size_gb:
        default: 20
      encryption:
        default: 'true'
      instance_count:
        default: 3
      organization_name:
        description: For tagging purposes
        default: Basic Service
      es_version:
        description: What version to use when deploying ES
        default: '7.10'
      es_linked_role:
        description: Whether or no to deploy a linked roll for ES
        default: true
      role_arn:
        description: The ARN of the role to use for ES
        default: ''
  config_files:
    vpc_name: .vpc_name
    db_fence_address: local.db_fence_address
    db_fence_password: '.db_password_fence != "" ? var.db_password_fence : random_password.fence_password.result'
    db_fence_name: .fence_database_name
    db_sheepdog_address: local.db_sheepdog_address
    db_sheepdog_username: .sheepdog_db_username
    db_sheepdog_password: '.db_password_sheepdog != "" ? var.db_password_sheepdog
      : random_password.sheepdog_password.result'
    db_sheepdog_name: .sheepdog_database_name
    db_peregrine_address: local.db_peregrine_address
    db_peregrine_password: '.db_password_peregrine != "" ? var.db_password_peregrine
      : random_password.peregrine_password.result'
    db_indexd_address: local.db_indexd_address
    db_indexd_username: .indexd_db_username
    db_indexd_password: '.db_password_indexd != "" ? var.db_password_indexd :
      random_password.indexd_password.result'
    db_indexd_name: .indexd_database_name
    hostname: .hostname
    google_client_secret: .google_client_secret
    google_client_id: .google_client_id
    hmac_encryption_key: '.hmac_encryption_key != "" ? var.hmac_encryption_key
      : base64encode(random_password.hmac_encryption_key.result)'
    sheepdog_secret_key: '.sheepdog_secret_key != "" ? var.sheepdog_secret_key
      : random_password.sheepdog_secret_key.result'
    sheepdog_indexd_password: '.sheepdog_indexd_password != "" ? var.sheepdog_indexd_password
      : random_password.sheepdog_indexd_password.result'
    sheepdog_oauth2_client_id: .sheepdog_oauth2_client_id
    sheepdog_oauth2_client_secret: .sheepdog_oauth2_client_secret
    gitops_path: .gitops_path
    ssl_certificate_id: .aws_cert_name
    aws_user_key: module.cdis_vpc.es_user_key
    aws_user_key_id: module.cdis_vpc.es_user_key_id
    indexd_prefix: .indexd_prefix
    mailgun_api_key: .mailgun_api_key
    mailgun_api_url: .mailgun_api_url
    mailgun_smtp_host: .mailgun_smtp_host
    resource:
      null_resource:
        config_setup:
          provisioner:
            local-exec:
            - command: mkdir -p .vpc_name_output; echo 'templatefile("path.module/creds.tpl",
                fence_host = var.db_fence_address, fence_user = var.db_fence_username,
                fence_pwd = var.db_fence_password, fence_db = var.db_fence_name, peregrine_host
                = var.db_peregrine_address, sheepdog_host = var.db_sheepdog_address,
                sheepdog_user = var.db_sheepdog_username, sheepdog_pwd = var.db_sheepdog_password,
                sheepdog_db = var.db_sheepdog_name, peregrine_pwd = var.db_peregrine_password,
                indexd_host = var.db_indexd_address, indexd_user = var.db_indexd_username,
                indexd_pwd = var.db_indexd_password, indexd_db = var.db_indexd_name,
                hostname = var.hostname, google_client_secret = var.google_client_secret,
                google_client_id = var.google_client_id, hmac_encryption_key = var.hmac_encryption_key,
                sheepdog_secret_key = var.sheepdog_secret_key, sheepdog_indexd_password
                = var.sheepdog_indexd_password, sheepdog_oauth2_client_id = var.sheepdog_oauth2_client_id,
                sheepdog_oauth2_client_secret = var.sheepdog_oauth2_client_secret,
                aws_user_key    = var.aws_user_key, aws_user_key_id = var.aws_user_key_id,
                indexd_prefix   = var.indexd_prefix, mailgun_api_key = var.mailgun_api_key,
                mailgun_api_url = var.mailgun_api_url, mailgun_smtp_host = var.mailgun_smtp_host)'
                >.vpc_name_output/creds.json
            - command: echo "templatefile("path.module/00configmap.yaml", vpc_name
                = var.vpc_name, hostname = var.hostname, revproxy_arn = var.ssl_certificate_id,
                gitops_path = var.gitops_path)" > .vpc_name_output/00configmap.yaml
    variable:
      vpc_name: 
      hostname: 
      ssl_certificate_id:
        default: AWS-CERTIFICATE-ID
      sheepdog_oauth2_client_id:
        default: deprecated
      sheepdog_oauth2_client_secret:
        default: deprecated
      sheepdog_indexd_password: 
      sheepdog_secret_key: 
      hmac_encryption_key:
        default: deprecated
      google_client_id: 
      google_client_secret: 
      db_fence_name:
        default: fence
      db_fence_password: 
      db_fence_username:
        default: fence_user
      db_fence_address: 
      db_indexd_name:
        default: indexd
      db_indexd_password: 
      db_indexd_username:
        default: indexd_user
      db_indexd_address: 
      db_sheepdog_password: 
      db_peregrine_password: 
      db_sheepdog_username:
        default: sheepdog
      db_sheepdog_name:
        default: sheepdog
      db_sheepdog_address: 
      db_peregrine_address: 
      aws_user_key: 
      aws_user_key_id: 
      indexd_prefix: 
      mailgun_api_key: 
      mailgun_smtp_host:
        default: smtp.mailgun.org
      mailgun_api_url:
        default: https://api.mailgun.net/v3/
      gitops_path:
        default: https://github.com/uc-cdis/cdis-manifest.git
  csoc_peering_connection:
    count: '.csoc_peering ? 1 : 0'
    vpc_name: .vpc_name
    route_table_name: .route_table_name
    csoc_vpc_id: .peering_vpc_id
    csoc_cidr: .peering_cidr
    organization_name: .organization_name
    vpc_cidr_block: .vpc_cidr_block
    pcx_id: module.cdis_vpc.vpc_peering_id
    providers:
      aws: aws.csoc
    data:
      aws_route_table:
        selected:
          filter:
            name: tag:Name
            values: .route_table_name
          vpc_id: .csoc_vpc_id
    resource:
      aws_vpc_peering_connection_accepter:
        vpcpeering:
          vpc_peering_connection_id: .pcx_id
          auto_accept: true
          tags:
            Name: VPC Peering between .vpc_name and adminVM vpc
            Environment: .vpc_name
            Organization: .organization_name
          lifecycle:
            ignore_changes: all
      aws_route:
        r:
          route_table_id: data.aws_route_table.selected.id
          destination_cidr_block: .vpc_cidr_block
          vpc_peering_connection_id: aws_vpc_peering_connection_accepter.vpcpeering.id
          depends_on: aws_vpc_peering_connection_accepter.vpcpeering
    variable:
      vpc_name:
        description: Name of the VPC
        type: string
        default: ''
      route_table_name:
        description: Name of the route table to use for the peering connection
        type: string
        default: eks_private
      csoc_vpc_id:
        description: VPC ID of the peering connection
        type: string
        default: vpc-e2b51d99
      csoc_cidr:
        description: CIDR block of the peering connection
        type: string
        default: ''
      organization_name:
        description: Name of the organization
        type: string
        default: cdis
      vpc_cidr_block:
        description: CIDR block of the VPC
        type: string
        default: ''
      pcx_id:
        description: ID of the peering connection
        type: string
        default: ''
  eks:
    count: '.deploy_eks ? 1 : 0'
    vpc_name: .vpc_name
    vpc_id: module.cdis_vpc.vpc_id
    ec2_keyname: .ec2_keyname
    instance_type: .instance_type
    peering_cidr: .peering_cidr
    csoc_managed: .csoc_managed
    secondary_cidr_block: .secondary_cidr_block
    users_policy: .users_policy
    worker_drive_size: .worker_drive_size
    eks_version: .eks_version
    jupyter_instance_type: .jupyter_instance_type
    workers_subnet_size: .workers_subnet_size
    bootstrap_script: .bootstrap_script
    jupyter_bootstrap_script: .jupyter_bootstrap_script
    kernel: .kernel
    jupyter_worker_drive_size: .jupyter_worker_drive_size
    cidrs_to_route_to_gw: .cidrs_to_route_to_gw
    organization_name: .organization_name
    peering_vpc_id: .peering_vpc_id
    jupyter_asg_desired_capacity: .jupyter_asg_desired_capacity
    jupyter_asg_max_size: .jupyter_asg_max_size
    jupyter_asg_min_size: .jupyter_asg_min_size
    iam-serviceaccount: .iam-serviceaccount
    oidc_eks_thumbprint: .oidc_eks_thumbprint
    domain_test: .domain_test
    ha_squid: .deploy_ha_squid
    dual_proxy: .dual_proxy
    single_az_for_jupyter: .single_az_for_jupyter
    sns_topic_arn: .sns_topic_arn
    activation_id: .activation_id
    customer_id: .customer_id
    workflow_instance_type: .workflow_instance_type
    workflow_bootstrap_script: .workflow_bootstrap_script
    workflow_worker_drive_size: .workflow_worker_drive_size
    workflow_asg_desired_capacity: .workflow_asg_desired_capacity
    workflow_asg_max_size: .workflow_asg_max_size
    workflow_asg_min_size: .workflow_asg_min_size
    deploy_workflow: .deploy_workflow
    fips: .fips
    fips_ami_kms: .fips_ami_kms
    fips_enabled_ami: .fips_enabled_ami
    availability_zones: .availability_zones
    use_asg: .use_asg
    use_karpenter: .use_karpenter
    karpenter_version: .karpenter_version
    deploy_karpenter_in_k8s: .deploy_karpenter_in_k8s
    ci_run: .ci_run
    eks_public_access: .eks_public_access
    enable_vpc_endpoints: .enable_vpc_endpoints
    spot_linked_role: .spot_linked_role
    csoc_account_id: .csoc_account_id
    k8s_bootstrap_resources: .k8s_bootstrap_resources
    depends_on: module.cdis_vpc.vpc_id module.cdis_vpc.vpc_peering_id module.cdis_vpc.squid_auto
    data:
      aws_caller_identity:
        current: 
      aws_region:
        current: 
      aws_vpc:
        the_vpc:
          id: data.aws_vpcs.vpcs.ids[0]
      aws_availability_zones:
        available:
          state: available
      aws_vpcs:
        vpcs:
          tags:
            Name: .vpc_name
      aws_nat_gateway:
        the_gateway:
          vpc_id: data.aws_vpc.the_vpc.id
          tags:
            Name: .vpc_name-ngw
          state: available
      aws_vpc_peering_connection:
        pc:
          vpc_id: data.aws_vpc.the_vpc.id
          peer_owner_id: .csoc_account_id
          status: active
      aws_vpc_endpoint_service:
      - logs:
          service: logs
      - ec2:
          service: ec2
      - autoscaling:
          service: autoscaling
      - ecr_dkr:
          service: ecr.dkr
      - ecr_api:
          service: ecr.api
      - ebs:
          service: ebs
      - sts:
          service: sts
      aws_route_table:
      - public_kube:
          vpc_id: data.aws_vpc.the_vpc.id
          tags:
            Name: main
      - private_kube_route_table:
          vpc_id: data.aws_vpc.the_vpc.id
          tags:
            Name: private_kube
      aws_ami:
        eks_worker:
          filter:
            name: name
            values: amazon-eks-node-.eks_version*
          most_recent: true
          owners: '602401143452'
      aws_security_group:
        local_traffic:
          vpc_id: data.aws_vpc.the_vpc.id
          name: local
      aws_autoscaling_group:
        squid_auto:
          count: '.ha_squid ? 1 : 0'
          name: squid-auto-.vpc_name
      aws_instances:
        squid_proxy:
          count: '.ha_squid ? var.dual_proxy ? 1 : 0 : 1'
          instance_tags:
            Name: .vpc_name.proxy_name
      aws_route53_zone:
        vpczone:
          name: internal.io.
          vpc_id: data.aws_vpc.the_vpc.id
      archive_file:
        lambda_function:
          type: zip
          source_file: path.module/lambda_function.py
          output_path: lambda_function_payload.zip
      aws_iam_policy_document:
      - with_resources:
          statement:
            actions: ec2:CreateRoute ec2:DeleteRoute ec2:ReplaceRoute route53:GetHostedZone
              route53:ChangeResourceRecordSets route53:ListResourceRecordSets
            effect: Allow
            resources: arn:aws:ec2:data.aws_region.current.name:data.aws_caller_identity.current.account_id:route-table/aws_route_table.eks_private.id
              arn:aws:ec2:data.aws_region.current.name:data.aws_caller_identity.current.account_id:route-table/data.aws_route_table.private_kube_route_table.id
              arn:aws:route53:::hostedzone/data.aws_route53_zone.vpczone.zone_id
      - without_resources:
          statement:
            actions: autoscaling:DescribeAutoScalingInstances route53:CreateHostedZone
              ec2:DescribeInstances route53:ListHostedZones ec2:DeleteNetworkInterface
              ec2:DisassociateRouteTable ec2:DescribeSecurityGroups ec2:AssociateRouteTable
              ec2:CreateNetworkInterface ec2:DescribeNetworkInterfaces autoscaling:DescribeAutoScalingGroups
              ec2:DescribeVpcs ec2:DescribeSubnets ec2:DescribeRouteTables ec2:DescribeInstanceAttribute
              ec2:ModifyInstanceAttribute
            effect: Allow
            resources: '*'
      - planx-csoc-alerts-topic_access:
          count: '.sns_topic_arn != "" ? 1 : 0'
          statement:
            actions: sns:Publish
            effect: Allow
            resources: .sns_topic_arn
      - irsa_assume_role:
          count: '.use_karpenter ? 1 : 0'
          statement:
            effect: Allow
            actions: sts:AssumeRoleWithWebIdentity
            principals:
              type: Federated
              identifiers: arn:aws:iam::local.account_id:oidc-provider/local.irsa_oidc_provider_url
            condition:
              test:
              - StringEquals
              - StringEquals
              variable:
              - local.irsa_oidc_provider_url:sub
              - local.irsa_oidc_provider_url:aud
              values:
              - system:serviceaccount:karpenter:karpenter
              - sts.amazonaws.com
      - irsa:
          count: '.use_karpenter ? 1 : 0'
          statement:
            sid:
            - Karpenter
            - Karpenter2
            - ConditionalEC2Termination
            - VisualEditor0
            actions:
            - ssm:GetParameter iam:PassRole iam:*InstanceProfile ec2:DescribeImages
              ec2:RunInstances ec2:DescribeSubnets ec2:DescribeSecurityGroups ec2:DescribeLaunchTemplates
              ec2:DescribeInstances ec2:DescribeInstanceTypes ec2:DescribeInstanceTypeOfferings
              ec2:DescribeAvailabilityZones ec2:DeleteLaunchTemplate ec2:CreateTags
              ec2:CreateLaunchTemplate ec2:CreateFleet ec2:DescribeSpotPriceHistory
              pricing:GetProducts eks:DescribeCluster
            - sqs:DeleteMessage sqs:GetQueueAttributes sqs:GetQueueUrl sqs:ReceiveMessage
            - ec2:TerminateInstances
            - kms:*
            effect:
            - Allow
            - Allow
            - Allow
            - Allow
            resources:
            - '*'
            - arn:aws:sqs:*:local.account_id:karpenter-sqs-vpc_name
            - '*'
            - '*'
            condition:
              test: StringLike
              variable: ec2:ResourceTag/Name
              values: '*karpenter*'
      - queue:
          count: '.use_karpenter ? 1 : 0'
          statement:
            sid: SqsWrite
            actions: sqs:SendMessage
            resources: aws_sqs_queue.this[0].arn
            principals:
              type: Service
              identifiers: events.amazonaws.com sqs.amazonaws.com
      aws_ecrpublic_authorization_token:
        token: 
    module:
      iam_policy:
        count: '.ha_squid ? 1 : 0'
        policy_name: .vpc_name-gw-checks-lambda-cwlg
        policy_path: /
        policy_description: IAM policy for .vpc_name-gw-checks-lambda to access
          CWLG
        policy_json: "\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    \n\
          \      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n\
          \        \"logs:CreateLogGroup\",\n        \"logs:CreateLogStream\",\n \
          \       \"logs:PutLogEvents\"\n      ],\n      \"Resource\": \"arn:aws:logs:*:*:*\"\
          \n    \n  ]\n"
        output:
          id:
            value: aws_iam_policy.policy.id
          name:
            value: aws_iam_policy.policy.name
          arn:
            value: aws_iam_policy.policy.arn
          descripton:
            value: aws_iam_policy.policy.description
          path:
            value: aws_iam_policy.policy.path
          policy:
            value: aws_iam_policy.policy.policy
        resource:
          aws_iam_policy:
            policy:
              name: .policy_name
              path: .policy_path
              description: .policy_description
              policy: .policy_json
        variable:
          policy_name:
            description: Name for the policy
          policy_path:
            description: 'Path in which to create the policy. '
          policy_description:
            description: Description for the policy
            default: ''
          policy_json:
            description: Basically the actual policy in JSON
      iam_role:
        count: '.ha_squid ? 1 : 0'
        role_name: .vpc_name-gw-checks-lambda-role
        role_description: Role for .vpc_name-gw-checks-lambda
        role_tags:
          Environment: .vpc_name
          Organization: .organization_name
        role_assume_role_policy: "\n  \"Version\": \"2012-10-17\",\n  \"Statement\"\
          : [\n    \n      \"Action\": \"sts:AssumeRole\",\n      \"Principal\":\
          \ \n        \"Service\": \"lambda.amazonaws.com\"\n      ,\n      \"Effect\"\
          : \"Allow\",\n      \"Sid\": \"\"\n    \n  ]\n"
        output:
          role_id:
            value: aws_iam_role.the_role.id
          role_arn:
            value: aws_iam_role.the_role.arn
        resource:
          aws_iam_role:
            the_role:
              name: .role_name
              description: .role_description
              assume_role_policy: .role_assume_role_policy
              force_detach_policies: .role_force_detach_policies
              tags: .role_tags
        variable:
          role_name:
            description: Name for the role to be created
          role_assume_role_policy:
            description: Assume role policy in JSON format
          role_tags:
            description: Tags for the role
            default: 
          role_force_detach_policies:
            description: Specifies to force detaching any policies the role has before
              destroying it. Defaults to false.
            default: 'false'
          role_description:
            description: Description for the role
            default: ''
      jupyter_pool:
        count: '.deploy_jupyter ? 1 : 0'
        scale_in_protection: false
        ec2_keyname: .ec2_keyname
        users_policy: .users_policy
        nodepool: jupyter
        vpc_name: .vpc_name
        csoc_cidr: .peering_cidr
        eks_cluster_endpoint: aws_eks_cluster.eks_cluster.endpoint
        eks_cluster_ca: aws_eks_cluster.eks_cluster.certificate_authority[0].data
        eks_private_subnets: aws_subnet.eks_private.*.id
        control_plane_sg: aws_security_group.eks_control_plane_sg.id
        default_nodepool_sg: aws_security_group.eks_nodes_sg.id
        eks_version: .eks_version
        nodepool_instance_type: .jupyter_instance_type
        kernel: .kernel
        bootstrap_script: .jupyter_bootstrap_script
        nodepool_worker_drive_size: .jupyter_worker_drive_size
        organization_name: .organization_name
        nodepool_asg_desired_capacity: .jupyter_asg_desired_capacity
        nodepool_asg_max_size: .jupyter_asg_max_size
        nodepool_asg_min_size: .jupyter_asg_min_size
        activation_id: .activation_id
        customer_id: .customer_id
        vpc_id: local.vpc_id
        data:
          aws_caller_identity:
            current: 
          aws_region:
            current: 
          aws_vpcs:
            vpcs:
              tags:
                Name: .vpc_name
          aws_vpc:
            the_vpc:
              id: data.aws_vpcs.vpcs.ids[0]
          aws_availability_zones:
            available:
              state: available
          aws_ami:
            eks_worker:
              filter:
                name: name
                values: amazon-eks-node-.eks_version*
              most_recent: true
              owners: '602401143452'
        locals:
          vpc_id: '.vpc_id != "" ? var.vpc_id : data.aws_vpc.the_vpc.id'
        output:
          nodepool_role:
            value: aws_iam_role.eks_node_role.arn
          nodepool_sg:
            value: aws_security_group.eks_nodes_sg.id
        resource:
          aws_iam_role:
          - eks_control_plane_role:
              name: .vpc_name_EKS_.nodepool_role
              assume_role_policy: "\n  \"Version\": \"2012-10-17\",\n  \"Statement\"\
                : [\n    \n      \"Action\": \"sts:AssumeRole\",\n      \"Principal\"\
                : \n        \"Service\": \"eks.amazonaws.com\"\n      ,\n      \"\
                Effect\": \"Allow\",\n      \"Sid\": \"\"\n    \n  ]\n"
          - eks_node_role:
              name: eks_.vpc_name_nodepool_.nodepool_role
              assume_role_policy: "\n  \"Version\": \"2012-10-17\",\n  \"Statement\"\
                : [\n    \n      \"Effect\": \"Allow\",\n      \"Principal\": \n\
                \        \"Service\": \"ec2.amazonaws.com\"\n      ,\n      \"Action\"\
                : \"sts:AssumeRole\"\n    \n  ]\n"
          aws_iam_role_policy_attachment:
          - eks-policy-AmazonEKSClusterPolicy:
              policy_arn: arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
              role: aws_iam_role.eks_control_plane_role.name
          - eks-policy-AmazonEKSServicePolicy:
              policy_arn: arn:aws:iam::aws:policy/AmazonEKSServicePolicy
              role: aws_iam_role.eks_control_plane_role.name
          - eks-policy-AmazonSSMManagedInstanceCore:
              policy_arn: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
              role: aws_iam_role.eks_control_plane_role.name
          - eks-node-AmazonEKSWorkerNodePolicy:
              policy_arn: arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
              role: aws_iam_role.eks_node_role.name
          - eks-node-AmazonEKS_CNI_Policy:
              policy_arn: arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
              role: aws_iam_role.eks_node_role.name
          - eks-node-AmazonEKSCSIDriverPolicy:
              policy_arn: arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy
              role: aws_iam_role.eks_node_role.name
          - eks-node-AmazonEC2ContainerRegistryReadOnly:
              policy_arn: arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
              role: aws_iam_role.eks_node_role.name
          - cloudwatch_logs_access:
              policy_arn: aws_iam_policy.cwl_access_policy.arn
              role: aws_iam_role.eks_node_role.name
          - asg_access:
              policy_arn: aws_iam_policy.asg_access.arn
              role: aws_iam_role.eks_node_role.name
          - kernel_access:
              policy_arn: aws_iam_policy.access_to_kernels.arn
              role: aws_iam_role.eks_node_role.name
          aws_iam_policy:
          - cwl_access_policy:
              name: .vpc_name_EKS_nodepool_.nodepool_access_to_cloudwatchlogs
              description: In order to avoid the creation of users and keys, we are
                using roles and policies.
              policy: "\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n\
                \        \n            \"Effect\": \"Allow\",\n            \"Action\"\
                : \"logs:DescribeLogGroups\",\n            \"Resource\": \"arn:aws:logs:data.aws_region.current.name:data.aws_caller_identity.current.account_id:log-group::log-stream:*\"\
                \n        ,\n        \n            \"Effect\": \"Allow\",\n    \
                \        \"Action\": [\n                \"logs:CreateLogStream\",\n\
                \                \"logs:PutLogEvents\",\n                \"logs:DescribeLogStreams\"\
                \n            ],\n            \"Resource\": \"arn:aws:logs:data.aws_region.current.name:data.aws_caller_identity.current.account_id:log-group:.vpc_name:log-stream:*\"\
                \n        \n    ]\n"
          - access_to_kernels:
              name: .vpc_name_EKS_nodepool_.nodepool_kernel_access
              description: To access custom Kernels
              policy: "\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n\
                \        \n            \"Sid\": \"\",\n            \"Effect\": \"\
                Allow\",\n            \"Action\": [\n                \"s3:List*\"\
                ,\n                \"s3:Get*\"\n            ],\n            \"Resource\"\
                : [\n                \"arn:aws:s3:::gen3-kernels/*\",\n          \
                \      \"arn:aws:s3:::gen3-kernels\",\n                \"arn:aws:s3:::qualys-agentpackage\"\
                ,\n                \"arn:aws:s3:::qualys-agentpackage/*\"\n      \
                \      ]\n        \n    ]\n"
          - asg_access:
              name: .vpc_name_EKS_nodepool_.nodepool_autoscaling_access
              description: Allow the deployment cluster-autoscaler to add or terminate
                instances accordingly
              policy: "\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n\
                \        \n            \"Effect\": \"Allow\",\n            \"Action\"\
                : [\n                \"autoscaling:DescribeAutoScalingGroups\",\n\
                \                \"autoscaling:DescribeAutoScalingInstances\",\n \
                \               \"autoscaling:DescribeTags\",\n                \"\
                autoscaling:SetDesiredCapacity\",\n                \"autoscaling:TerminateInstanceInAutoScalingGroup\"\
                ,\n                \"autoscaling:DescribeLaunchConfigurations\"\n\
                \            ],\n            \"Resource\": \"*\"\n        \n    ]\n\
                "
          aws_iam_instance_profile:
            eks_node_instance_profile:
              name: .vpc_name_EKS_nodepool_.nodepool
              role: aws_iam_role.eks_node_role.name
          aws_security_group:
          - eks_nodes_sg:
              name: .vpc_name_EKS_nodepool_.nodepool_sg
              description: 'Security group for all nodes in pool .nodepool in
                the EKS cluster [.vpc_name] '
              vpc_id: local.vpc_id
              egress:
                from_port: 0
                to_port: 0
                protocol: '-1'
                cidr_blocks: 0.0.0.0/0
              tags: 'tomap("Name": ".vpc_name-nodes-sg-.nodepool",
                "kubernetes.io/cluster/.vpc_name": "owned", "karpenter.sh/discovery":
                ".vpc_name-.nodepool")'
          - ssh:
              name: ssh_eks_.vpc_name-nodepool-.nodepool
              description: security group that only enables ssh
              vpc_id: local.vpc_id
              ingress:
                from_port: 22
                to_port: 22
                protocol: TCP
                cidr_blocks: 0.0.0.0/0
              tags:
                Environment: .vpc_name
                Organization: .organization_name
                Name: ssh_eks_.vpc_name-nodepool-.nodepool
                karpenter.sh/discovery: .vpc_name-.nodepool
          aws_security_group_rule:
          - https_nodes_to_plane:
              type: ingress
              from_port: 443
              to_port: 443
              protocol: tcp
              security_group_id: .control_plane_sg
              source_security_group_id: aws_security_group.eks_nodes_sg.id
              depends_on: aws_security_group.eks_nodes_sg
          - communication_plane_to_nodes:
              type: ingress
              from_port: 80
              to_port: 65534
              protocol: tcp
              security_group_id: aws_security_group.eks_nodes_sg.id
              source_security_group_id: .control_plane_sg
              depends_on: aws_security_group.eks_nodes_sg
          - nodes_internode_communications:
              type: ingress
              from_port: 0
              to_port: 0
              protocol: '-1'
              description: allow nodes to communicate with each other
              security_group_id: aws_security_group.eks_nodes_sg.id
              self: true
          - nodes_interpool_communications:
              type: ingress
              from_port: 0
              to_port: 0
              protocol: '-1'
              description: allow default nodes to communicate with each other
              security_group_id: aws_security_group.eks_nodes_sg.id
              source_security_group_id: .default_nodepool_sg
          aws_launch_template:
            eks_launch_template:
              name_prefix: eks-.vpc_name-nodepool-.nodepool
              instance_type: .nodepool_instance_type
              image_id: data.aws_ami.eks_worker.id
              key_name: .ec2_keyname
              iam_instance_profile:
                name: aws_iam_instance_profile.eks_node_instance_profile.name
              network_interfaces:
                associate_public_ip_address: false
                security_groups: aws_security_group.eks_nodes_sg.id aws_security_group.ssh.id
              user_data: 'sensitive(base64encode(templatefile("path.module/../../../../flavors/eks/.bootstrap_script",
                "eks_ca": ".eks_cluster_ca", "eks_endpoint": ".eks_cluster_endpoint",
                "eks_region": "data.aws_region.current.name", "vpc_name": ".vpc_name",
                "ssh_keys": "templatefile("path.module/../../../../files/authorized_keys/ops_team",
                )", "nodepool": ".nodepool", "lifecycle_type": "ONDEMAND",
                "kernel": ".kernel", "activation_id": ".activation_id",
                "customer_id": ".customer_id")))'
              block_device_mappings:
                device_name: /dev/xvda
                ebs:
                  volume_size: .nodepool_worker_drive_size
              tag_specifications:
                resource_type: instance
                tags:
                  Name: eks-.vpc_name-.nodepool
              lifecycle:
                create_before_destroy: true
          aws_autoscaling_group:
            eks_autoscaling_group:
              desired_capacity: .nodepool_asg_desired_capacity
              protect_from_scale_in: .scale_in_protection
              max_size: .nodepool_asg_max_size
              min_size: .nodepool_asg_min_size
              name: eks-.nodepoolworker-node-.vpc_name
              vpc_zone_identifier: flatten([.eks_private_subnets])
              launch_template:
                id: aws_launch_template.eks_launch_template.id
                version: $Latest
              tag:
                key:
                - Environment
                - Name
                - kubernetes.io/cluster/.vpc_name
                - k8s.io/cluster-autoscaler/enabled
                - k8s.io/cluster-type/eks
                - k8s.io/nodepool/.nodepool
                - k8s.io/cluster-autoscaler/node-template/label/role
                - k8s.io/cluster-autoscaler/node-template/taint/role
                value:
                - .vpc_name
                - eks-.vpc_name-.nodepool
                - owned
                - ''
                - ''
                - ''
                - .nodepool
                - .nodepool:NoSchedule
                propagate_at_launch:
                - true
                - true
                - true
                - true
                - true
                - true
                - true
                - true
              lifecycle:
                ignore_changes: desired_capacity
        variable:
          vpc_name: 
          vpc_id:
            default: ''
          ec2_keyname:
            default: someone@uchicago.edu
          instance_type:
            default: t3.large
          nodepool_instance_type:
            default: t3.large
          csoc_cidr:
            default: 10.128.0.0/20
          users_policy: 
          nodepool:
            default: jupyter
          eks_cluster_ca:
            default: ''
          eks_cluster_endpoint:
            default: ''
          eks_private_subnets: 
          control_plane_sg: 
          default_nodepool_sg: 
          deploy_nodepool_pool:
            default: false
          eks_version: 
          kernel:
            default: N/A
          bootstrap_script:
            default: bootstrap-2.0.0.sh
          nodepool_worker_drive_size:
            default: 30
          organization_name:
            default: Basic Service
          nodepool_asg_desired_capacity:
            default: 0
          nodepool_asg_max_size:
            default: 10
          nodepool_asg_min_size:
            default: 0
          activation_id:
            default: ''
          customer_id:
            default: ''
          scale_in_protection:
            description: set scale-in protection on ASG
            default: false
      workflow_pool:
        count: '.deploy_workflow ? 1 : 0'
        scale_in_protection: true
        ec2_keyname: .ec2_keyname
        users_policy: .users_policy
        nodepool: workflow
        vpc_name: .vpc_name
        csoc_cidr: .peering_cidr
        eks_cluster_endpoint: aws_eks_cluster.eks_cluster.endpoint
        eks_cluster_ca: aws_eks_cluster.eks_cluster.certificate_authority[0].data
        eks_private_subnets: local.eks_priv_subnets
        control_plane_sg: aws_security_group.eks_control_plane_sg.id
        default_nodepool_sg: aws_security_group.eks_nodes_sg.id
        eks_version: .eks_version
        nodepool_instance_type: .workflow_instance_type
        kernel: .kernel
        bootstrap_script: .workflow_bootstrap_script
        nodepool_worker_drive_size: .workflow_worker_drive_size
        organization_name: .organization_name
        nodepool_asg_desired_capacity: .workflow_asg_desired_capacity
        nodepool_asg_max_size: .workflow_asg_max_size
        nodepool_asg_min_size: .workflow_asg_min_size
        activation_id: .activation_id
        customer_id: .customer_id
        vpc_id: local.vpc_id
        data:
          aws_caller_identity:
            current: 
          aws_region:
            current: 
          aws_vpcs:
            vpcs:
              tags:
                Name: .vpc_name
          aws_vpc:
            the_vpc:
              id: data.aws_vpcs.vpcs.ids[0]
          aws_availability_zones:
            available:
              state: available
          aws_ami:
            eks_worker:
              filter:
                name: name
                values: amazon-eks-node-.eks_version*
              most_recent: true
              owners: '602401143452'
        locals:
          vpc_id: '.vpc_id != "" ? var.vpc_id : data.aws_vpc.the_vpc.id'
        output:
          nodepool_role:
            value: aws_iam_role.eks_node_role.arn
          nodepool_sg:
            value: aws_security_group.eks_nodes_sg.id
        resource:
          aws_iam_role:
          - eks_control_plane_role:
              name: .vpc_name_EKS_.nodepool_role
              assume_role_policy: "\n  \"Version\": \"2012-10-17\",\n  \"Statement\"\
                : [\n    \n      \"Action\": \"sts:AssumeRole\",\n      \"Principal\"\
                : \n        \"Service\": \"eks.amazonaws.com\"\n      ,\n      \"\
                Effect\": \"Allow\",\n      \"Sid\": \"\"\n    \n  ]\n"
          - eks_node_role:
              name: eks_.vpc_name_nodepool_.nodepool_role
              assume_role_policy: "\n  \"Version\": \"2012-10-17\",\n  \"Statement\"\
                : [\n    \n      \"Effect\": \"Allow\",\n      \"Principal\": \n\
                \        \"Service\": \"ec2.amazonaws.com\"\n      ,\n      \"Action\"\
                : \"sts:AssumeRole\"\n    \n  ]\n"
          aws_iam_role_policy_attachment:
          - eks-policy-AmazonEKSClusterPolicy:
              policy_arn: arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
              role: aws_iam_role.eks_control_plane_role.name
          - eks-policy-AmazonEKSServicePolicy:
              policy_arn: arn:aws:iam::aws:policy/AmazonEKSServicePolicy
              role: aws_iam_role.eks_control_plane_role.name
          - eks-policy-AmazonSSMManagedInstanceCore:
              policy_arn: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
              role: aws_iam_role.eks_control_plane_role.name
          - eks-node-AmazonEKSWorkerNodePolicy:
              policy_arn: arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
              role: aws_iam_role.eks_node_role.name
          - eks-node-AmazonEKS_CNI_Policy:
              policy_arn: arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
              role: aws_iam_role.eks_node_role.name
          - eks-node-AmazonEKSCSIDriverPolicy:
              policy_arn: arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy
              role: aws_iam_role.eks_node_role.name
          - eks-node-AmazonEC2ContainerRegistryReadOnly:
              policy_arn: arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
              role: aws_iam_role.eks_node_role.name
          - cloudwatch_logs_access:
              policy_arn: aws_iam_policy.cwl_access_policy.arn
              role: aws_iam_role.eks_node_role.name
          - asg_access:
              policy_arn: aws_iam_policy.asg_access.arn
              role: aws_iam_role.eks_node_role.name
          - kernel_access:
              policy_arn: aws_iam_policy.access_to_kernels.arn
              role: aws_iam_role.eks_node_role.name
          aws_iam_policy:
          - cwl_access_policy:
              name: .vpc_name_EKS_nodepool_.nodepool_access_to_cloudwatchlogs
              description: In order to avoid the creation of users and keys, we are
                using roles and policies.
              policy: "\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n\
                \        \n            \"Effect\": \"Allow\",\n            \"Action\"\
                : \"logs:DescribeLogGroups\",\n            \"Resource\": \"arn:aws:logs:data.aws_region.current.name:data.aws_caller_identity.current.account_id:log-group::log-stream:*\"\
                \n        ,\n        \n            \"Effect\": \"Allow\",\n    \
                \        \"Action\": [\n                \"logs:CreateLogStream\",\n\
                \                \"logs:PutLogEvents\",\n                \"logs:DescribeLogStreams\"\
                \n            ],\n            \"Resource\": \"arn:aws:logs:data.aws_region.current.name:data.aws_caller_identity.current.account_id:log-group:.vpc_name:log-stream:*\"\
                \n        \n    ]\n"
          - access_to_kernels:
              name: .vpc_name_EKS_nodepool_.nodepool_kernel_access
              description: To access custom Kernels
              policy: "\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n\
                \        \n            \"Sid\": \"\",\n            \"Effect\": \"\
                Allow\",\n            \"Action\": [\n                \"s3:List*\"\
                ,\n                \"s3:Get*\"\n            ],\n            \"Resource\"\
                : [\n                \"arn:aws:s3:::gen3-kernels/*\",\n          \
                \      \"arn:aws:s3:::gen3-kernels\",\n                \"arn:aws:s3:::qualys-agentpackage\"\
                ,\n                \"arn:aws:s3:::qualys-agentpackage/*\"\n      \
                \      ]\n        \n    ]\n"
          - asg_access:
              name: .vpc_name_EKS_nodepool_.nodepool_autoscaling_access
              description: Allow the deployment cluster-autoscaler to add or terminate
                instances accordingly
              policy: "\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n\
                \        \n            \"Effect\": \"Allow\",\n            \"Action\"\
                : [\n                \"autoscaling:DescribeAutoScalingGroups\",\n\
                \                \"autoscaling:DescribeAutoScalingInstances\",\n \
                \               \"autoscaling:DescribeTags\",\n                \"\
                autoscaling:SetDesiredCapacity\",\n                \"autoscaling:TerminateInstanceInAutoScalingGroup\"\
                ,\n                \"autoscaling:DescribeLaunchConfigurations\"\n\
                \            ],\n            \"Resource\": \"*\"\n        \n    ]\n\
                "
          aws_iam_instance_profile:
            eks_node_instance_profile:
              name: .vpc_name_EKS_nodepool_.nodepool
              role: aws_iam_role.eks_node_role.name
          aws_security_group:
          - eks_nodes_sg:
              name: .vpc_name_EKS_nodepool_.nodepool_sg
              description: 'Security group for all nodes in pool .nodepool in
                the EKS cluster [.vpc_name] '
              vpc_id: local.vpc_id
              egress:
                from_port: 0
                to_port: 0
                protocol: '-1'
                cidr_blocks: 0.0.0.0/0
              tags: 'tomap("Name": ".vpc_name-nodes-sg-.nodepool",
                "kubernetes.io/cluster/.vpc_name": "owned", "karpenter.sh/discovery":
                ".vpc_name-.nodepool")'
          - ssh:
              name: ssh_eks_.vpc_name-nodepool-.nodepool
              description: security group that only enables ssh
              vpc_id: local.vpc_id
              ingress:
                from_port: 22
                to_port: 22
                protocol: TCP
                cidr_blocks: 0.0.0.0/0
              tags:
                Environment: .vpc_name
                Organization: .organization_name
                Name: ssh_eks_.vpc_name-nodepool-.nodepool
                karpenter.sh/discovery: .vpc_name-.nodepool
          aws_security_group_rule:
          - https_nodes_to_plane:
              type: ingress
              from_port: 443
              to_port: 443
              protocol: tcp
              security_group_id: .control_plane_sg
              source_security_group_id: aws_security_group.eks_nodes_sg.id
              depends_on: aws_security_group.eks_nodes_sg
          - communication_plane_to_nodes:
              type: ingress
              from_port: 80
              to_port: 65534
              protocol: tcp
              security_group_id: aws_security_group.eks_nodes_sg.id
              source_security_group_id: .control_plane_sg
              depends_on: aws_security_group.eks_nodes_sg
          - nodes_internode_communications:
              type: ingress
              from_port: 0
              to_port: 0
              protocol: '-1'
              description: allow nodes to communicate with each other
              security_group_id: aws_security_group.eks_nodes_sg.id
              self: true
          - nodes_interpool_communications:
              type: ingress
              from_port: 0
              to_port: 0
              protocol: '-1'
              description: allow default nodes to communicate with each other
              security_group_id: aws_security_group.eks_nodes_sg.id
              source_security_group_id: .default_nodepool_sg
          aws_launch_template:
            eks_launch_template:
              name_prefix: eks-.vpc_name-nodepool-.nodepool
              instance_type: .nodepool_instance_type
              image_id: data.aws_ami.eks_worker.id
              key_name: .ec2_keyname
              iam_instance_profile:
                name: aws_iam_instance_profile.eks_node_instance_profile.name
              network_interfaces:
                associate_public_ip_address: false
                security_groups: aws_security_group.eks_nodes_sg.id aws_security_group.ssh.id
              user_data: 'sensitive(base64encode(templatefile("path.module/../../../../flavors/eks/.bootstrap_script",
                "eks_ca": ".eks_cluster_ca", "eks_endpoint": ".eks_cluster_endpoint",
                "eks_region": "data.aws_region.current.name", "vpc_name": ".vpc_name",
                "ssh_keys": "templatefile("path.module/../../../../files/authorized_keys/ops_team",
                )", "nodepool": ".nodepool", "lifecycle_type": "ONDEMAND",
                "kernel": ".kernel", "activation_id": ".activation_id",
                "customer_id": ".customer_id")))'
              block_device_mappings:
                device_name: /dev/xvda
                ebs:
                  volume_size: .nodepool_worker_drive_size
              tag_specifications:
                resource_type: instance
                tags:
                  Name: eks-.vpc_name-.nodepool
              lifecycle:
                create_before_destroy: true
          aws_autoscaling_group:
            eks_autoscaling_group:
              desired_capacity: .nodepool_asg_desired_capacity
              protect_from_scale_in: .scale_in_protection
              max_size: .nodepool_asg_max_size
              min_size: .nodepool_asg_min_size
              name: eks-.nodepoolworker-node-.vpc_name
              vpc_zone_identifier: flatten([.eks_private_subnets])
              launch_template:
                id: aws_launch_template.eks_launch_template.id
                version: $Latest
              tag:
                key:
                - Environment
                - Name
                - kubernetes.io/cluster/.vpc_name
                - k8s.io/cluster-autoscaler/enabled
                - k8s.io/cluster-type/eks
                - k8s.io/nodepool/.nodepool
                - k8s.io/cluster-autoscaler/node-template/label/role
                - k8s.io/cluster-autoscaler/node-template/taint/role
                value:
                - .vpc_name
                - eks-.vpc_name-.nodepool
                - owned
                - ''
                - ''
                - ''
                - .nodepool
                - .nodepool:NoSchedule
                propagate_at_launch:
                - true
                - true
                - true
                - true
                - true
                - true
                - true
                - true
              lifecycle:
                ignore_changes: desired_capacity
        variable:
          vpc_name: 
          vpc_id:
            default: ''
          ec2_keyname:
            default: someone@uchicago.edu
          instance_type:
            default: t3.large
          nodepool_instance_type:
            default: t3.large
          csoc_cidr:
            default: 10.128.0.0/20
          users_policy: 
          nodepool:
            default: jupyter
          eks_cluster_ca:
            default: ''
          eks_cluster_endpoint:
            default: ''
          eks_private_subnets: 
          control_plane_sg: 
          default_nodepool_sg: 
          deploy_nodepool_pool:
            default: false
          eks_version: 
          kernel:
            default: N/A
          bootstrap_script:
            default: bootstrap-2.0.0.sh
          nodepool_worker_drive_size:
            default: 30
          organization_name:
            default: Basic Service
          nodepool_asg_desired_capacity:
            default: 0
          nodepool_asg_max_size:
            default: 10
          nodepool_asg_min_size:
            default: 0
          activation_id:
            default: ''
          customer_id:
            default: ''
          scale_in_protection:
            description: set scale-in protection on ASG
            default: false
    locals:
      azs: '.availability_zones != 0 ? var.availability_zones : data.aws_availability_zones.available.names'
      secondary_azs: '.secondary_availability_zones != 0 ? var.secondary_availability_zones
        : data.aws_availability_zones.available.names'
      ami: '.fips ? var.fips_enabled_ami : data.aws_ami.eks_worker.id'
      eks_priv_subnets: '.secondary_cidr_block != "" ? aws_subnet.eks_secondary_subnet.*.id
        : aws_subnet.eks_private.*.id'
      vpc_id: '.vpc_id != "" ? var.vpc_id : data.aws_vpc.the_vpc.id'
      config-map-aws-auth: "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: aws-auth\n\
        \  namespace: kube-system\ndata:\n  mapRoles: |\n    - rolearn: aws_iam_role.eks_node_role.arn\n\
        \      username: system:node:EC2PrivateDNSName\n      groups:\n      \
        \  - system:bootstrappers\n        - system:nodes\n    - rolearn: aws_iam_role.karpenter[0].arn\
        \ \n      username: system:node:SessionName\n      groups:\n        -\
        \ system:bootstrappers\n        - system:nodes\n        - system:node-proxier\n\
        \    - rolearn: .deploy_workflow ? module.workflow_pool[0].nodepool_role\
        \ : \"\"\n      username: system:node:EC2PrivateDNSName\n      groups:\n\
        \        - system:bootstrappers\n        - system:nodes\n    - rolearn: .deploy_jupyter\
        \ ? module.jupyter_pool[0].nodepool_role : \"\"\n      username: system:node:EC2PrivateDNSName\n\
        \      groups:\n        - system:bootstrappers\n        - system:nodes"
      account_id: data.aws_caller_identity.current.account_id
      create_irsa: true
      irsa_name: .vpc_name-karpenter-sa
      irsa_policy_name: local.irsa_name
      irsa_oidc_provider_url: replace(aws_eks_cluster.eks_cluster.identity[0].oidc[0].issuer,
        "https://", "")
      irsa_tag_values: aws_eks_cluster.eks_cluster.id
      enable_spot_termination: true
      queue_name: .vpc_name-aws_eks_cluster.eks_cluster.id
      events:
        health_event:
          name: HealthEvent
          description: Karpenter interrupt - AWS health event
          event_pattern:
            source: aws.health
            detail-type: AWS Health Event
        spot_interupt:
          name: SpotInterrupt
          description: Karpenter interrupt - EC2 spot instance interruption warning
          event_pattern:
            source: aws.ec2
            detail-type: EC2 Spot Instance Interruption Warning
        instance_rebalance:
          name: InstanceRebalance
          description: Karpenter interrupt - EC2 instance rebalance recommendation
          event_pattern:
            source: aws.ec2
            detail-type: EC2 Instance Rebalance Recommendation
        instance_state_change:
          name: InstanceStateChange
          description: Karpenter interrupt - EC2 instance state-change notification
          event_pattern:
            source: aws.ec2
            detail-type: EC2 Instance State-change Notification
    output:
      kubeconfig:
        value: 'templatefile("path.module/kubeconfig.tpl", "vpc_name": ".vpc_name",
          "eks_name": "aws_eks_cluster.eks_cluster.id", "eks_endpoint": "aws_eks_cluster.eks_cluster.endpoint",
          "eks_cert": "aws_eks_cluster.eks_cluster.certificate_authority[0].data")'
        sensitive: true
      config_map_aws_auth:
        value: local.config-map-aws-auth
        sensitive: true
      cluster_endpoint:
        value: aws_eks_cluster.eks_cluster.endpoint
        sensitive: true
      cluster_certificate_authority_data:
        value: aws_eks_cluster.eks_cluster.certificate_authority[0].data
        sensitive: true
      cluster_name:
        value: aws_eks_cluster.eks_cluster.name
      oidc_provider_arn:
        value: replace(aws_eks_cluster.eks_cluster.identity[0].oidc[0].issuer, "https://",
          "")
      cluster_oidc_provider_url:
        value: aws_eks_cluster.eks_cluster.identity[0].oidc[0].issuer
      cluster_oidc_provider_arn:
        value: aws_iam_openid_connect_provider.identity_provider[0].arn
    resource:
      aws_launch_template:
        eks_launch_template:
          name_prefix: eks-.vpc_name
          image_id: local.ami
          instance_type: .instance_type
          key_name: .ec2_keyname
          iam_instance_profile:
            name: aws_iam_instance_profile.eks_node_instance_profile.name
          network_interfaces:
            associate_public_ip_address: false
            security_groups: aws_security_group.eks_nodes_sg.id aws_security_group.ssh.id
          user_data: 'sensitive(base64encode(templatefile("path.module/../../../../flavors/eks/.bootstrap_script",
            "eks_ca": "aws_eks_cluster.eks_cluster.certificate_authority[0].data",
            "eks_endpoint": "aws_eks_cluster.eks_cluster.endpoint", "eks_region":
            "data.aws_region.current.name", "vpc_name": ".vpc_name", "ssh_keys":
            "templatefile("path.module/../../../../files/authorized_keys/ops_team",
            )", "nodepool": "default", "lifecycle_type": "ONDEMAND", "activation_id":
            ".activation_id", "customer_id": ".customer_id")))'
          block_device_mappings:
            device_name: /dev/xvda
            ebs:
              volume_size: .worker_drive_size
          tag_specifications:
            resource_type: instance
            tags:
              Name: eks-.vpc_name-node
          lifecycle:
            create_before_destroy: true
      aws_autoscaling_group:
        eks_autoscaling_group:
          count: '.use_asg ? 1 : 0'
          service_linked_role_arn: aws_iam_service_linked_role.autoscaling.arn
          desired_capacity: 2
          max_size: 10
          min_size: 2
          name: eks-worker-node-.vpc_name
          vpc_zone_identifier: flatten([aws_subnet.eks_private.*.id])
          launch_template:
            id: aws_launch_template.eks_launch_template.id
            version: $Latest
          tag:
            key:
            - Environment
            - Name
            - kubernetes.io/cluster/.vpc_name
            - k8s.io/cluster-autoscaler/enabled
            - k8s.io/cluster-type/eks
            - k8s.io/nodepool/default
            value:
            - .vpc_name
            - eks-.vpc_name
            - owned
            - ''
            - ''
            - ''
            propagate_at_launch:
            - true
            - true
            - true
            - true
            - true
            - true
          lifecycle:
            ignore_changes: desired_capacity max_size min_size
      aws_iam_role:
      - eks_control_plane_role:
          name: .vpc_name_EKS_role
          assume_role_policy: "\n  \"Version\": \"2012-10-17\",\n  \"Statement\"\
            : [\n    \n      \"Action\": \"sts:AssumeRole\",\n      \"Principal\"\
            : \n        \"Service\": \"eks.amazonaws.com\"\n      ,\n      \"Effect\"\
            : \"Allow\",\n      \"Sid\": \"\"\n    \n  ]\n"
      - eks_node_role:
          name: eks_.vpc_name_workers_role
          assume_role_policy: "\n  \"Version\": \"2012-10-17\",\n  \"Statement\"\
            : [\n    \n      \"Effect\": \"Allow\",\n      \"Principal\": \n   \
            \     \"Service\": \"ec2.amazonaws.com\"\n      ,\n      \"Action\":\
            \ \"sts:AssumeRole\"\n    \n  ]\n"
          tags:
            Environment: .vpc_name
            Organization: .organization_name
      - irsa:
          count: '.use_karpenter ? 1 : 0'
          name: local.irsa_name
          assume_role_policy: data.aws_iam_policy_document.irsa_assume_role[0].json
          force_detach_policies: true
      - karpenter:
          count: '.use_karpenter ? 1 : 0'
          name: .vpc_name-karpenter-fargate-role
          assume_role_policy: 'jsonencode("Statement": ["Action": "sts:AssumeRole",
            "Effect": "Allow", "Principal": "Service": "eks-fargate-pods.amazonaws.com"],
            "Version": "2012-10-17")'
      aws_iam_role_policy_attachment:
      - eks-policy-AmazonEKSClusterPolicy:
          policy_arn: arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
          role: aws_iam_role.eks_control_plane_role.name
      - eks-policy-AmazonEKSServicePolicy:
          policy_arn: arn:aws:iam::aws:policy/AmazonEKSServicePolicy
          role: aws_iam_role.eks_control_plane_role.name
      - eks-node-AmazonEKSWorkerNodePolicy:
          policy_arn: arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
          role: aws_iam_role.eks_node_role.name
      - eks-node-AmazonEKS_CNI_Policy:
          policy_arn: arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
          role: aws_iam_role.eks_node_role.name
      - eks-node-AmazonEKSCSIDriverPolicy:
          policy_arn: arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy
          role: aws_iam_role.eks_node_role.name
      - eks-node-AmazonEC2ContainerRegistryReadOnly:
          policy_arn: arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
          role: aws_iam_role.eks_node_role.name
      - cloudwatch_logs_access:
          policy_arn: aws_iam_policy.cwl_access_policy.arn
          role: aws_iam_role.eks_node_role.name
      - asg_access:
          policy_arn: aws_iam_policy.asg_access.arn
          role: aws_iam_role.eks_node_role.name
      - bucket_read:
          policy_arn: arn:aws:iam::data.aws_caller_identity.current.account_id:policy/bucket_reader_cdis-gen3-users_.users_policy
          role: aws_iam_role.eks_node_role.name
      - eks-policy-AmazonSSMManagedInstanceCore:
          policy_arn: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
          role: aws_iam_role.eks_node_role.name
      - irsa:
          count: '.use_karpenter ? 1 : 0'
          role: aws_iam_role.irsa[0].name
          policy_arn: aws_iam_policy.irsa[0].arn
      - karpenter-role-policy:
          count: '.use_karpenter ? 1 : 0'
          policy_arn: arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy
          role: aws_iam_role.karpenter[0].name
      - lambda_logs:
          count: '.ha_squid ? 1 : 0'
          role: module.iam_role[0].role_id
          policy_arn: module.iam_policy[0].arn
      random_shuffle:
      - az:
          count: 1
          input: local.azs
          result_count: length(local.azs)
      - secondary_az:
          count: 1
          input: local.secondary_azs
          result_count: length(local.secondary_azs)
      aws_subnet:
      - eks_private:
          count: random_shuffle.az[0].result_count
          vpc_id: local.vpc_id
          cidr_block: '.workers_subnet_size == 22 ? cidrsubnet(data.aws_vpc.the_vpc.cidr_block,
            2, (1 + count.index)) : var.workers_subnet_size == 23 ? cidrsubnet(data.aws_vpc.the_vpc.cidr_block,
            3, (2 + count.index)) : cidrsubnet(data.aws_vpc.the_vpc.cidr_block, 4,
            (7 + count.index))'
          availability_zone: random_shuffle.az[0].result[count.index]
          map_public_ip_on_launch: false
          lifecycle:
            ignore_changes: tags availability_zone cidr_block
          tags: 'tomap("Name": "eks_private_count.index", "Environment": ".vpc_name",
            "Organization": ".organization_name", "kubernetes.io/cluster/.vpc_name":
            "owned", "kubernetes.io/role/internal-elb": "1", "karpenter.sh/discovery":
            ".vpc_name")'
      - eks_secondary_subnet:
          count: '.secondary_cidr_block != "" ? 4 : 0'
          vpc_id: local.vpc_id
          cidr_block: cidrsubnet(var.secondary_cidr_block, 2, count.index)
          availability_zone: random_shuffle.secondary_az[0].result[count.index]
          map_public_ip_on_launch: false
          lifecycle:
            ignore_changes: tags availability_zone cidr_block
          tags: 'tomap("Name": "eks_secondary_cidr_subnet_count.index", "Environment":
            ".vpc_name", "Organization": ".organization_name", "kubernetes.io/cluster/.vpc_name":
            "owned", "kubernetes.io/role/internal-elb": "1", "karpenter.sh/discovery":
            ".vpc_name")'
      - eks_public:
          count: random_shuffle.az[0].result_count
          vpc_id: local.vpc_id
          cidr_block: '.workers_subnet_size == 22 ? cidrsubnet(data.aws_vpc.the_vpc.cidr_block,
            5, (4 + count.index)) : cidrsubnet(data.aws_vpc.the_vpc.cidr_block, 4,
            (10 + count.index))'
          map_public_ip_on_launch: true
          availability_zone: random_shuffle.az[0].result[count.index]
          lifecycle:
            ignore_changes: tags availability_zone cidr_block
          tags: 'tomap("Name": "eks_public_count.index", "Environment": ".vpc_name",
            "Organization": ".organization_name", "kubernetes.io/cluster/.vpc_name":
            "shared", "kubernetes.io/role/elb": "1", "KubernetesCluster": ".vpc_name")'
      aws_route_table:
        eks_private:
          vpc_id: local.vpc_id
          lifecycle: 
          tags:
            Name: eks_private
            Environment: .vpc_name
            Organization: .organization_name
      aws_route:
      - for_peering:
          route_table_id: aws_route_table.eks_private.id
          destination_cidr_block: .peering_cidr
          vpc_peering_connection_id: data.aws_vpc_peering_connection.pc.id
      - skip_proxy:
          count: length(var.cidrs_to_route_to_gw)
          route_table_id: aws_route_table.eks_private.id
          destination_cidr_block: element(var.cidrs_to_route_to_gw, count.index)
          nat_gateway_id: data.aws_nat_gateway.the_gateway.id
          depends_on: aws_route_table.eks_private
      - public_access:
          count: '.ha_squid ? var.dual_proxy ? 1 : 0 : 1'
          destination_cidr_block: 0.0.0.0/0
          route_table_id: aws_route_table.eks_private.id
          network_interface_id: data.aws_instances.squid_proxy[count.index].ids[0]
      aws_route_table_association:
      - private_kube:
          count: random_shuffle.az[0].result_count
          subnet_id: aws_subnet.eks_private.*.id[count.index]
          route_table_id: aws_route_table.eks_private.id
          depends_on: aws_subnet.eks_private
      - secondary_subnet_kube:
          count: '.secondary_cidr_block != "" ? random_shuffle.secondary_az[0].result_count
            : 0'
          subnet_id: aws_subnet.eks_secondary_subnet.*.id[count.index]
          route_table_id: aws_route_table.eks_private.id
          depends_on: aws_subnet.eks_secondary_subnet
      - public_kube:
          count: random_shuffle.az[0].result_count
          subnet_id: aws_subnet.eks_public.*.id[count.index]
          route_table_id: data.aws_route_table.public_kube.id
          lifecycle: 
      aws_security_group:
      - eks_control_plane_sg:
          name: .vpc_name-control-plane
          description: Cluster communication with worker nodes [.vpc_name]
          vpc_id: local.vpc_id
          egress:
            from_port: 0
            to_port: 0
            protocol: '-1'
            cidr_blocks: 0.0.0.0/0
          tags:
            Name: .vpc_name-control-plane-sg
            Environment: .vpc_name
            Organization: .organization_name
      - eks_nodes_sg:
          name: .vpc_name_EKS_workers_sg
          description: 'Security group for all nodes in the EKS cluster [.vpc_name] '
          vpc_id: local.vpc_id
          egress:
            from_port: 0
            to_port: 0
            protocol: '-1'
            cidr_blocks: 0.0.0.0/0
          tags: 'tomap("Name": ".vpc_name-nodes-sg", "kubernetes.io/cluster/.vpc_name":
            "owned", "karpenter.sh/discovery": ".vpc_name")'
      - ssh:
          name: ssh_eks_.vpc_name
          description: security group that only enables ssh
          vpc_id: local.vpc_id
          ingress:
            from_port: 22
            to_port: 22
            protocol: TCP
            cidr_blocks: 0.0.0.0/0
          tags: 'tomap("Environment": ".vpc_name", "Organization": ".organization_name",
            "Name": "ssh_eks_.vpc_name", "karpenter.sh/discovery": ".vpc_name")'
      aws_eks_cluster:
        eks_cluster:
          name: .vpc_name
          role_arn: aws_iam_role.eks_control_plane_role.arn
          version: .eks_version
          vpc_config:
            subnet_ids: flatten([aws_subnet.eks_private[*].id])
            security_group_ids: aws_security_group.eks_control_plane_sg.id
            endpoint_private_access: 'true'
            endpoint_public_access: .eks_public_access
          depends_on: aws_iam_role_policy_attachment.eks-policy-AmazonEKSClusterPolicy
            aws_iam_role_policy_attachment.eks-policy-AmazonEKSServicePolicy aws_subnet.eks_private
          tags:
            Environment: .vpc_name
            Organization: .organization_name
      aws_iam_policy:
      - cwl_access_policy:
          name: .vpc_name_EKS_workers_access_to_cloudwatchlogs
          description: In order to avoid the creation of users and keys, we are using
            roles and policies.
          policy: "\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n   \
            \     \n            \"Effect\": \"Allow\",\n            \"Action\": \"\
            logs:DescribeLogGroups\",\n            \"Resource\": \"arn:aws:logs:data.aws_region.current.name:data.aws_caller_identity.current.account_id:log-group::log-stream:*\"\
            \n        ,\n        \n            \"Effect\": \"Allow\",\n        \
            \    \"Action\": [\n                \"logs:CreateLogStream\",\n      \
            \          \"logs:PutLogEvents\",\n                \"logs:DescribeLogStreams\"\
            \n            ],\n            \"Resource\": \"arn:aws:logs:data.aws_region.current.name:data.aws_caller_identity.current.account_id:log-group:.vpc_name:log-stream:*\"\
            \n        \n    ]\n"
      - asg_access:
          name: .vpc_name_EKS_workers_autoscaling_access
          description: Allow the deployment cluster-autoscaler to add or terminate
            instances accordingly
          policy: "\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n   \
            \     \n            \"Effect\": \"Allow\",\n            \"Action\": [\n\
            \                \"autoscaling:DescribeAutoScalingGroups\",\n        \
            \        \"autoscaling:DescribeAutoScalingInstances\",\n             \
            \   \"autoscaling:DescribeTags\",\n                \"autoscaling:SetDesiredCapacity\"\
            ,\n                \"autoscaling:TerminateInstanceInAutoScalingGroup\"\
            ,\n                \"autoscaling:DescribeLaunchConfigurations\"\n    \
            \        ],\n            \"Resource\": \"*\"\n        \n    ]\n"
      - irsa:
          count: '.use_karpenter ? 1 : 0'
          name: local.irsa_policy_name-policy
          policy: data.aws_iam_policy_document.irsa[0].json
      aws_iam_role_policy:
      - csoc_alert_sns_access:
          count: '.sns_topic_arn != "" ? 1 : 0'
          name: .vpc_name_CSOC_alert_SNS_topic_acess
          policy: data.aws_iam_policy_document.planx-csoc-alerts-topic_access[count.index].json
          role: aws_iam_role.eks_node_role.id
      - lambda_policy_resources:
          count: '.ha_squid ? 1 : 0'
          name: resources_acces
          policy: data.aws_iam_policy_document.with_resources.json
          role: module.iam_role[0].role_id
      - lambda_policy_no_resources:
          count: '.ha_squid ? 1 : 0'
          name: no_resources_acces
          policy: data.aws_iam_policy_document.without_resources.json
          role: module.iam_role[0].role_id
      aws_iam_instance_profile:
        eks_node_instance_profile:
          name: .vpc_name_EKS_workers
          role: aws_iam_role.eks_node_role.name
      aws_security_group_rule:
      - https_nodes_to_plane:
          type: ingress
          from_port: 443
          to_port: 443
          protocol: tcp
          security_group_id: aws_security_group.eks_control_plane_sg.id
          source_security_group_id: aws_security_group.eks_nodes_sg.id
          depends_on: aws_security_group.eks_nodes_sg aws_security_group.eks_control_plane_sg
          description: from the workers to the control plane
      - https_csoc_to_plane:
          count: '.csoc_managed ? 1 : 0'
          type: ingress
          from_port: 443
          to_port: 443
          protocol: tcp
          security_group_id: aws_security_group.eks_control_plane_sg.id
          cidr_blocks: .peering_cidr
          depends_on: aws_security_group.eks_nodes_sg aws_security_group.eks_control_plane_sg
          description: from the CSOC to the control plane
      - communication_plane_to_nodes:
          type: ingress
          from_port: 80
          to_port: 65534
          protocol: tcp
          security_group_id: aws_security_group.eks_nodes_sg.id
          source_security_group_id: aws_security_group.eks_control_plane_sg.id
          depends_on: aws_security_group.eks_nodes_sg aws_security_group.eks_control_plane_sg
          description: from the control plane to the nodes
      - nodes_internode_communications:
          type: ingress
          from_port: 0
          to_port: 0
          protocol: '-1'
          description: allow nodes to communicate with each other
          security_group_id: aws_security_group.eks_nodes_sg.id
          self: true
      - nodes_interpool_communications:
          count: '.deploy_jupyter ? 1 : 0'
          type: ingress
          from_port: 0
          to_port: 0
          protocol: '-1'
          description: allow jupyter nodes to talk to the default
          security_group_id: aws_security_group.eks_nodes_sg.id
          source_security_group_id: module.jupyter_pool[0].nodepool_sg
      - workflow_nodes_interpool_communications:
          count: '.deploy_workflow ? 1 : 0'
          type: ingress
          from_port: 0
          to_port: 0
          protocol: '-1'
          description: allow workflow nodes to talk to the default
          security_group_id: aws_security_group.eks_nodes_sg.id
          source_security_group_id: module.workflow_pool[0].nodepool_sg
      aws_iam_service_linked_role:
      - autoscaling:
          aws_service_name: autoscaling.amazonaws.com
          custom_suffix: .vpc_name
          lifecycle:
            ignore_changes: custom_suffix
      - ec2_spot:
          count: '.use_karpenter && var.spot_linked_role ? 1 : 0'
          aws_service_name: spot.amazonaws.com
          description: Service-linked role for EC2 Spot Instances
      aws_kms_grant:
        kms:
          count: '.fips ? 1 : 0'
          name: kms-cmk-eks
          key_id: .fips_ami_kms
          grantee_principal: aws_iam_service_linked_role.autoscaling.arn
          operations: Encrypt Decrypt ReEncryptFrom ReEncryptTo GenerateDataKey GenerateDataKeyWithoutPlaintext
            DescribeKey CreateGrant
      kubectl_manifest:
      - aws-auth:
          count: '.k8s_bootstrap_resources ? 1 : 0'
          yaml_body: local.config-map-aws-auth
      - karpenter_node_pool:
          count: '.k8s_bootstrap_resources && var.use_karpenter && var.deploy_karpenter_in_k8s
            ? 1 : 0'
          yaml_body: "---\napiVersion: karpenter.sh/v1beta1\nkind: NodePool\nmetadata:\n\
            \  name: default\nspec:\n  disruption:\n    consolidateAfter: 30s\n  \
            \  consolidationPolicy: WhenEmpty\n    expireAfter: \"168h\"\n  limits:\n\
            \    cpu: \"1000\"\n    memory: 1000Gi\n  template:\n    metadata:\n \
            \     labels:\n        role: default\n    spec:\n      kubelet:\n    \
            \    evictionHard:\n          memory.available: 5%\n        evictionSoft:\n\
            \          memory.available: 10%\n        evictionSoftGracePeriod:\n \
            \         memory.available: 5m\n        kubeReserved:\n          cpu:\
            \ 480m\n          ephemeral-storage: 3Gi\n          memory: 1632Mi\n \
            \     nodeClassRef:\n        apiVersion: karpenter.k8s.aws/v1beta1\n \
            \       kind: EC2NodeClass\n        name: default\n      requirements:\n\
            \      - key: karpenter.sh/capacity-type\n        operator: In\n     \
            \   values:\n        - on-demand\n        - spot\n      - key: kubernetes.io/arch\n\
            \        operator: In\n        values:\n        - amd64\n      - key:\
            \ karpenter.k8s.aws/instance-category\n        operator: In\n        values:\n\
            \        - c\n        - m\n        - r\n        - t"
          depends_on: helm_release.karpenter
          lifecycle:
            ignore_changes: all
      - karpenter_node_class:
          count: '.k8s_bootstrap_resources && var.use_karpenter && var.deploy_karpenter_in_k8s
            ? 1 : 0'
          yaml_body: "    ---\n    apiVersion: karpenter.k8s.aws/v1beta1\n    kind:\
            \ EC2NodeClass\n    metadata:\n      name: default\n    spec:\n      amiFamily:\
            \ AL2\n      amiSelectorTerms:\n      - name: \"EKS-FIPS*\"\n        owner:\
            \ \"143731057154\"\n      blockDeviceMappings:\n      - deviceName: /dev/xvda\n\
            \        ebs:\n          deleteOnTermination: true\n          encrypted:\
            \ true\n          volumeSize: .worker_drive_sizeGi\n          volumeType:\
            \ gp3\n      metadataOptions:\n        httpEndpoint: enabled\n       \
            \ httpProtocolIPv6: disabled\n        httpPutResponseHopLimit: 2\n   \
            \     httpTokens: optional\n      role: eks_.vpc_name_workers_role\n\
            \n      securityGroupSelectorTerms:\n      - tags:\n          karpenter.sh/discovery:\
            \ .vpc_name\n\n      subnetSelectorTerms:\n      - tags:\n     \
            \     karpenter.sh/discovery: .vpc_name\n\n      tags:\n       \
            \ Environment: .vpc_name\n        Name: eks-.vpc_name-karpenter\n\
            \        karpenter.sh/discovery: .vpc_name\n        purpose: default\n\
            \n      userData: |\n        MIME-Version: 1.0\n        Content-Type:\
            \ multipart/mixed; boundary=\"BOUNDARY\"\n\n        --BOUNDARY\n     \
            \   Content-Type: text/x-shellscript; charset=\"us-ascii\"\n\n       \
            \ #!/bin/bash -x\n        instanceId=$(curl -s http://169.254.169.254/latest/dynamic/instance-identity/document\
            \ | jq -r .instanceId)\n        curl https://raw.githubusercontent.com/uc-cdis/cloud-automation/master/files/authorized_keys/ops_team\
            \ >> /home/ec2-user/.ssh/authorized_keys\n        echo \"$(jq '.registryPullQPS=0'\
            \ /etc/kubernetes/kubelet/kubelet-config.json)\" > /etc/kubernetes/kubelet/kubelet-config.json\n\
            \        sysctl -w fs.inotify.max_user_watches=12000\n\n        sudo yum\
            \ update -y\n\n        --BOUNDARY--"
          depends_on: helm_release.karpenter
          lifecycle:
            ignore_changes: all
      null_resource:
        config_setup:
          triggers:
            kubeconfig_change: 'sensitive(templatefile("path.module/kubeconfig.tpl",
              "vpc_name": ".vpc_name", "eks_name": "aws_eks_cluster.eks_cluster.id",
              "eks_endpoint": "aws_eks_cluster.eks_cluster.endpoint", "eks_cert":
              "aws_eks_cluster.eks_cluster.certificate_authority[0].data"))'
            configmap_change: sensitive(local.config-map-aws-auth)
          provisioner:
            local-exec:
            - command: mkdir -p .vpc_name_output_EKS; echo 'templatefile("path.module/kubeconfig.tpl",
                vpc_name = var.vpc_name, eks_name = aws_eks_cluster.eks_cluster.id,
                eks_endpoint = aws_eks_cluster.eks_cluster.endpoint, eks_cert = aws_eks_cluster.eks_cluster.certificate_authority.0.data,)'
                >.vpc_name_output_EKS/kubeconfig
            - command: echo "local.config-map-aws-auth" > .vpc_name_output_EKS/aws-auth-cm.yaml
            - command: echo "templatefile("path.module/init_cluster.sh",  vpc_name
                = var.vpc_name, kubeconfig_path = ".vpc_name_output_EKS/kubeconfig",
                auth_configmap = ".vpc_name_output_EKS/aws-auth-cm.yaml")"
                > .vpc_name_output_EKS/init_cluster.sh
          depends_on: aws_autoscaling_group.eks_autoscaling_group
      aws_iam_openid_connect_provider:
        identity_provider:
          count: '.iam-serviceaccount ? var.eks_version == "1.12" ? 0 : 1 : 0'
          url: aws_eks_cluster.eks_cluster.identity.0.oidc.0.issuer
          client_id_list: sts.amazonaws.com
          thumbprint_list: .oidc_eks_thumbprint
          depends_on: aws_eks_cluster.eks_cluster
      aws_vpc_endpoint:
      - ec2:
          count: '.enable_vpc_endpoints ? 1 : 0'
          vpc_id: data.aws_vpc.the_vpc.id
          service_name: data.aws_vpc_endpoint_service.ec2.service_name
          vpc_endpoint_type: Interface
          security_group_ids: data.aws_security_group.local_traffic.id
          private_dns_enabled: true
          subnet_ids: flatten([aws_subnet.eks_private[*].id])
          tags:
            Name: to ec2
            Environment: .vpc_name
            Organization: .organization_name
          lifecycle:
            ignore_changes: all
      - sts:
          count: '.enable_vpc_endpoints ? 1 : 0'
          vpc_id: data.aws_vpc.the_vpc.id
          service_name: data.aws_vpc_endpoint_service.sts.service_name
          vpc_endpoint_type: Interface
          security_group_ids: data.aws_security_group.local_traffic.id
          private_dns_enabled: true
          subnet_ids: flatten([aws_subnet.eks_private[*].id])
          tags:
            Name: to sts
            Environment: .vpc_name
            Organization: .organization_name
          lifecycle:
            ignore_changes: all
      - autoscaling:
          count: '.enable_vpc_endpoints ? 1 : 0'
          vpc_id: data.aws_vpc.the_vpc.id
          service_name: data.aws_vpc_endpoint_service.autoscaling.service_name
          vpc_endpoint_type: Interface
          security_group_ids: data.aws_security_group.local_traffic.id
          private_dns_enabled: true
          subnet_ids: flatten([aws_subnet.eks_private[*].id])
          tags:
            Name: to autoscaling
            Environment: .vpc_name
            Organization: .organization_name
          lifecycle:
            ignore_changes: all
      - ecr-dkr:
          count: '.enable_vpc_endpoints ? 1 : 0'
          vpc_id: data.aws_vpc.the_vpc.id
          service_name: data.aws_vpc_endpoint_service.ecr_dkr.service_name
          vpc_endpoint_type: Interface
          security_group_ids: data.aws_security_group.local_traffic.id
          private_dns_enabled: true
          subnet_ids: flatten([aws_subnet.eks_private[*].id])
          tags:
            Name: to ecr dkr
            Environment: .vpc_name
            Organization: .organization_name
          lifecycle:
            ignore_changes: all
      - ecr-api:
          count: '.enable_vpc_endpoints ? 1 : 0'
          vpc_id: data.aws_vpc.the_vpc.id
          service_name: data.aws_vpc_endpoint_service.ecr_api.service_name
          vpc_endpoint_type: Interface
          security_group_ids: data.aws_security_group.local_traffic.id
          private_dns_enabled: true
          subnet_ids: flatten([aws_subnet.eks_private[*].id])
          tags:
            Name: to ecr api
            Environment: .vpc_name
            Organization: .organization_name
          lifecycle:
            ignore_changes: all
      - ebs:
          count: '.enable_vpc_endpoints ? 1 : 0'
          vpc_id: data.aws_vpc.the_vpc.id
          service_name: data.aws_vpc_endpoint_service.ebs.service_name
          vpc_endpoint_type: Interface
          security_group_ids: data.aws_security_group.local_traffic.id
          private_dns_enabled: true
          subnet_ids: flatten([aws_subnet.eks_private[*].id])
          tags:
            Name: to ebs
            Environment: .vpc_name
            Organization: .organization_name
          lifecycle:
            ignore_changes: all
      - k8s-s3:
          vpc_id: data.aws_vpc.the_vpc.id
          service_name: com.amazonaws.data.aws_region.current.name.s3
          route_table_ids: flatten([data.aws_route_table.public_kube.id, aws_route_table.eks_private[*].id])
          depends_on: aws_route_table.eks_private
          tags:
            Name: to s3
            Environment: .vpc_name
            Organization: .organization_name
          lifecycle:
            ignore_changes: all
      - k8s-logs:
          count: '.enable_vpc_endpoints ? 1 : 0'
          vpc_id: data.aws_vpc.the_vpc.id
          service_name: data.aws_vpc_endpoint_service.logs.service_name
          vpc_endpoint_type: Interface
          security_group_ids: data.aws_security_group.local_traffic.id
          private_dns_enabled: true
          subnet_ids: flatten([aws_subnet.eks_private[*].id])
          tags:
            Name: to cloudwatch logs
            Environment: .vpc_name
            Organization: .organization_name
          lifecycle:
            ignore_changes: all
      aws_sqs_queue:
        this:
          count: '.use_karpenter ? 1 : 0'
          name: local.queue_name
          message_retention_seconds: 300
      aws_sqs_queue_policy:
        this:
          count: '.use_karpenter ? 1 : 0'
          queue_url: aws_sqs_queue.this[0].url
          policy: data.aws_iam_policy_document.queue[0].json
      aws_cloudwatch_event_rule:
      - this:
          for_each: 'for k , v in local.events : k => v if var.use_karpenter'
          name_prefix: .vpc_name-each.value.name-
          description: each.value.description
          event_pattern: jsonencode(each.value.event_pattern)
          tags:
            ClusterName: aws_eks_cluster.eks_cluster.id
      - gw_checks_rule:
          count: '.ha_squid ? 1 : 0'
          name: .vpc_name-GW-checks-job
          description: Check if the gateway is working every minute
          schedule_expression: rate(1 minute)
          tags:
            Environment: .vpc_name
            Organization: .organization_name
      aws_cloudwatch_event_target:
      - this:
          for_each: 'for k , v in local.events : k => v if var.use_karpenter'
          rule: aws_cloudwatch_event_rule.this[each.key].name
          target_id: KarpenterInterruptionQueueTarget
          arn: aws_sqs_queue.this[0].arn
      - cw_to_lambda:
          count: '.ha_squid ? 1 : 0'
          rule: aws_cloudwatch_event_rule.gw_checks_rule[count.index].name
          arn: aws_lambda_function.gw_checks[count.index].arn
      aws_eks_fargate_profile:
        karpenter:
          count: '.use_karpenter ? 1 : 0'
          cluster_name: aws_eks_cluster.eks_cluster.name
          fargate_profile_name: karpenter
          pod_execution_role_arn: aws_iam_role.karpenter[0].arn
          subnet_ids: local.eks_priv_subnets
          selector:
            namespace: karpenter
      time_sleep:
        wait_60_seconds:
          create_duration: 60s
          depends_on: aws_eks_fargate_profile.karpenter
      helm_release:
        karpenter:
          count: '.k8s_bootstrap_resources && var.use_karpenter && var.deploy_karpenter_in_k8s
            ? 1 : 0'
          namespace: karpenter
          create_namespace: true
          name: karpenter
          repository: oci://public.ecr.aws/karpenter
          chart: karpenter
          version: .karpenter_version
          set:
            name:
            - settings.aws.clusterName
            - settings.aws.clusterEndpoint
            - serviceAccount.annotations.eks\.amazonaws\.com/role-arn
            - settings.aws.defaultInstanceProfile
            - settings.aws.interruptionQueueName
            - dnsPolicy
            value:
            - aws_eks_cluster.eks_cluster.id
            - aws_eks_cluster.eks_cluster.endpoint
            - aws_iam_role.irsa[0].arn
            - aws_iam_instance_profile.eks_node_instance_profile.id
            - aws_sqs_queue.this[0].name
            - Default
          depends_on: time_sleep.wait_60_seconds
          lifecycle:
            ignore_changes: all
      aws_cloudwatch_log_group:
        gwl_group:
          count: '.ha_squid ? 1 : 0'
          name: /aws/lambda/.vpc_name-gw-checks-lambda
          retention_in_days: 14
      aws_lambda_function:
        gw_checks:
          count: '.ha_squid ? 1 : 0'
          filename: lambda_function_payload.zip
          function_name: .vpc_name-gw-checks-lambda
          role: module.iam_role[0].role_arn
          handler: lambda_function.lambda_handler
          timeout: 45
          description: Checks for internet access from the worker nodes subnets
          depends_on: aws_cloudwatch_log_group.gwl_group
          source_code_hash: data.archive_file.lambda_function.output_base64sha256
          runtime: python3.8
          vpc_config:
            subnet_ids: flatten([aws_subnet.eks_private.*.id])
            security_group_ids: aws_security_group.eks_nodes_sg.id
          environment:
            variables:
              vpc_name: .vpc_name
              domain_test: .domain_test
          tags:
            Environment: .vpc_name
            Organization: .organization_name
      aws_lambda_permission:
        allow_cloudwatch:
          count: '.ha_squid ? 1 : 0'
          statement_id: AllowExecutionFromCloudWatch
          action: lambda:InvokeFunction
          function_name: aws_lambda_function.gw_checks[count.index].function_name
          principal: events.amazonaws.com
          source_arn: aws_cloudwatch_event_rule.gw_checks_rule[count.index].arn
    terraform:
      required_providers:
        kubectl:
          source: gavinbunney/kubectl
    variable:
      vpc_name: 
      vpc_id:
        default: ''
      csoc_account_id:
        default: '433568766270'
      csoc_managed:
        default: false
      ec2_keyname:
        default: someone@uchicago.edu
      instance_type:
        default: t3.large
      jupyter_instance_type:
        default: t3.large
      workflow_instance_type:
        default: t3.2xlarge
      peering_cidr:
        default: 10.128.0.0/20
      secondary_cidr_block:
        default: ''
      peering_vpc_id:
        default: vpc-e2b51d99
      users_policy: 
      worker_drive_size:
        default: 30
      eks_version:
        default: '1.25'
      workers_subnet_size:
        default: 24
      kernel:
        default: N/A
      bootstrap_script:
        default: bootstrap.sh
      jupyter_bootstrap_script:
        default: bootstrap.sh
      jupyter_worker_drive_size:
        default: 30
      workflow_bootstrap_script:
        default: bootstrap.sh
      workflow_worker_drive_size:
        default: 30
      cidrs_to_route_to_gw:
        default: []
      organization_name:
        default: Basic Services
      proxy_name:
        default: ' HTTP Proxy'
      jupyter_asg_desired_capacity:
        default: 0
      jupyter_asg_max_size:
        default: 10
      jupyter_asg_min_size:
        default: 0
      workflow_asg_desired_capacity:
        default: 0
      workflow_asg_max_size:
        default: 50
      workflow_asg_min_size:
        default: 0
      iam-serviceaccount:
        default: false
      oidc_eks_thumbprint:
        description: Thumbprint for the AWS OIDC identity provider
        default: 9e99a48a9960b14926bb7f3b02e22da2b0ab7280
      availability_zones:
        description: AZ to be used by EKS nodes
        default: us-east-1a us-east-1c us-east-1d
      domain_test:
        description: Domain for the lambda function to check for the proxy
        default: www.google.com
      ha_squid:
        description: Is HA squid deployed?
        default: false
      deploy_workflow:
        description: Deploy workflow nodepool?
        default: false
      secondary_availability_zones:
        description: AZ to be used by EKS nodes in the secondary subnet
        default: us-east-1a us-east-1b us-east-1c us-east-1d
      deploy_jupyter:
        description: Deploy workflow nodepool?
        default: true
      dual_proxy:
        description: Single instance and HA
      single_az_for_jupyter:
        description: Jupyter notebooks on a single AZ
        default: false
      sns_topic_arn:
        description: SNS topic ARN for alerts
        default: arn:aws:sns:us-east-1:433568766270:planx-csoc-alerts-topic
      activation_id:
        default: ''
      customer_id:
        default: ''
      fips:
        default: false
      fips_ami_kms:
        default: arn:aws:kms:us-east-1:707767160287:key/mrk-697897f040ef45b0aa3cebf38a916f99
      fips_enabled_ami:
        default: ami-0de87e3680dcb13ec
      use_asg:
        default: true
      use_karpenter:
        default: false
      deploy_karpenter_in_k8s:
        default: false
        description: Allows you to enable the Karpenter Helm chart and associated
          resources without deploying the other parts of karpenter (i.e. the roles,
          permissions, and SQS queue)
      karpenter_version:
        default: v0.32.9
      spot_linked_role:
        default: false
      scale_in_protection:
        description: set scale-in protection on ASG
        default: false
      ci_run:
        default: false
      eks_public_access:
        default: 'true'
      enable_vpc_endpoints:
        default: true
      k8s_bootstrap_resources:
        default: false
        description: If set to true, creates resources for bootstrapping a kubernetes
          cluster (such as karpenter configs and helm releases)
  gen3_deployment:
    count: '.deploy_gen3 && var.deploy_aurora ? 1 : 0'
    aurora_password: module.aurora[0].aurora_cluster_master_password
    aurora_hostname: module.aurora[0].aurora_cluster_writer_endpoint
    aurora_username: module.aurora[0].aurora_cluster_master_username
    cluster_endpoint: module.eks[0].cluster_endpoint
    cluster_ca_cert: module.eks[0].cluster_certificate_authority_data
    cluster_name: module.eks[0].cluster_name
    ambassador_enabled: .ambassador_enabled
    arborist_enabled: .arborist_enabled
    argo_enabled: .argo_enabled
    audit_enabled: .audit_enabled
    aws-es-proxy_enabled: .aws-es-proxy_enabled
    dbgap_enabled: .dbgap_enabled
    dd_enabled: .dd_enabled
    dictionary_url: .dictionary_url
    dispatcher_job_number: .dispatcher_job_number
    fence_enabled: .fence_enabled
    guppy_enabled: .guppy_enabled
    hatchery_enabled: .hatchery_enabled
    hostname: .hostname
    indexd_enabled: .indexd_enabled
    indexd_prefix: .indexd_prefix
    ingress_enabled: .ingress_enabled
    manifestservice_enabled: .manifestservice_enabled
    metadata_enabled: .metadata_enabled
    netpolicy_enabled: .netpolicy_enabled
    peregrine_enabled: .peregrine_enabled
    pidgin_enabled: .pidgin_enabled
    portal_enabled: .portal_enabled
    public_datasets: .public_datasets
    requestor_enabled: .requestor_enabled
    revproxy_arn: .revproxy_arn
    revproxy_enabled: .revproxy_enabled
    sheepdog_enabled: .sheepdog_enabled
    slack_send_dbgap: .slack_send_dbgap
    slack_webhook: .slack_webhook
    ssjdispatcher_enabled: .ssjdispatcher_enabled
    tier_access_level: .tier_access_level
    tier_access_limit: .tier_access_limit
    usersync_enabled: .usersync_enabled
    usersync_schedule: .usersync_schedule
    useryaml_s3_path: .useryaml_s3_path
    wts_enabled: .wts_enabled
    fence_config_path: .fence_config_path
    useryaml_path: .useryaml_path
    gitops_path: .gitops_path
    google_client_id: .google_client_id
    google_client_secret: .google_client_secret
    fence_access_key: .fence_access_key
    fence_secret_key: .fence_secret_key
    upload_bucket: .upload_bucket
    namespace: .namespace
    module:
      arborist-db:
        count: '.arborist_enabled ? 1 : 0'
        vpc_name: .vpc_name
        service: arborist
        admin_database_username: .aurora_username
        admin_database_password: .aurora_password
        namespace: .namespace
        secrets_manager_enabled: true
        data:
          aws_caller_identity:
            current: 
          aws_eks_cluster:
            eks:
              name: .vpc_name
          aws_secretsmanager_secret:
            aurora-master-password:
              name: .vpc_name_aurora-master-password
          aws_secretsmanager_secret_version:
            aurora-master-password:
              secret_id: data.aws_secretsmanager_secret.aurora-master-password.id
          aws_iam_policy_document:
          - policy:
              statement:
                actions: secretsmanager:GetSecretValue secretsmanager:DescribeSecret
                resources: module.secrets_manager[0].secret-arn
          - sa_policy:
              statement:
                actions: sts:AssumeRoleWithWebIdentity
                principals:
                  type: Federated
                  identifiers: arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/local.eks_oidc_issuer
                condition:
                  test: StringEquals
                  variable: local.eks_oidc_issuer:sub
                  values: system:serviceaccount:local.sa_namespace:local.sa_name
          aws_db_instance:
            database:
              db_instance_identifier: .vpc_name-aurora-cluster-instance
        locals:
          sa_name: .service-sa
          sa_namespace: .namespace
          eks_oidc_issuer: trimprefix(data.aws_eks_cluster.eks.identity[0].oidc[0].issuer,
            "https://")
          database_name: '.database_name != "" ? var.database_name : ".service_.namespace"'
          database_username: '.username != "" ? var.username : ".service_.namespace"'
          database_password: '.password != "" ? var.password : random_password.db_password[0].result'
        resource:
          aws_iam_policy:
            secrets_manager_policy:
              count: '.secrets_manager_enabled ? 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-policy
              description: Policy for .vpc_name-.service to access secrets
                manager
              policy: data.aws_iam_policy_document.policy.json
          aws_iam_role:
            role:
              count: '.secrets_manager_enabled ? var.role != "" ? 0 : 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-role
              assume_role_policy: data.aws_iam_policy_document.sa_policy.json
          aws_iam_role_policy_attachment:
            new_attach:
              count: '.secrets_manager_enabled ? 1 : 0'
              role: '.role != "" ? var.role : aws_iam_role.role[0].name'
              policy_arn: aws_iam_policy.secrets_manager_policy[0].arn
          random_password:
            db_password:
              count: '.password != "" ? 0 : 1'
              length: 16
              special: false
          null_resource:
          - db_setup:
              provisioner:
                local-exec:
                  command: psql -h data.aws_db_instance.database.address -U .admin_database_username
                    -d .admin_database_name -c "CREATE DATABASE \"local.database_name\";"
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
                  on_failure: continue
              triggers:
                database: local.database_name
          - user_setup:
              provisioner:
                local-exec:
                  command: "psql -h data.aws_db_instance.database.address -U .admin_database_username\
                    \ -d .admin_database_name -c \"templatefile(\"path.module/db_setup.tftpl\"\
                    , \n          username  = local.database_username\n         \
                    \ database  = local.database_name\n          password  = local.database_password\n\
                    \        )\""
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.db_setup
          - db_restore:
              count: '.db_restore && var.dump_file_to_restore != "" ? 1 : 0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n\naws s3 cp \".dump_file_to_restore\" - --quiet |\
                    \ psql -h \"data.aws_db_instance.database.address\" -U \"local.database_username\"\
                    \ -d \"local.database_name\"\necho \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
          - db_dump:
              count: '.db_dump && var.dump_file_storage_location != "" ? 1 :
                0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n    \npg_dump --username=\"local.database_username\"\
                    \ --dbname=\"local.database_name\" --host=\"data.aws_db_instance.database.address\"\
                    \ --no-password --no-owner --no-privileges >> ./dump.sql && aws\
                    \ s3 cp ./dump.sql .dump_file_storage_location && rm ./dump.sql\n\
                    echo \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
        module:
          secrets_manager:
            count: '.secrets_manager_enabled ? 1 : 0'
            vpc_name: .vpc_name
            secret: 'templatefile("path.module/secrets_manager.tftpl", "hostname":
              "data.aws_db_instance.database.address", "database": "local.database_name",
              "username": "local.database_username", "password": "local.database_password")'
            secret_name: .namespace-.service-creds
            depends_on: null_resource.user_setup
            data:
              aws_caller_identity:
                current: 
              aws_vpcs:
                vpcs:
                  tags:
                    Name: .vpc_name
              aws_security_group:
                private:
                  vpc_id: data.aws_vpc.the_vpc.id
                  name: local
              aws_vpc:
                the_vpc:
                  id: data.aws_vpcs.vpcs.ids[0]
            output:
              secret-arn:
                value: aws_secretsmanager_secret.secret.arn
            resource:
              aws_secretsmanager_secret:
                secret:
                  name: .vpc_name_.secret_name
              aws_secretsmanager_secret_version:
                secret:
                  secret_id: aws_secretsmanager_secret.secret.id
                  secret_string: .secret
            variable:
              vpc_name: 
              secret: 
              secret_name: 
        terraform: []
        variable:
          vpc_name:
            default: ''
          service:
            default: ''
          admin_database_username:
            default: postgres
          admin_database_name:
            default: postgres
          admin_database_password:
            default: ''
          namespace:
            default: default
          role:
            default: ''
          database_name:
            default: ''
          username:
            default: ''
          password:
            default: ''
          secrets_manager_enabled:
            default: true
          dump_file_to_restore:
            default: ''
          dump_file_storage_location:
            default: ''
          db_restore:
            default: false
          db_dump:
            default: false
          db_job_role_arn:
            default: ''
      argo-db:
        count: '.argo_enabled ? 1 : 0'
        vpc_name: .vpc_name
        service: argo
        admin_database_username: .aurora_username
        admin_database_password: .aurora_password
        namespace: .namespace
        secrets_manager_enabled: true
        data:
          aws_caller_identity:
            current: 
          aws_eks_cluster:
            eks:
              name: .vpc_name
          aws_secretsmanager_secret:
            aurora-master-password:
              name: .vpc_name_aurora-master-password
          aws_secretsmanager_secret_version:
            aurora-master-password:
              secret_id: data.aws_secretsmanager_secret.aurora-master-password.id
          aws_iam_policy_document:
          - policy:
              statement:
                actions: secretsmanager:GetSecretValue secretsmanager:DescribeSecret
                resources: module.secrets_manager[0].secret-arn
          - sa_policy:
              statement:
                actions: sts:AssumeRoleWithWebIdentity
                principals:
                  type: Federated
                  identifiers: arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/local.eks_oidc_issuer
                condition:
                  test: StringEquals
                  variable: local.eks_oidc_issuer:sub
                  values: system:serviceaccount:local.sa_namespace:local.sa_name
          aws_db_instance:
            database:
              db_instance_identifier: .vpc_name-aurora-cluster-instance
        locals:
          sa_name: .service-sa
          sa_namespace: .namespace
          eks_oidc_issuer: trimprefix(data.aws_eks_cluster.eks.identity[0].oidc[0].issuer,
            "https://")
          database_name: '.database_name != "" ? var.database_name : ".service_.namespace"'
          database_username: '.username != "" ? var.username : ".service_.namespace"'
          database_password: '.password != "" ? var.password : random_password.db_password[0].result'
        resource:
          aws_iam_policy:
            secrets_manager_policy:
              count: '.secrets_manager_enabled ? 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-policy
              description: Policy for .vpc_name-.service to access secrets
                manager
              policy: data.aws_iam_policy_document.policy.json
          aws_iam_role:
            role:
              count: '.secrets_manager_enabled ? var.role != "" ? 0 : 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-role
              assume_role_policy: data.aws_iam_policy_document.sa_policy.json
          aws_iam_role_policy_attachment:
            new_attach:
              count: '.secrets_manager_enabled ? 1 : 0'
              role: '.role != "" ? var.role : aws_iam_role.role[0].name'
              policy_arn: aws_iam_policy.secrets_manager_policy[0].arn
          random_password:
            db_password:
              count: '.password != "" ? 0 : 1'
              length: 16
              special: false
          null_resource:
          - db_setup:
              provisioner:
                local-exec:
                  command: psql -h data.aws_db_instance.database.address -U .admin_database_username
                    -d .admin_database_name -c "CREATE DATABASE \"local.database_name\";"
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
                  on_failure: continue
              triggers:
                database: local.database_name
          - user_setup:
              provisioner:
                local-exec:
                  command: "psql -h data.aws_db_instance.database.address -U .admin_database_username\
                    \ -d .admin_database_name -c \"templatefile(\"path.module/db_setup.tftpl\"\
                    , \n          username  = local.database_username\n         \
                    \ database  = local.database_name\n          password  = local.database_password\n\
                    \        )\""
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.db_setup
          - db_restore:
              count: '.db_restore && var.dump_file_to_restore != "" ? 1 : 0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n\naws s3 cp \".dump_file_to_restore\" - --quiet |\
                    \ psql -h \"data.aws_db_instance.database.address\" -U \"local.database_username\"\
                    \ -d \"local.database_name\"\necho \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
          - db_dump:
              count: '.db_dump && var.dump_file_storage_location != "" ? 1 :
                0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n    \npg_dump --username=\"local.database_username\"\
                    \ --dbname=\"local.database_name\" --host=\"data.aws_db_instance.database.address\"\
                    \ --no-password --no-owner --no-privileges >> ./dump.sql && aws\
                    \ s3 cp ./dump.sql .dump_file_storage_location && rm ./dump.sql\n\
                    echo \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
        module:
          secrets_manager:
            count: '.secrets_manager_enabled ? 1 : 0'
            vpc_name: .vpc_name
            secret: 'templatefile("path.module/secrets_manager.tftpl", "hostname":
              "data.aws_db_instance.database.address", "database": "local.database_name",
              "username": "local.database_username", "password": "local.database_password")'
            secret_name: .namespace-.service-creds
            depends_on: null_resource.user_setup
            data:
              aws_caller_identity:
                current: 
              aws_vpcs:
                vpcs:
                  tags:
                    Name: .vpc_name
              aws_security_group:
                private:
                  vpc_id: data.aws_vpc.the_vpc.id
                  name: local
              aws_vpc:
                the_vpc:
                  id: data.aws_vpcs.vpcs.ids[0]
            output:
              secret-arn:
                value: aws_secretsmanager_secret.secret.arn
            resource:
              aws_secretsmanager_secret:
                secret:
                  name: .vpc_name_.secret_name
              aws_secretsmanager_secret_version:
                secret:
                  secret_id: aws_secretsmanager_secret.secret.id
                  secret_string: .secret
            variable:
              vpc_name: 
              secret: 
              secret_name: 
        terraform: []
        variable:
          vpc_name:
            default: ''
          service:
            default: ''
          admin_database_username:
            default: postgres
          admin_database_name:
            default: postgres
          admin_database_password:
            default: ''
          namespace:
            default: default
          role:
            default: ''
          database_name:
            default: ''
          username:
            default: ''
          password:
            default: ''
          secrets_manager_enabled:
            default: true
          dump_file_to_restore:
            default: ''
          dump_file_storage_location:
            default: ''
          db_restore:
            default: false
          db_dump:
            default: false
          db_job_role_arn:
            default: ''
      audit-db:
        count: '.audit_enabled ? 1 : 0'
        vpc_name: .vpc_name
        service: audit
        admin_database_username: .aurora_username
        admin_database_password: .aurora_password
        namespace: .namespace
        secrets_manager_enabled: true
        data:
          aws_caller_identity:
            current: 
          aws_eks_cluster:
            eks:
              name: .vpc_name
          aws_secretsmanager_secret:
            aurora-master-password:
              name: .vpc_name_aurora-master-password
          aws_secretsmanager_secret_version:
            aurora-master-password:
              secret_id: data.aws_secretsmanager_secret.aurora-master-password.id
          aws_iam_policy_document:
          - policy:
              statement:
                actions: secretsmanager:GetSecretValue secretsmanager:DescribeSecret
                resources: module.secrets_manager[0].secret-arn
          - sa_policy:
              statement:
                actions: sts:AssumeRoleWithWebIdentity
                principals:
                  type: Federated
                  identifiers: arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/local.eks_oidc_issuer
                condition:
                  test: StringEquals
                  variable: local.eks_oidc_issuer:sub
                  values: system:serviceaccount:local.sa_namespace:local.sa_name
          aws_db_instance:
            database:
              db_instance_identifier: .vpc_name-aurora-cluster-instance
        locals:
          sa_name: .service-sa
          sa_namespace: .namespace
          eks_oidc_issuer: trimprefix(data.aws_eks_cluster.eks.identity[0].oidc[0].issuer,
            "https://")
          database_name: '.database_name != "" ? var.database_name : ".service_.namespace"'
          database_username: '.username != "" ? var.username : ".service_.namespace"'
          database_password: '.password != "" ? var.password : random_password.db_password[0].result'
        resource:
          aws_iam_policy:
            secrets_manager_policy:
              count: '.secrets_manager_enabled ? 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-policy
              description: Policy for .vpc_name-.service to access secrets
                manager
              policy: data.aws_iam_policy_document.policy.json
          aws_iam_role:
            role:
              count: '.secrets_manager_enabled ? var.role != "" ? 0 : 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-role
              assume_role_policy: data.aws_iam_policy_document.sa_policy.json
          aws_iam_role_policy_attachment:
            new_attach:
              count: '.secrets_manager_enabled ? 1 : 0'
              role: '.role != "" ? var.role : aws_iam_role.role[0].name'
              policy_arn: aws_iam_policy.secrets_manager_policy[0].arn
          random_password:
            db_password:
              count: '.password != "" ? 0 : 1'
              length: 16
              special: false
          null_resource:
          - db_setup:
              provisioner:
                local-exec:
                  command: psql -h data.aws_db_instance.database.address -U .admin_database_username
                    -d .admin_database_name -c "CREATE DATABASE \"local.database_name\";"
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
                  on_failure: continue
              triggers:
                database: local.database_name
          - user_setup:
              provisioner:
                local-exec:
                  command: "psql -h data.aws_db_instance.database.address -U .admin_database_username\
                    \ -d .admin_database_name -c \"templatefile(\"path.module/db_setup.tftpl\"\
                    , \n          username  = local.database_username\n         \
                    \ database  = local.database_name\n          password  = local.database_password\n\
                    \        )\""
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.db_setup
          - db_restore:
              count: '.db_restore && var.dump_file_to_restore != "" ? 1 : 0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n\naws s3 cp \".dump_file_to_restore\" - --quiet |\
                    \ psql -h \"data.aws_db_instance.database.address\" -U \"local.database_username\"\
                    \ -d \"local.database_name\"\necho \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
          - db_dump:
              count: '.db_dump && var.dump_file_storage_location != "" ? 1 :
                0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n    \npg_dump --username=\"local.database_username\"\
                    \ --dbname=\"local.database_name\" --host=\"data.aws_db_instance.database.address\"\
                    \ --no-password --no-owner --no-privileges >> ./dump.sql && aws\
                    \ s3 cp ./dump.sql .dump_file_storage_location && rm ./dump.sql\n\
                    echo \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
        module:
          secrets_manager:
            count: '.secrets_manager_enabled ? 1 : 0'
            vpc_name: .vpc_name
            secret: 'templatefile("path.module/secrets_manager.tftpl", "hostname":
              "data.aws_db_instance.database.address", "database": "local.database_name",
              "username": "local.database_username", "password": "local.database_password")'
            secret_name: .namespace-.service-creds
            depends_on: null_resource.user_setup
            data:
              aws_caller_identity:
                current: 
              aws_vpcs:
                vpcs:
                  tags:
                    Name: .vpc_name
              aws_security_group:
                private:
                  vpc_id: data.aws_vpc.the_vpc.id
                  name: local
              aws_vpc:
                the_vpc:
                  id: data.aws_vpcs.vpcs.ids[0]
            output:
              secret-arn:
                value: aws_secretsmanager_secret.secret.arn
            resource:
              aws_secretsmanager_secret:
                secret:
                  name: .vpc_name_.secret_name
              aws_secretsmanager_secret_version:
                secret:
                  secret_id: aws_secretsmanager_secret.secret.id
                  secret_string: .secret
            variable:
              vpc_name: 
              secret: 
              secret_name: 
        terraform: []
        variable:
          vpc_name:
            default: ''
          service:
            default: ''
          admin_database_username:
            default: postgres
          admin_database_name:
            default: postgres
          admin_database_password:
            default: ''
          namespace:
            default: default
          role:
            default: ''
          database_name:
            default: ''
          username:
            default: ''
          password:
            default: ''
          secrets_manager_enabled:
            default: true
          dump_file_to_restore:
            default: ''
          dump_file_storage_location:
            default: ''
          db_restore:
            default: false
          db_dump:
            default: false
          db_job_role_arn:
            default: ''
      audit-sqs:
        sqs_name: audit
        output:
          sqs-url:
            value: aws_sqs_queue.generic_queue.id
          sqs-arn:
            value: aws_sqs_queue.generic_queue.arn
        resource:
          aws_sqs_queue:
            generic_queue:
              name: .sqs_name
              visibility_timeout_seconds: 300
              message_retention_seconds: 1209600
              tags:
                Organization: gen3
                description: Created by SQS module
        variable:
          sqs_name: 
          slack_webhook:
            default: ''
      dicom-server-db:
        count: '.dicom-server_enabled ? 1 : 0'
        vpc_name: .vpc_name
        service: dicom-server
        admin_database_username: .aurora_username
        admin_database_password: .aurora_password
        namespace: .namespace
        secrets_manager_enabled: true
        data:
          aws_caller_identity:
            current: 
          aws_eks_cluster:
            eks:
              name: .vpc_name
          aws_secretsmanager_secret:
            aurora-master-password:
              name: .vpc_name_aurora-master-password
          aws_secretsmanager_secret_version:
            aurora-master-password:
              secret_id: data.aws_secretsmanager_secret.aurora-master-password.id
          aws_iam_policy_document:
          - policy:
              statement:
                actions: secretsmanager:GetSecretValue secretsmanager:DescribeSecret
                resources: module.secrets_manager[0].secret-arn
          - sa_policy:
              statement:
                actions: sts:AssumeRoleWithWebIdentity
                principals:
                  type: Federated
                  identifiers: arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/local.eks_oidc_issuer
                condition:
                  test: StringEquals
                  variable: local.eks_oidc_issuer:sub
                  values: system:serviceaccount:local.sa_namespace:local.sa_name
          aws_db_instance:
            database:
              db_instance_identifier: .vpc_name-aurora-cluster-instance
        locals:
          sa_name: .service-sa
          sa_namespace: .namespace
          eks_oidc_issuer: trimprefix(data.aws_eks_cluster.eks.identity[0].oidc[0].issuer,
            "https://")
          database_name: '.database_name != "" ? var.database_name : ".service_.namespace"'
          database_username: '.username != "" ? var.username : ".service_.namespace"'
          database_password: '.password != "" ? var.password : random_password.db_password[0].result'
        resource:
          aws_iam_policy:
            secrets_manager_policy:
              count: '.secrets_manager_enabled ? 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-policy
              description: Policy for .vpc_name-.service to access secrets
                manager
              policy: data.aws_iam_policy_document.policy.json
          aws_iam_role:
            role:
              count: '.secrets_manager_enabled ? var.role != "" ? 0 : 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-role
              assume_role_policy: data.aws_iam_policy_document.sa_policy.json
          aws_iam_role_policy_attachment:
            new_attach:
              count: '.secrets_manager_enabled ? 1 : 0'
              role: '.role != "" ? var.role : aws_iam_role.role[0].name'
              policy_arn: aws_iam_policy.secrets_manager_policy[0].arn
          random_password:
            db_password:
              count: '.password != "" ? 0 : 1'
              length: 16
              special: false
          null_resource:
          - db_setup:
              provisioner:
                local-exec:
                  command: psql -h data.aws_db_instance.database.address -U .admin_database_username
                    -d .admin_database_name -c "CREATE DATABASE \"local.database_name\";"
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
                  on_failure: continue
              triggers:
                database: local.database_name
          - user_setup:
              provisioner:
                local-exec:
                  command: "psql -h data.aws_db_instance.database.address -U .admin_database_username\
                    \ -d .admin_database_name -c \"templatefile(\"path.module/db_setup.tftpl\"\
                    , \n          username  = local.database_username\n         \
                    \ database  = local.database_name\n          password  = local.database_password\n\
                    \        )\""
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.db_setup
          - db_restore:
              count: '.db_restore && var.dump_file_to_restore != "" ? 1 : 0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n\naws s3 cp \".dump_file_to_restore\" - --quiet |\
                    \ psql -h \"data.aws_db_instance.database.address\" -U \"local.database_username\"\
                    \ -d \"local.database_name\"\necho \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
          - db_dump:
              count: '.db_dump && var.dump_file_storage_location != "" ? 1 :
                0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n    \npg_dump --username=\"local.database_username\"\
                    \ --dbname=\"local.database_name\" --host=\"data.aws_db_instance.database.address\"\
                    \ --no-password --no-owner --no-privileges >> ./dump.sql && aws\
                    \ s3 cp ./dump.sql .dump_file_storage_location && rm ./dump.sql\n\
                    echo \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
        module:
          secrets_manager:
            count: '.secrets_manager_enabled ? 1 : 0'
            vpc_name: .vpc_name
            secret: 'templatefile("path.module/secrets_manager.tftpl", "hostname":
              "data.aws_db_instance.database.address", "database": "local.database_name",
              "username": "local.database_username", "password": "local.database_password")'
            secret_name: .namespace-.service-creds
            depends_on: null_resource.user_setup
            data:
              aws_caller_identity:
                current: 
              aws_vpcs:
                vpcs:
                  tags:
                    Name: .vpc_name
              aws_security_group:
                private:
                  vpc_id: data.aws_vpc.the_vpc.id
                  name: local
              aws_vpc:
                the_vpc:
                  id: data.aws_vpcs.vpcs.ids[0]
            output:
              secret-arn:
                value: aws_secretsmanager_secret.secret.arn
            resource:
              aws_secretsmanager_secret:
                secret:
                  name: .vpc_name_.secret_name
              aws_secretsmanager_secret_version:
                secret:
                  secret_id: aws_secretsmanager_secret.secret.id
                  secret_string: .secret
            variable:
              vpc_name: 
              secret: 
              secret_name: 
        terraform: []
        variable:
          vpc_name:
            default: ''
          service:
            default: ''
          admin_database_username:
            default: postgres
          admin_database_name:
            default: postgres
          admin_database_password:
            default: ''
          namespace:
            default: default
          role:
            default: ''
          database_name:
            default: ''
          username:
            default: ''
          password:
            default: ''
          secrets_manager_enabled:
            default: true
          dump_file_to_restore:
            default: ''
          dump_file_storage_location:
            default: ''
          db_restore:
            default: false
          db_dump:
            default: false
          db_job_role_arn:
            default: ''
      dicom-viewer-db:
        count: '.dicom-viewer_enabled ? 1 : 0'
        vpc_name: .vpc_name
        service: dicom
        admin_database_username: .aurora_username
        admin_database_password: .aurora_password
        namespace: .namespace
        secrets_manager_enabled: true
        data:
          aws_caller_identity:
            current: 
          aws_eks_cluster:
            eks:
              name: .vpc_name
          aws_secretsmanager_secret:
            aurora-master-password:
              name: .vpc_name_aurora-master-password
          aws_secretsmanager_secret_version:
            aurora-master-password:
              secret_id: data.aws_secretsmanager_secret.aurora-master-password.id
          aws_iam_policy_document:
          - policy:
              statement:
                actions: secretsmanager:GetSecretValue secretsmanager:DescribeSecret
                resources: module.secrets_manager[0].secret-arn
          - sa_policy:
              statement:
                actions: sts:AssumeRoleWithWebIdentity
                principals:
                  type: Federated
                  identifiers: arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/local.eks_oidc_issuer
                condition:
                  test: StringEquals
                  variable: local.eks_oidc_issuer:sub
                  values: system:serviceaccount:local.sa_namespace:local.sa_name
          aws_db_instance:
            database:
              db_instance_identifier: .vpc_name-aurora-cluster-instance
        locals:
          sa_name: .service-sa
          sa_namespace: .namespace
          eks_oidc_issuer: trimprefix(data.aws_eks_cluster.eks.identity[0].oidc[0].issuer,
            "https://")
          database_name: '.database_name != "" ? var.database_name : ".service_.namespace"'
          database_username: '.username != "" ? var.username : ".service_.namespace"'
          database_password: '.password != "" ? var.password : random_password.db_password[0].result'
        resource:
          aws_iam_policy:
            secrets_manager_policy:
              count: '.secrets_manager_enabled ? 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-policy
              description: Policy for .vpc_name-.service to access secrets
                manager
              policy: data.aws_iam_policy_document.policy.json
          aws_iam_role:
            role:
              count: '.secrets_manager_enabled ? var.role != "" ? 0 : 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-role
              assume_role_policy: data.aws_iam_policy_document.sa_policy.json
          aws_iam_role_policy_attachment:
            new_attach:
              count: '.secrets_manager_enabled ? 1 : 0'
              role: '.role != "" ? var.role : aws_iam_role.role[0].name'
              policy_arn: aws_iam_policy.secrets_manager_policy[0].arn
          random_password:
            db_password:
              count: '.password != "" ? 0 : 1'
              length: 16
              special: false
          null_resource:
          - db_setup:
              provisioner:
                local-exec:
                  command: psql -h data.aws_db_instance.database.address -U .admin_database_username
                    -d .admin_database_name -c "CREATE DATABASE \"local.database_name\";"
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
                  on_failure: continue
              triggers:
                database: local.database_name
          - user_setup:
              provisioner:
                local-exec:
                  command: "psql -h data.aws_db_instance.database.address -U .admin_database_username\
                    \ -d .admin_database_name -c \"templatefile(\"path.module/db_setup.tftpl\"\
                    , \n          username  = local.database_username\n         \
                    \ database  = local.database_name\n          password  = local.database_password\n\
                    \        )\""
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.db_setup
          - db_restore:
              count: '.db_restore && var.dump_file_to_restore != "" ? 1 : 0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n\naws s3 cp \".dump_file_to_restore\" - --quiet |\
                    \ psql -h \"data.aws_db_instance.database.address\" -U \"local.database_username\"\
                    \ -d \"local.database_name\"\necho \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
          - db_dump:
              count: '.db_dump && var.dump_file_storage_location != "" ? 1 :
                0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n    \npg_dump --username=\"local.database_username\"\
                    \ --dbname=\"local.database_name\" --host=\"data.aws_db_instance.database.address\"\
                    \ --no-password --no-owner --no-privileges >> ./dump.sql && aws\
                    \ s3 cp ./dump.sql .dump_file_storage_location && rm ./dump.sql\n\
                    echo \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
        module:
          secrets_manager:
            count: '.secrets_manager_enabled ? 1 : 0'
            vpc_name: .vpc_name
            secret: 'templatefile("path.module/secrets_manager.tftpl", "hostname":
              "data.aws_db_instance.database.address", "database": "local.database_name",
              "username": "local.database_username", "password": "local.database_password")'
            secret_name: .namespace-.service-creds
            depends_on: null_resource.user_setup
            data:
              aws_caller_identity:
                current: 
              aws_vpcs:
                vpcs:
                  tags:
                    Name: .vpc_name
              aws_security_group:
                private:
                  vpc_id: data.aws_vpc.the_vpc.id
                  name: local
              aws_vpc:
                the_vpc:
                  id: data.aws_vpcs.vpcs.ids[0]
            output:
              secret-arn:
                value: aws_secretsmanager_secret.secret.arn
            resource:
              aws_secretsmanager_secret:
                secret:
                  name: .vpc_name_.secret_name
              aws_secretsmanager_secret_version:
                secret:
                  secret_id: aws_secretsmanager_secret.secret.id
                  secret_string: .secret
            variable:
              vpc_name: 
              secret: 
              secret_name: 
        terraform: []
        variable:
          vpc_name:
            default: ''
          service:
            default: ''
          admin_database_username:
            default: postgres
          admin_database_name:
            default: postgres
          admin_database_password:
            default: ''
          namespace:
            default: default
          role:
            default: ''
          database_name:
            default: ''
          username:
            default: ''
          password:
            default: ''
          secrets_manager_enabled:
            default: true
          dump_file_to_restore:
            default: ''
          dump_file_storage_location:
            default: ''
          db_restore:
            default: false
          db_dump:
            default: false
          db_job_role_arn:
            default: ''
      fence-db:
        count: '.fence_enabled ? 1 : 0'
        vpc_name: .vpc_name
        service: fence
        admin_database_username: .aurora_username
        admin_database_password: .aurora_password
        namespace: .namespace
        secrets_manager_enabled: true
        data:
          aws_caller_identity:
            current: 
          aws_eks_cluster:
            eks:
              name: .vpc_name
          aws_secretsmanager_secret:
            aurora-master-password:
              name: .vpc_name_aurora-master-password
          aws_secretsmanager_secret_version:
            aurora-master-password:
              secret_id: data.aws_secretsmanager_secret.aurora-master-password.id
          aws_iam_policy_document:
          - policy:
              statement:
                actions: secretsmanager:GetSecretValue secretsmanager:DescribeSecret
                resources: module.secrets_manager[0].secret-arn
          - sa_policy:
              statement:
                actions: sts:AssumeRoleWithWebIdentity
                principals:
                  type: Federated
                  identifiers: arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/local.eks_oidc_issuer
                condition:
                  test: StringEquals
                  variable: local.eks_oidc_issuer:sub
                  values: system:serviceaccount:local.sa_namespace:local.sa_name
          aws_db_instance:
            database:
              db_instance_identifier: .vpc_name-aurora-cluster-instance
        locals:
          sa_name: .service-sa
          sa_namespace: .namespace
          eks_oidc_issuer: trimprefix(data.aws_eks_cluster.eks.identity[0].oidc[0].issuer,
            "https://")
          database_name: '.database_name != "" ? var.database_name : ".service_.namespace"'
          database_username: '.username != "" ? var.username : ".service_.namespace"'
          database_password: '.password != "" ? var.password : random_password.db_password[0].result'
        resource:
          aws_iam_policy:
            secrets_manager_policy:
              count: '.secrets_manager_enabled ? 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-policy
              description: Policy for .vpc_name-.service to access secrets
                manager
              policy: data.aws_iam_policy_document.policy.json
          aws_iam_role:
            role:
              count: '.secrets_manager_enabled ? var.role != "" ? 0 : 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-role
              assume_role_policy: data.aws_iam_policy_document.sa_policy.json
          aws_iam_role_policy_attachment:
            new_attach:
              count: '.secrets_manager_enabled ? 1 : 0'
              role: '.role != "" ? var.role : aws_iam_role.role[0].name'
              policy_arn: aws_iam_policy.secrets_manager_policy[0].arn
          random_password:
            db_password:
              count: '.password != "" ? 0 : 1'
              length: 16
              special: false
          null_resource:
          - db_setup:
              provisioner:
                local-exec:
                  command: psql -h data.aws_db_instance.database.address -U .admin_database_username
                    -d .admin_database_name -c "CREATE DATABASE \"local.database_name\";"
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
                  on_failure: continue
              triggers:
                database: local.database_name
          - user_setup:
              provisioner:
                local-exec:
                  command: "psql -h data.aws_db_instance.database.address -U .admin_database_username\
                    \ -d .admin_database_name -c \"templatefile(\"path.module/db_setup.tftpl\"\
                    , \n          username  = local.database_username\n         \
                    \ database  = local.database_name\n          password  = local.database_password\n\
                    \        )\""
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.db_setup
          - db_restore:
              count: '.db_restore && var.dump_file_to_restore != "" ? 1 : 0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n\naws s3 cp \".dump_file_to_restore\" - --quiet |\
                    \ psql -h \"data.aws_db_instance.database.address\" -U \"local.database_username\"\
                    \ -d \"local.database_name\"\necho \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
          - db_dump:
              count: '.db_dump && var.dump_file_storage_location != "" ? 1 :
                0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n    \npg_dump --username=\"local.database_username\"\
                    \ --dbname=\"local.database_name\" --host=\"data.aws_db_instance.database.address\"\
                    \ --no-password --no-owner --no-privileges >> ./dump.sql && aws\
                    \ s3 cp ./dump.sql .dump_file_storage_location && rm ./dump.sql\n\
                    echo \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
        module:
          secrets_manager:
            count: '.secrets_manager_enabled ? 1 : 0'
            vpc_name: .vpc_name
            secret: 'templatefile("path.module/secrets_manager.tftpl", "hostname":
              "data.aws_db_instance.database.address", "database": "local.database_name",
              "username": "local.database_username", "password": "local.database_password")'
            secret_name: .namespace-.service-creds
            depends_on: null_resource.user_setup
            data:
              aws_caller_identity:
                current: 
              aws_vpcs:
                vpcs:
                  tags:
                    Name: .vpc_name
              aws_security_group:
                private:
                  vpc_id: data.aws_vpc.the_vpc.id
                  name: local
              aws_vpc:
                the_vpc:
                  id: data.aws_vpcs.vpcs.ids[0]
            output:
              secret-arn:
                value: aws_secretsmanager_secret.secret.arn
            resource:
              aws_secretsmanager_secret:
                secret:
                  name: .vpc_name_.secret_name
              aws_secretsmanager_secret_version:
                secret:
                  secret_id: aws_secretsmanager_secret.secret.id
                  secret_string: .secret
            variable:
              vpc_name: 
              secret: 
              secret_name: 
        terraform: []
        variable:
          vpc_name:
            default: ''
          service:
            default: ''
          admin_database_username:
            default: postgres
          admin_database_name:
            default: postgres
          admin_database_password:
            default: ''
          namespace:
            default: default
          role:
            default: ''
          database_name:
            default: ''
          username:
            default: ''
          password:
            default: ''
          secrets_manager_enabled:
            default: true
          dump_file_to_restore:
            default: ''
          dump_file_storage_location:
            default: ''
          db_restore:
            default: false
          db_dump:
            default: false
          db_job_role_arn:
            default: ''
      grafana-s3-bucket:
        count: '.namespace == "default" && var.deploy_grafana ? 1 : 0'
        bucket_name: .vpc_name-observability-bucket
        resource:
          aws_s3_bucket:
            mybucket:
              bucket: .bucket_name
              lifecycle:
                ignore_changes: tags tags_all
          aws_s3_bucket_server_side_encryption_configuration:
          - default_kms_encryption:
              count: '.aes_encryption ? 0 : var.kms_key_id != "" ? 0 : 1'
              bucket: aws_s3_bucket.mybucket.id
              rule:
                apply_server_side_encryption_by_default:
                  sse_algorithm: aws:kms
          - kms_key_encryption:
              count: '.aes_encryption ? 0 : var.kms_key_id != "" ? 1 : 0'
              bucket: aws_s3_bucket.mybucket.id
              rule:
                apply_server_side_encryption_by_default:
                  kms_master_key_id: .kms_key_id
                  sse_algorithm: aws:kms
          - aes_encryption:
              count: '.aes_encryption ? 1 : 0'
              bucket: aws_s3_bucket.mybucket.id
              rule:
                apply_server_side_encryption_by_default:
                  sse_algorithm: AES256
          aws_s3_bucket_lifecycle_configuration:
            mybucket:
              count: '.bucket_lifecycle_configuration != "" ? 1 : 0'
              bucket: aws_s3_bucket.mybucket.id
              rule:
                status: Enabled
                id: mybucket
                abort_incomplete_multipart_upload:
                  days_after_initiation: 7
          aws_s3_bucket_logging:
            mybucket:
              count: '.logging_bucket_name != "" ? 1 : 0'
              bucket: aws_s3_bucket.mybucket.id
              target_bucket: .logging_bucket_name
              target_prefix: log/.bucket_name/
              lifecycle:
                ignore_changes: all
          aws_s3_bucket_ownership_controls:
            mybucket:
              bucket: aws_s3_bucket.mybucket.id
              rule:
                object_ownership: .bucket_ownership
          aws_s3_bucket_public_access_block:
            mybucket:
              count: '.public_access_block ? 1 : 0'
              bucket: aws_s3_bucket.mybucket.id
              block_public_acls: .block_public_acls
              block_public_policy: .block_public_policy
              ignore_public_acls: .ignore_public_acls
              restrict_public_buckets: .restrict_public_buckets
          aws_s3_bucket_versioning:
            name:
              count: '.versioning ? 1 : 0'
              bucket: aws_s3_bucket.mybucket.id
              versioning_configuration:
                status: Enabled
          aws_s3_bucket_policy:
            mybucket:
              count: '.policy_role_arn != "" ? 1 : 0'
              bucket: aws_s3_bucket.mybucket.id
              policy: 'jsonencode("Version": "2012-10-17", "Statement": ["Effect":
                "Allow", "Principal": "AWS": ".policy_role_arn", "Action":
                ".policy_actions", "Resource": ["aws_s3_bucket.mybucket.arn",
                "aws_s3_bucket.mybucket.arn/*"]])'
        variable:
          bucket_name: 
          bucket_ownership:
            default: BucketOwnerEnforced
          logging_bucket_name:
            type: string
            default: ''
          aes_encryption:
            default: false
          kms_key_id:
            description: The KMS key to use for the bucket
            default: ''
          bucket_lifecycle_configuration:
            default: ''
          public_access_block:
            default: true
          block_public_acls:
            default: true
          block_public_policy:
            default: true
          ignore_public_acls:
            default: true
          restrict_public_buckets:
            default: true
          versioning:
            default: false
          policy_role_arn:
            default: ''
          policy_actions:
            type: list(string)
            default: s3:GetObject s3:PutObject s3:DeleteObject
      indexd-db:
        count: '.indexd_enabled ? 1 : 0'
        vpc_name: .vpc_name
        service: indexd
        admin_database_username: .aurora_username
        admin_database_password: .aurora_password
        namespace: .namespace
        secrets_manager_enabled: true
        data:
          aws_caller_identity:
            current: 
          aws_eks_cluster:
            eks:
              name: .vpc_name
          aws_secretsmanager_secret:
            aurora-master-password:
              name: .vpc_name_aurora-master-password
          aws_secretsmanager_secret_version:
            aurora-master-password:
              secret_id: data.aws_secretsmanager_secret.aurora-master-password.id
          aws_iam_policy_document:
          - policy:
              statement:
                actions: secretsmanager:GetSecretValue secretsmanager:DescribeSecret
                resources: module.secrets_manager[0].secret-arn
          - sa_policy:
              statement:
                actions: sts:AssumeRoleWithWebIdentity
                principals:
                  type: Federated
                  identifiers: arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/local.eks_oidc_issuer
                condition:
                  test: StringEquals
                  variable: local.eks_oidc_issuer:sub
                  values: system:serviceaccount:local.sa_namespace:local.sa_name
          aws_db_instance:
            database:
              db_instance_identifier: .vpc_name-aurora-cluster-instance
        locals:
          sa_name: .service-sa
          sa_namespace: .namespace
          eks_oidc_issuer: trimprefix(data.aws_eks_cluster.eks.identity[0].oidc[0].issuer,
            "https://")
          database_name: '.database_name != "" ? var.database_name : ".service_.namespace"'
          database_username: '.username != "" ? var.username : ".service_.namespace"'
          database_password: '.password != "" ? var.password : random_password.db_password[0].result'
        resource:
          aws_iam_policy:
            secrets_manager_policy:
              count: '.secrets_manager_enabled ? 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-policy
              description: Policy for .vpc_name-.service to access secrets
                manager
              policy: data.aws_iam_policy_document.policy.json
          aws_iam_role:
            role:
              count: '.secrets_manager_enabled ? var.role != "" ? 0 : 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-role
              assume_role_policy: data.aws_iam_policy_document.sa_policy.json
          aws_iam_role_policy_attachment:
            new_attach:
              count: '.secrets_manager_enabled ? 1 : 0'
              role: '.role != "" ? var.role : aws_iam_role.role[0].name'
              policy_arn: aws_iam_policy.secrets_manager_policy[0].arn
          random_password:
            db_password:
              count: '.password != "" ? 0 : 1'
              length: 16
              special: false
          null_resource:
          - db_setup:
              provisioner:
                local-exec:
                  command: psql -h data.aws_db_instance.database.address -U .admin_database_username
                    -d .admin_database_name -c "CREATE DATABASE \"local.database_name\";"
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
                  on_failure: continue
              triggers:
                database: local.database_name
          - user_setup:
              provisioner:
                local-exec:
                  command: "psql -h data.aws_db_instance.database.address -U .admin_database_username\
                    \ -d .admin_database_name -c \"templatefile(\"path.module/db_setup.tftpl\"\
                    , \n          username  = local.database_username\n         \
                    \ database  = local.database_name\n          password  = local.database_password\n\
                    \        )\""
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.db_setup
          - db_restore:
              count: '.db_restore && var.dump_file_to_restore != "" ? 1 : 0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n\naws s3 cp \".dump_file_to_restore\" - --quiet |\
                    \ psql -h \"data.aws_db_instance.database.address\" -U \"local.database_username\"\
                    \ -d \"local.database_name\"\necho \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
          - db_dump:
              count: '.db_dump && var.dump_file_storage_location != "" ? 1 :
                0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n    \npg_dump --username=\"local.database_username\"\
                    \ --dbname=\"local.database_name\" --host=\"data.aws_db_instance.database.address\"\
                    \ --no-password --no-owner --no-privileges >> ./dump.sql && aws\
                    \ s3 cp ./dump.sql .dump_file_storage_location && rm ./dump.sql\n\
                    echo \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
        module:
          secrets_manager:
            count: '.secrets_manager_enabled ? 1 : 0'
            vpc_name: .vpc_name
            secret: 'templatefile("path.module/secrets_manager.tftpl", "hostname":
              "data.aws_db_instance.database.address", "database": "local.database_name",
              "username": "local.database_username", "password": "local.database_password")'
            secret_name: .namespace-.service-creds
            depends_on: null_resource.user_setup
            data:
              aws_caller_identity:
                current: 
              aws_vpcs:
                vpcs:
                  tags:
                    Name: .vpc_name
              aws_security_group:
                private:
                  vpc_id: data.aws_vpc.the_vpc.id
                  name: local
              aws_vpc:
                the_vpc:
                  id: data.aws_vpcs.vpcs.ids[0]
            output:
              secret-arn:
                value: aws_secretsmanager_secret.secret.arn
            resource:
              aws_secretsmanager_secret:
                secret:
                  name: .vpc_name_.secret_name
              aws_secretsmanager_secret_version:
                secret:
                  secret_id: aws_secretsmanager_secret.secret.id
                  secret_string: .secret
            variable:
              vpc_name: 
              secret: 
              secret_name: 
        terraform: []
        variable:
          vpc_name:
            default: ''
          service:
            default: ''
          admin_database_username:
            default: postgres
          admin_database_name:
            default: postgres
          admin_database_password:
            default: ''
          namespace:
            default: default
          role:
            default: ''
          database_name:
            default: ''
          username:
            default: ''
          password:
            default: ''
          secrets_manager_enabled:
            default: true
          dump_file_to_restore:
            default: ''
          dump_file_storage_location:
            default: ''
          db_restore:
            default: false
          db_dump:
            default: false
          db_job_role_arn:
            default: ''
      manifest-s3-bucket:
        bucket_name: manifestservice-.vpc_name-.namespace
        resource:
          aws_s3_bucket:
            mybucket:
              bucket: .bucket_name
              lifecycle:
                ignore_changes: tags tags_all
          aws_s3_bucket_server_side_encryption_configuration:
          - default_kms_encryption:
              count: '.aes_encryption ? 0 : var.kms_key_id != "" ? 0 : 1'
              bucket: aws_s3_bucket.mybucket.id
              rule:
                apply_server_side_encryption_by_default:
                  sse_algorithm: aws:kms
          - kms_key_encryption:
              count: '.aes_encryption ? 0 : var.kms_key_id != "" ? 1 : 0'
              bucket: aws_s3_bucket.mybucket.id
              rule:
                apply_server_side_encryption_by_default:
                  kms_master_key_id: .kms_key_id
                  sse_algorithm: aws:kms
          - aes_encryption:
              count: '.aes_encryption ? 1 : 0'
              bucket: aws_s3_bucket.mybucket.id
              rule:
                apply_server_side_encryption_by_default:
                  sse_algorithm: AES256
          aws_s3_bucket_lifecycle_configuration:
            mybucket:
              count: '.bucket_lifecycle_configuration != "" ? 1 : 0'
              bucket: aws_s3_bucket.mybucket.id
              rule:
                status: Enabled
                id: mybucket
                abort_incomplete_multipart_upload:
                  days_after_initiation: 7
          aws_s3_bucket_logging:
            mybucket:
              count: '.logging_bucket_name != "" ? 1 : 0'
              bucket: aws_s3_bucket.mybucket.id
              target_bucket: .logging_bucket_name
              target_prefix: log/.bucket_name/
              lifecycle:
                ignore_changes: all
          aws_s3_bucket_ownership_controls:
            mybucket:
              bucket: aws_s3_bucket.mybucket.id
              rule:
                object_ownership: .bucket_ownership
          aws_s3_bucket_public_access_block:
            mybucket:
              count: '.public_access_block ? 1 : 0'
              bucket: aws_s3_bucket.mybucket.id
              block_public_acls: .block_public_acls
              block_public_policy: .block_public_policy
              ignore_public_acls: .ignore_public_acls
              restrict_public_buckets: .restrict_public_buckets
          aws_s3_bucket_versioning:
            name:
              count: '.versioning ? 1 : 0'
              bucket: aws_s3_bucket.mybucket.id
              versioning_configuration:
                status: Enabled
          aws_s3_bucket_policy:
            mybucket:
              count: '.policy_role_arn != "" ? 1 : 0'
              bucket: aws_s3_bucket.mybucket.id
              policy: 'jsonencode("Version": "2012-10-17", "Statement": ["Effect":
                "Allow", "Principal": "AWS": ".policy_role_arn", "Action":
                ".policy_actions", "Resource": ["aws_s3_bucket.mybucket.arn",
                "aws_s3_bucket.mybucket.arn/*"]])'
        variable:
          bucket_name: 
          bucket_ownership:
            default: BucketOwnerEnforced
          logging_bucket_name:
            type: string
            default: ''
          aes_encryption:
            default: false
          kms_key_id:
            description: The KMS key to use for the bucket
            default: ''
          bucket_lifecycle_configuration:
            default: ''
          public_access_block:
            default: true
          block_public_acls:
            default: true
          block_public_policy:
            default: true
          ignore_public_acls:
            default: true
          restrict_public_buckets:
            default: true
          versioning:
            default: false
          policy_role_arn:
            default: ''
          policy_actions:
            type: list(string)
            default: s3:GetObject s3:PutObject s3:DeleteObject
      metadata-db:
        count: '.metadata_enabled ? 1 : 0'
        vpc_name: .vpc_name
        service: metadata
        admin_database_username: .aurora_username
        admin_database_password: .aurora_password
        namespace: .namespace
        secrets_manager_enabled: true
        data:
          aws_caller_identity:
            current: 
          aws_eks_cluster:
            eks:
              name: .vpc_name
          aws_secretsmanager_secret:
            aurora-master-password:
              name: .vpc_name_aurora-master-password
          aws_secretsmanager_secret_version:
            aurora-master-password:
              secret_id: data.aws_secretsmanager_secret.aurora-master-password.id
          aws_iam_policy_document:
          - policy:
              statement:
                actions: secretsmanager:GetSecretValue secretsmanager:DescribeSecret
                resources: module.secrets_manager[0].secret-arn
          - sa_policy:
              statement:
                actions: sts:AssumeRoleWithWebIdentity
                principals:
                  type: Federated
                  identifiers: arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/local.eks_oidc_issuer
                condition:
                  test: StringEquals
                  variable: local.eks_oidc_issuer:sub
                  values: system:serviceaccount:local.sa_namespace:local.sa_name
          aws_db_instance:
            database:
              db_instance_identifier: .vpc_name-aurora-cluster-instance
        locals:
          sa_name: .service-sa
          sa_namespace: .namespace
          eks_oidc_issuer: trimprefix(data.aws_eks_cluster.eks.identity[0].oidc[0].issuer,
            "https://")
          database_name: '.database_name != "" ? var.database_name : ".service_.namespace"'
          database_username: '.username != "" ? var.username : ".service_.namespace"'
          database_password: '.password != "" ? var.password : random_password.db_password[0].result'
        resource:
          aws_iam_policy:
            secrets_manager_policy:
              count: '.secrets_manager_enabled ? 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-policy
              description: Policy for .vpc_name-.service to access secrets
                manager
              policy: data.aws_iam_policy_document.policy.json
          aws_iam_role:
            role:
              count: '.secrets_manager_enabled ? var.role != "" ? 0 : 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-role
              assume_role_policy: data.aws_iam_policy_document.sa_policy.json
          aws_iam_role_policy_attachment:
            new_attach:
              count: '.secrets_manager_enabled ? 1 : 0'
              role: '.role != "" ? var.role : aws_iam_role.role[0].name'
              policy_arn: aws_iam_policy.secrets_manager_policy[0].arn
          random_password:
            db_password:
              count: '.password != "" ? 0 : 1'
              length: 16
              special: false
          null_resource:
          - db_setup:
              provisioner:
                local-exec:
                  command: psql -h data.aws_db_instance.database.address -U .admin_database_username
                    -d .admin_database_name -c "CREATE DATABASE \"local.database_name\";"
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
                  on_failure: continue
              triggers:
                database: local.database_name
          - user_setup:
              provisioner:
                local-exec:
                  command: "psql -h data.aws_db_instance.database.address -U .admin_database_username\
                    \ -d .admin_database_name -c \"templatefile(\"path.module/db_setup.tftpl\"\
                    , \n          username  = local.database_username\n         \
                    \ database  = local.database_name\n          password  = local.database_password\n\
                    \        )\""
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.db_setup
          - db_restore:
              count: '.db_restore && var.dump_file_to_restore != "" ? 1 : 0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n\naws s3 cp \".dump_file_to_restore\" - --quiet |\
                    \ psql -h \"data.aws_db_instance.database.address\" -U \"local.database_username\"\
                    \ -d \"local.database_name\"\necho \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
          - db_dump:
              count: '.db_dump && var.dump_file_storage_location != "" ? 1 :
                0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n    \npg_dump --username=\"local.database_username\"\
                    \ --dbname=\"local.database_name\" --host=\"data.aws_db_instance.database.address\"\
                    \ --no-password --no-owner --no-privileges >> ./dump.sql && aws\
                    \ s3 cp ./dump.sql .dump_file_storage_location && rm ./dump.sql\n\
                    echo \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
        module:
          secrets_manager:
            count: '.secrets_manager_enabled ? 1 : 0'
            vpc_name: .vpc_name
            secret: 'templatefile("path.module/secrets_manager.tftpl", "hostname":
              "data.aws_db_instance.database.address", "database": "local.database_name",
              "username": "local.database_username", "password": "local.database_password")'
            secret_name: .namespace-.service-creds
            depends_on: null_resource.user_setup
            data:
              aws_caller_identity:
                current: 
              aws_vpcs:
                vpcs:
                  tags:
                    Name: .vpc_name
              aws_security_group:
                private:
                  vpc_id: data.aws_vpc.the_vpc.id
                  name: local
              aws_vpc:
                the_vpc:
                  id: data.aws_vpcs.vpcs.ids[0]
            output:
              secret-arn:
                value: aws_secretsmanager_secret.secret.arn
            resource:
              aws_secretsmanager_secret:
                secret:
                  name: .vpc_name_.secret_name
              aws_secretsmanager_secret_version:
                secret:
                  secret_id: aws_secretsmanager_secret.secret.id
                  secret_string: .secret
            variable:
              vpc_name: 
              secret: 
              secret_name: 
        terraform: []
        variable:
          vpc_name:
            default: ''
          service:
            default: ''
          admin_database_username:
            default: postgres
          admin_database_name:
            default: postgres
          admin_database_password:
            default: ''
          namespace:
            default: default
          role:
            default: ''
          database_name:
            default: ''
          username:
            default: ''
          password:
            default: ''
          secrets_manager_enabled:
            default: true
          dump_file_to_restore:
            default: ''
          dump_file_storage_location:
            default: ''
          db_restore:
            default: false
          db_dump:
            default: false
          db_job_role_arn:
            default: ''
      requestor-db:
        count: '.requestor_enabled ? 1 : 0'
        vpc_name: .vpc_name
        service: requestor
        admin_database_username: .aurora_username
        admin_database_password: .aurora_password
        namespace: .namespace
        secrets_manager_enabled: true
        data:
          aws_caller_identity:
            current: 
          aws_eks_cluster:
            eks:
              name: .vpc_name
          aws_secretsmanager_secret:
            aurora-master-password:
              name: .vpc_name_aurora-master-password
          aws_secretsmanager_secret_version:
            aurora-master-password:
              secret_id: data.aws_secretsmanager_secret.aurora-master-password.id
          aws_iam_policy_document:
          - policy:
              statement:
                actions: secretsmanager:GetSecretValue secretsmanager:DescribeSecret
                resources: module.secrets_manager[0].secret-arn
          - sa_policy:
              statement:
                actions: sts:AssumeRoleWithWebIdentity
                principals:
                  type: Federated
                  identifiers: arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/local.eks_oidc_issuer
                condition:
                  test: StringEquals
                  variable: local.eks_oidc_issuer:sub
                  values: system:serviceaccount:local.sa_namespace:local.sa_name
          aws_db_instance:
            database:
              db_instance_identifier: .vpc_name-aurora-cluster-instance
        locals:
          sa_name: .service-sa
          sa_namespace: .namespace
          eks_oidc_issuer: trimprefix(data.aws_eks_cluster.eks.identity[0].oidc[0].issuer,
            "https://")
          database_name: '.database_name != "" ? var.database_name : ".service_.namespace"'
          database_username: '.username != "" ? var.username : ".service_.namespace"'
          database_password: '.password != "" ? var.password : random_password.db_password[0].result'
        resource:
          aws_iam_policy:
            secrets_manager_policy:
              count: '.secrets_manager_enabled ? 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-policy
              description: Policy for .vpc_name-.service to access secrets
                manager
              policy: data.aws_iam_policy_document.policy.json
          aws_iam_role:
            role:
              count: '.secrets_manager_enabled ? var.role != "" ? 0 : 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-role
              assume_role_policy: data.aws_iam_policy_document.sa_policy.json
          aws_iam_role_policy_attachment:
            new_attach:
              count: '.secrets_manager_enabled ? 1 : 0'
              role: '.role != "" ? var.role : aws_iam_role.role[0].name'
              policy_arn: aws_iam_policy.secrets_manager_policy[0].arn
          random_password:
            db_password:
              count: '.password != "" ? 0 : 1'
              length: 16
              special: false
          null_resource:
          - db_setup:
              provisioner:
                local-exec:
                  command: psql -h data.aws_db_instance.database.address -U .admin_database_username
                    -d .admin_database_name -c "CREATE DATABASE \"local.database_name\";"
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
                  on_failure: continue
              triggers:
                database: local.database_name
          - user_setup:
              provisioner:
                local-exec:
                  command: "psql -h data.aws_db_instance.database.address -U .admin_database_username\
                    \ -d .admin_database_name -c \"templatefile(\"path.module/db_setup.tftpl\"\
                    , \n          username  = local.database_username\n         \
                    \ database  = local.database_name\n          password  = local.database_password\n\
                    \        )\""
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.db_setup
          - db_restore:
              count: '.db_restore && var.dump_file_to_restore != "" ? 1 : 0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n\naws s3 cp \".dump_file_to_restore\" - --quiet |\
                    \ psql -h \"data.aws_db_instance.database.address\" -U \"local.database_username\"\
                    \ -d \"local.database_name\"\necho \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
          - db_dump:
              count: '.db_dump && var.dump_file_storage_location != "" ? 1 :
                0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n    \npg_dump --username=\"local.database_username\"\
                    \ --dbname=\"local.database_name\" --host=\"data.aws_db_instance.database.address\"\
                    \ --no-password --no-owner --no-privileges >> ./dump.sql && aws\
                    \ s3 cp ./dump.sql .dump_file_storage_location && rm ./dump.sql\n\
                    echo \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
        module:
          secrets_manager:
            count: '.secrets_manager_enabled ? 1 : 0'
            vpc_name: .vpc_name
            secret: 'templatefile("path.module/secrets_manager.tftpl", "hostname":
              "data.aws_db_instance.database.address", "database": "local.database_name",
              "username": "local.database_username", "password": "local.database_password")'
            secret_name: .namespace-.service-creds
            depends_on: null_resource.user_setup
            data:
              aws_caller_identity:
                current: 
              aws_vpcs:
                vpcs:
                  tags:
                    Name: .vpc_name
              aws_security_group:
                private:
                  vpc_id: data.aws_vpc.the_vpc.id
                  name: local
              aws_vpc:
                the_vpc:
                  id: data.aws_vpcs.vpcs.ids[0]
            output:
              secret-arn:
                value: aws_secretsmanager_secret.secret.arn
            resource:
              aws_secretsmanager_secret:
                secret:
                  name: .vpc_name_.secret_name
              aws_secretsmanager_secret_version:
                secret:
                  secret_id: aws_secretsmanager_secret.secret.id
                  secret_string: .secret
            variable:
              vpc_name: 
              secret: 
              secret_name: 
        terraform: []
        variable:
          vpc_name:
            default: ''
          service:
            default: ''
          admin_database_username:
            default: postgres
          admin_database_name:
            default: postgres
          admin_database_password:
            default: ''
          namespace:
            default: default
          role:
            default: ''
          database_name:
            default: ''
          username:
            default: ''
          password:
            default: ''
          secrets_manager_enabled:
            default: true
          dump_file_to_restore:
            default: ''
          dump_file_storage_location:
            default: ''
          db_restore:
            default: false
          db_dump:
            default: false
          db_job_role_arn:
            default: ''
      sheepdog-db:
        count: '.sheepdog_enabled ? 1 : 0'
        vpc_name: .vpc_name
        service: sheepdog
        admin_database_username: .aurora_username
        admin_database_password: .aurora_password
        namespace: .namespace
        secrets_manager_enabled: true
        data:
          aws_caller_identity:
            current: 
          aws_eks_cluster:
            eks:
              name: .vpc_name
          aws_secretsmanager_secret:
            aurora-master-password:
              name: .vpc_name_aurora-master-password
          aws_secretsmanager_secret_version:
            aurora-master-password:
              secret_id: data.aws_secretsmanager_secret.aurora-master-password.id
          aws_iam_policy_document:
          - policy:
              statement:
                actions: secretsmanager:GetSecretValue secretsmanager:DescribeSecret
                resources: module.secrets_manager[0].secret-arn
          - sa_policy:
              statement:
                actions: sts:AssumeRoleWithWebIdentity
                principals:
                  type: Federated
                  identifiers: arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/local.eks_oidc_issuer
                condition:
                  test: StringEquals
                  variable: local.eks_oidc_issuer:sub
                  values: system:serviceaccount:local.sa_namespace:local.sa_name
          aws_db_instance:
            database:
              db_instance_identifier: .vpc_name-aurora-cluster-instance
        locals:
          sa_name: .service-sa
          sa_namespace: .namespace
          eks_oidc_issuer: trimprefix(data.aws_eks_cluster.eks.identity[0].oidc[0].issuer,
            "https://")
          database_name: '.database_name != "" ? var.database_name : ".service_.namespace"'
          database_username: '.username != "" ? var.username : ".service_.namespace"'
          database_password: '.password != "" ? var.password : random_password.db_password[0].result'
        resource:
          aws_iam_policy:
            secrets_manager_policy:
              count: '.secrets_manager_enabled ? 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-policy
              description: Policy for .vpc_name-.service to access secrets
                manager
              policy: data.aws_iam_policy_document.policy.json
          aws_iam_role:
            role:
              count: '.secrets_manager_enabled ? var.role != "" ? 0 : 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-role
              assume_role_policy: data.aws_iam_policy_document.sa_policy.json
          aws_iam_role_policy_attachment:
            new_attach:
              count: '.secrets_manager_enabled ? 1 : 0'
              role: '.role != "" ? var.role : aws_iam_role.role[0].name'
              policy_arn: aws_iam_policy.secrets_manager_policy[0].arn
          random_password:
            db_password:
              count: '.password != "" ? 0 : 1'
              length: 16
              special: false
          null_resource:
          - db_setup:
              provisioner:
                local-exec:
                  command: psql -h data.aws_db_instance.database.address -U .admin_database_username
                    -d .admin_database_name -c "CREATE DATABASE \"local.database_name\";"
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
                  on_failure: continue
              triggers:
                database: local.database_name
          - user_setup:
              provisioner:
                local-exec:
                  command: "psql -h data.aws_db_instance.database.address -U .admin_database_username\
                    \ -d .admin_database_name -c \"templatefile(\"path.module/db_setup.tftpl\"\
                    , \n          username  = local.database_username\n         \
                    \ database  = local.database_name\n          password  = local.database_password\n\
                    \        )\""
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.db_setup
          - db_restore:
              count: '.db_restore && var.dump_file_to_restore != "" ? 1 : 0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n\naws s3 cp \".dump_file_to_restore\" - --quiet |\
                    \ psql -h \"data.aws_db_instance.database.address\" -U \"local.database_username\"\
                    \ -d \"local.database_name\"\necho \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
          - db_dump:
              count: '.db_dump && var.dump_file_storage_location != "" ? 1 :
                0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n    \npg_dump --username=\"local.database_username\"\
                    \ --dbname=\"local.database_name\" --host=\"data.aws_db_instance.database.address\"\
                    \ --no-password --no-owner --no-privileges >> ./dump.sql && aws\
                    \ s3 cp ./dump.sql .dump_file_storage_location && rm ./dump.sql\n\
                    echo \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
        module:
          secrets_manager:
            count: '.secrets_manager_enabled ? 1 : 0'
            vpc_name: .vpc_name
            secret: 'templatefile("path.module/secrets_manager.tftpl", "hostname":
              "data.aws_db_instance.database.address", "database": "local.database_name",
              "username": "local.database_username", "password": "local.database_password")'
            secret_name: .namespace-.service-creds
            depends_on: null_resource.user_setup
            data:
              aws_caller_identity:
                current: 
              aws_vpcs:
                vpcs:
                  tags:
                    Name: .vpc_name
              aws_security_group:
                private:
                  vpc_id: data.aws_vpc.the_vpc.id
                  name: local
              aws_vpc:
                the_vpc:
                  id: data.aws_vpcs.vpcs.ids[0]
            output:
              secret-arn:
                value: aws_secretsmanager_secret.secret.arn
            resource:
              aws_secretsmanager_secret:
                secret:
                  name: .vpc_name_.secret_name
              aws_secretsmanager_secret_version:
                secret:
                  secret_id: aws_secretsmanager_secret.secret.id
                  secret_string: .secret
            variable:
              vpc_name: 
              secret: 
              secret_name: 
        terraform: []
        variable:
          vpc_name:
            default: ''
          service:
            default: ''
          admin_database_username:
            default: postgres
          admin_database_name:
            default: postgres
          admin_database_password:
            default: ''
          namespace:
            default: default
          role:
            default: ''
          database_name:
            default: ''
          username:
            default: ''
          password:
            default: ''
          secrets_manager_enabled:
            default: true
          dump_file_to_restore:
            default: ''
          dump_file_storage_location:
            default: ''
          db_restore:
            default: false
          db_dump:
            default: false
          db_job_role_arn:
            default: ''
      ssjdispatcher-sqs:
        sqs_name: ssjdispatcher
        output:
          sqs-url:
            value: aws_sqs_queue.generic_queue.id
          sqs-arn:
            value: aws_sqs_queue.generic_queue.arn
        resource:
          aws_sqs_queue:
            generic_queue:
              name: .sqs_name
              visibility_timeout_seconds: 300
              message_retention_seconds: 1209600
              tags:
                Organization: gen3
                description: Created by SQS module
        variable:
          sqs_name: 
          slack_webhook:
            default: ''
      wts-db:
        count: '.wts_enabled ? 1 : 0'
        vpc_name: .vpc_name
        service: wts
        admin_database_username: .aurora_username
        admin_database_password: .aurora_password
        namespace: .namespace
        secrets_manager_enabled: true
        data:
          aws_caller_identity:
            current: 
          aws_eks_cluster:
            eks:
              name: .vpc_name
          aws_secretsmanager_secret:
            aurora-master-password:
              name: .vpc_name_aurora-master-password
          aws_secretsmanager_secret_version:
            aurora-master-password:
              secret_id: data.aws_secretsmanager_secret.aurora-master-password.id
          aws_iam_policy_document:
          - policy:
              statement:
                actions: secretsmanager:GetSecretValue secretsmanager:DescribeSecret
                resources: module.secrets_manager[0].secret-arn
          - sa_policy:
              statement:
                actions: sts:AssumeRoleWithWebIdentity
                principals:
                  type: Federated
                  identifiers: arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/local.eks_oidc_issuer
                condition:
                  test: StringEquals
                  variable: local.eks_oidc_issuer:sub
                  values: system:serviceaccount:local.sa_namespace:local.sa_name
          aws_db_instance:
            database:
              db_instance_identifier: .vpc_name-aurora-cluster-instance
        locals:
          sa_name: .service-sa
          sa_namespace: .namespace
          eks_oidc_issuer: trimprefix(data.aws_eks_cluster.eks.identity[0].oidc[0].issuer,
            "https://")
          database_name: '.database_name != "" ? var.database_name : ".service_.namespace"'
          database_username: '.username != "" ? var.username : ".service_.namespace"'
          database_password: '.password != "" ? var.password : random_password.db_password[0].result'
        resource:
          aws_iam_policy:
            secrets_manager_policy:
              count: '.secrets_manager_enabled ? 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-policy
              description: Policy for .vpc_name-.service to access secrets
                manager
              policy: data.aws_iam_policy_document.policy.json
          aws_iam_role:
            role:
              count: '.secrets_manager_enabled ? var.role != "" ? 0 : 1 : 0'
              name: .vpc_name-.service-.namespace-creds-access-role
              assume_role_policy: data.aws_iam_policy_document.sa_policy.json
          aws_iam_role_policy_attachment:
            new_attach:
              count: '.secrets_manager_enabled ? 1 : 0'
              role: '.role != "" ? var.role : aws_iam_role.role[0].name'
              policy_arn: aws_iam_policy.secrets_manager_policy[0].arn
          random_password:
            db_password:
              count: '.password != "" ? 0 : 1'
              length: 16
              special: false
          null_resource:
          - db_setup:
              provisioner:
                local-exec:
                  command: psql -h data.aws_db_instance.database.address -U .admin_database_username
                    -d .admin_database_name -c "CREATE DATABASE \"local.database_name\";"
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
                  on_failure: continue
              triggers:
                database: local.database_name
          - user_setup:
              provisioner:
                local-exec:
                  command: "psql -h data.aws_db_instance.database.address -U .admin_database_username\
                    \ -d .admin_database_name -c \"templatefile(\"path.module/db_setup.tftpl\"\
                    , \n          username  = local.database_username\n         \
                    \ database  = local.database_name\n          password  = local.database_password\n\
                    \        )\""
                  environment:
                    PGPASSWORD: '.admin_database_password != "" ? var.admin_database_password
                      : data.aws_secretsmanager_secret_version.aurora-master-password.secret_string'
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.db_setup
          - db_restore:
              count: '.db_restore && var.dump_file_to_restore != "" ? 1 : 0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n\naws s3 cp \".dump_file_to_restore\" - --quiet |\
                    \ psql -h \"data.aws_db_instance.database.address\" -U \"local.database_username\"\
                    \ -d \"local.database_name\"\necho \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
          - db_dump:
              count: '.db_dump && var.dump_file_storage_location != "" ? 1 :
                0'
              provisioner:
                local-exec:
                  interpreter: /bin/bash -c
                  command: "# If we have a role to assume, then assume it and set\
                    \ the credentials\nif [[ .db_job_role_arn != \"\" ]]; then\n\
                    \  CREDENTIALS=(`aws sts assume-role --role-arn .db_job_role_arn\
                    \ --role-session-name \"db-migration-cli\" --query \"[Credentials.AccessKeyId,Credentials.SecretAccessKey,Credentials.SessionToken]\"\
                    \ --output text`)\n  unset AWS_PROFILE\n  export AWS_DEFAULT_REGION=us-east-1\n\
                    \  export AWS_ACCESS_KEY_ID=\"$CREDENTIALS[0]\"\n  export AWS_SECRET_ACCESS_KEY=\"\
                    $CREDENTIALS[1]\"\n  export AWS_SESSION_TOKEN=\"$CREDENTIALS[2]\"\
                    \nfi\n    \npg_dump --username=\"local.database_username\"\
                    \ --dbname=\"local.database_name\" --host=\"data.aws_db_instance.database.address\"\
                    \ --no-password --no-owner --no-privileges >> ./dump.sql && aws\
                    \ s3 cp ./dump.sql .dump_file_storage_location && rm ./dump.sql\n\
                    echo \"Done restoring database\""
                  environment:
                    PGPASSWORD: local.database_password
              triggers:
                username: local.database_username
                database: local.database_name
                password: local.database_password
              depends_on: null_resource.user_setup
        module:
          secrets_manager:
            count: '.secrets_manager_enabled ? 1 : 0'
            vpc_name: .vpc_name
            secret: 'templatefile("path.module/secrets_manager.tftpl", "hostname":
              "data.aws_db_instance.database.address", "database": "local.database_name",
              "username": "local.database_username", "password": "local.database_password")'
            secret_name: .namespace-.service-creds
            depends_on: null_resource.user_setup
            data:
              aws_caller_identity:
                current: 
              aws_vpcs:
                vpcs:
                  tags:
                    Name: .vpc_name
              aws_security_group:
                private:
                  vpc_id: data.aws_vpc.the_vpc.id
                  name: local
              aws_vpc:
                the_vpc:
                  id: data.aws_vpcs.vpcs.ids[0]
            output:
              secret-arn:
                value: aws_secretsmanager_secret.secret.arn
            resource:
              aws_secretsmanager_secret:
                secret:
                  name: .vpc_name_.secret_name
              aws_secretsmanager_secret_version:
                secret:
                  secret_id: aws_secretsmanager_secret.secret.id
                  secret_string: .secret
            variable:
              vpc_name: 
              secret: 
              secret_name: 
        terraform: []
        variable:
          vpc_name:
            default: ''
          service:
            default: ''
          admin_database_username:
            default: postgres
          admin_database_name:
            default: postgres
          admin_database_password:
            default: ''
          namespace:
            default: default
          role:
            default: ''
          database_name:
            default: ''
          username:
            default: ''
          password:
            default: ''
          secrets_manager_enabled:
            default: true
          dump_file_to_restore:
            default: ''
          dump_file_storage_location:
            default: ''
          db_restore:
            default: false
          db_dump:
            default: false
          db_job_role_arn:
            default: ''
    data:
      aws_caller_identity:
        current: 
    locals:
      values: 'templatefile("path.module/values.tftpl", "account_id": "data.aws_caller_identity.current.account_id",
        "namespace": ".namespace", "ambassador_enabled": ".ambassador_enabled",
        "arborist_enabled": ".arborist_enabled", "argo_enabled": ".argo_enabled",
        "audit_enabled": ".audit_enabled", "audit_service_account": "aws_iam_role.audit-role[0].arn",
        "aurora_hostname": ".aurora_hostname", "aurora_username": ".aurora_username",
        "aurora_password": ".aurora_password", "aws-es-proxy_enabled": ".aws-es-proxy_enabled",
        "dbgap_enabled": ".dbgap_enabled", "dd_enabled": ".dd_enabled",
        "external_secrets_operator_iam_role": "aws_iam_role.external-secrets-role[0].arn",
        "deploy_grafana": ".deploy_grafana", "deploy_s3_mountpoint": ".deploy_s3_mountpoint",
        "dicom-server_enabled": ".dicom-server_enabled", "dicom-viewer_enabled":
        ".dicom-viewer_enabled", "dictionary_url": ".dictionary_url",
        "dispatcher_job_number": ".dispatcher_job_number", "es_endpoint": ".es_endpoint",
        "es_secret_name": "aws_secretsmanager_secret.es_user_creds.name", "fence_config_secret_name":
        "aws_secretsmanager_secret.fence_config.name", "fence_enabled": ".fence_enabled",
        "fence_service_account": "aws_iam_role.fence-role[0].arn", "frontend_root":
        ".gen3ff_enabled ? "gen3ff" : "portal"", "gitops_file": ".gitops_path
        != "" ? indent(4, file(var.gitops_path)) : """, "gen3ff_enabled": ".gen3ff_enabled",
        "gen3ff_repo": ".gen3ff_repo", "gen3ff_tag": ".gen3ff_tag", "guppy_enabled":
        ".guppy_enabled", "hatchery_enabled": ".hatchery_enabled", "hatchery_service_account":
        "aws_iam_role.hatchery-role[0].arn", "hostname": ".hostname", "indexd_enabled":
        ".indexd_enabled", "indexd_prefix": ".indexd_prefix", "ingress_enabled":
        ".ingress_enabled", "manifestservice_enabled": ".manifestservice_enabled",
        "metadata_enabled": ".metadata_enabled", "netpolicy_enabled": ".netpolicy_enabled",
        "peregrine_enabled": ".peregrine_enabled", "pidgin_enabled": ".pidgin_enabled",
        "portal_enabled": ".portal_enabled", "public_datasets": ".public_datasets",
        "requestor_enabled": ".requestor_enabled", "revproxy_arn": ".revproxy_arn",
        "revproxy_enabled": ".revproxy_enabled", "sheepdog_enabled": ".sheepdog_enabled",
        "slack_send_dbgap": ".slack_send_dbgap", "slack_webhook": ".slack_webhook",
        "ssjdispatcher_enabled": ".ssjdispatcher_enabled", "sower_enabled":
        ".sower_enabled", "tier_access_level": ".tier_access_level", "tier_access_limit":
        ".tier_access_limit", "usersync_enabled": ".usersync_enabled",
        "usersync_schedule": ".usersync_schedule", "user_yaml": ".useryaml_path
        != "" ? indent(4, file(var.useryaml_path)) : """, "useryaml_s3_path": ".useryaml_s3_path",
        "vpc_name": ".vpc_name", "waf_arn": ".waf_arn", "wts_enabled":
        ".wts_enabled")'
    resource:
      helm_release:
        gen3:
          count: '.deploy_gen3 ? 1 : 0'
          name: .namespace
          repository: http://helm.gen3.org
          chart: gen3
          namespace: .namespace
          create_namespace: true
          wait: false
          values: local.values
      local_file:
        values:
          count: '.deploy_gen3 ? 1 : 0'
          filename: values.yaml
          content: local.values
      aws_secretsmanager_secret:
      - secret:
          name: .vpc_name_.namespace-values
      - fence_config:
          name: .vpc_name_.namespace-fence-config
      - es_user_creds:
          name: .vpc_name_.namespace-aws-es-proxy-creds
      aws_secretsmanager_secret_version:
      - secret:
          secret_id: aws_secretsmanager_secret.secret.id
          secret_string: local.values
      - fence_config:
          secret_id: aws_secretsmanager_secret.fence_config.id
          secret_string: '.fence_config_path != "" ? file(var.fence_config_path)
            : templatefile("path.module/fence-config.tftpl", "hostname": ".hostname",
            "google_client_id": ".google_client_id", "google_client_secret":
            ".google_client_secret", "fence_access_key": ".fence_access_key",
            "fence_secret_key": ".fence_secret_key", "upload_bucket": ".upload_bucket")'
          lifecycle:
            ignore_changes: secret_string
      - es_user_creds:
          secret_id: aws_secretsmanager_secret.es_user_creds.id
          secret_string: 'templatefile("path.module/aws-user-creds.tftpl", "access_key":
            ".es_user_key", "access_secret": ".es_user_secret")'
      aws_iam_role:
      - audit-role:
          count: '.audit_enabled ? 1 : 0'
          name: .vpc_name-.namespace-audit-sa
          description: Role for ES proxy service account for .vpc_name
          assume_role_policy: 'jsonencode("Version": "2012-10-17", "Statement":
            ["Effect": "Allow", "Principal": "Service": "ec2.amazonaws.com", "Action":
            "sts:AssumeRole", "Sid": "", "Effect": "Allow", "Principal": "Federated":
            "arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/.oidc_provider_arn",
            "Action": "sts:AssumeRoleWithWebIdentity", "Condition": "StringEquals":
            ".oidc_provider_arn:sub": ["system:serviceaccount:.namespace:audit-sa"],
            ".oidc_provider_arn:aud": "sts.amazonaws.com"])'
          path: /gen3-service/
      - fence-role:
          count: '.fence_enabled ? 1 : 0'
          name: .vpc_name-.namespace-fence-sa
          description: Role for ES proxy service account for .vpc_name
          assume_role_policy: 'jsonencode("Version": "2012-10-17", "Statement":
            ["Effect": "Allow", "Principal": "Service": "ec2.amazonaws.com", "Action":
            "sts:AssumeRole", "Sid": "", "Effect": "Allow", "Principal": "Federated":
            "arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/.oidc_provider_arn",
            "Action": "sts:AssumeRoleWithWebIdentity", "Condition": "StringEquals":
            ".oidc_provider_arn:sub": ["system:serviceaccount:.namespace:fence-sa"],
            ".oidc_provider_arn:aud": "sts.amazonaws.com"])'
          path: /gen3-service/
      - gitops-role:
          name: .vpc_name-.namespace-gitops-sa
          description: Role for gitops service account for .vpc_name
          assume_role_policy: 'jsonencode("Version": "2012-10-17", "Statement":
            ["Effect": "Allow", "Principal": "Service": "ec2.amazonaws.com", "Action":
            "sts:AssumeRole", "Sid": "", "Effect": "Allow", "Principal": "Federated":
            "arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/.oidc_provider_arn",
            "Action": "sts:AssumeRoleWithWebIdentity", "Condition": "StringEquals":
            ".oidc_provider_arn:sub": ["system:serviceaccount:.namespace:gitops-sa"],
            ".oidc_provider_arn:aud": "sts.amazonaws.com"])'
          path: /gen3-service/
      - hatchery-role:
          count: '.hatchery_enabled ? 1 : 0'
          name: .vpc_name-.namespace-hatchery-sa
          description: Role for ES proxy service account for .vpc_name
          assume_role_policy: 'jsonencode("Version": "2012-10-17", "Statement":
            ["Effect": "Allow", "Principal": "Service": "ec2.amazonaws.com", "Action":
            "sts:AssumeRole", "Sid": "", "Effect": "Allow", "Principal": "Federated":
            "arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/.oidc_provider_arn",
            "Action": "sts:AssumeRoleWithWebIdentity", "Condition": "StringEquals":
            ".oidc_provider_arn:sub": ["system:serviceaccount:.namespace:hatchery-sa"],
            ".oidc_provider_arn:aud": "sts.amazonaws.com"])'
          path: /gen3-service/
      - manifestservice-role:
          count: '.manifestservice_enabled ? 1 : 0'
          name: .vpc_name-.namespace-manifestservice-sa
          description: Role for manifestservice service account for .vpc_name
          assume_role_policy: 'jsonencode("Version": "2012-10-17", "Statement":
            ["Effect": "Allow", "Principal": "Service": "ec2.amazonaws.com", "Action":
            "sts:AssumeRole", "Sid": "", "Effect": "Allow", "Principal": "Federated":
            "arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/.oidc_provider_arn",
            "Action": "sts:AssumeRoleWithWebIdentity", "Condition": "StringEquals":
            ".oidc_provider_arn:sub": ["system:serviceaccount:.namespace:manifestservice-sa"],
            ".oidc_provider_arn:aud": "sts.amazonaws.com"])'
          path: /gen3-service/
      - aws-load-balancer-controller-role:
          count: '.namespace == "default" ? 1 : 0'
          name: .vpc_name-aws-load-balancer-controller-sa
          description: Role for ALB controller service account for .vpc_name
          assume_role_policy: 'jsonencode("Version": "2012-10-17", "Statement":
            ["Effect": "Allow", "Principal": "Service": "ec2.amazonaws.com", "Action":
            "sts:AssumeRole", "Sid": "", "Effect": "Allow", "Principal": "Federated":
            "arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/.oidc_provider_arn",
            "Action": "sts:AssumeRoleWithWebIdentity", "Condition": "StringEquals":
            ".oidc_provider_arn:sub": ["system:serviceaccount:kube-system:aws-load-balancer-controller"],
            ".oidc_provider_arn:aud": "sts.amazonaws.com"])'
          path: /gen3-service/
      - external-secrets-role:
          count: '.namespace == "default" || var.deploy_external_secrets ? 1
            : 0'
          name: .vpc_name-.namespace-external-secrets-sa
          description: Role for external-secrets service account for .vpc_name
          assume_role_policy: 'jsonencode("Version": "2012-10-17", "Statement":
            ["Effect": "Allow", "Principal": "Service": "ec2.amazonaws.com", "Action":
            "sts:AssumeRole", "Sid": "", "Effect": "Allow", "Principal": "Federated":
            "arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/.oidc_provider_arn",
            "Action": "sts:AssumeRoleWithWebIdentity", "Condition": "StringEquals":
            ".oidc_provider_arn:sub": ["system:serviceaccount:.namespace:external-secrets"],
            ".oidc_provider_arn:aud": "sts.amazonaws.com"])'
          path: /gen3-service/
      - s3-mountpoint-role:
          count: '.namespace == "default" || var.deploy_s3_mountpoint ? 1 : 0'
          name: .vpc_name-.namespace-s3-mountpoint-sa
          description: Role for s3 mountpoint service account for .vpc_name
          assume_role_policy: 'jsonencode("Version": "2012-10-17", "Statement":
            ["Effect": "Allow", "Principal": "Service": "ec2.amazonaws.com", "Action":
            "sts:AssumeRole", "Sid": "", "Effect": "Allow", "Principal": "Federated":
            "arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/.oidc_provider_arn",
            "Action": "sts:AssumeRoleWithWebIdentity", "Condition": "StringEquals":
            ".oidc_provider_arn:sub": ["system:serviceaccount:kube-system:s3-csi-driver-sa"],
            ".oidc_provider_arn:aud": "sts.amazonaws.com"])'
          path: /gen3-service/
      - grafana-role:
          count: '.namespace == "default" && var.deploy_grafana ? 1 : 0'
          name: .vpc_name-observability-role
          description: Role for grafana service account for .vpc_name
          assume_role_policy: 'jsonencode("Version": "2012-10-17", "Statement":
            ["Effect": "Allow", "Principal": "Service": "ec2.amazonaws.com", "Action":
            "sts:AssumeRole", "Sid": "", "Effect": "Allow", "Principal": "Federated":
            "arn:aws:iam::data.aws_caller_identity.current.account_id:oidc-provider/.oidc_provider_arn",
            "Action": "sts:AssumeRoleWithWebIdentity", "Condition": "StringEquals":
            ".oidc_provider_arn:sub": ["system:serviceaccount:monitoring:observability"],
            ".oidc_provider_arn:aud": "sts.amazonaws.com"])'
          path: /gen3-service/
      aws_iam_role_policy:
      - audit-role-policy:
          name: audit-role-policy
          role: aws_iam_role.audit-role[0].id
          policy: 'jsonencode("Version": "2012-10-17", "Statement": ["Action":
            ["sqs:ReceiveMessage", "sqs:GetQueueAttributes", "sqs:DeleteMessage"],
            "Effect": "Allow", "Resource": ["module.audit-sqs.sqs-arn"]])'
      - fence-role-policy:
          name: fence-role-policy
          role: aws_iam_role.fence-role[0].id
          policy: 'jsonencode("Version": "2012-10-17", "Statement": ["Action":
            ["sqs:SendMessage"], "Effect": "Allow", "Resource": ["module.audit-sqs.sqs-arn"]])'
      - gitops-role-policy:
          name: gitops-role-policy
          role: aws_iam_role.gitops-role.id
          policy: 'jsonencode("Version": "2012-10-17", "Statement": ["Action":
            ["s3:List*", "s3:Get*"], "Effect": "Allow", "Resource": ["arn:aws:s3:::dashboard-data.aws_caller_identity.current.account_id-.vpc_name-gen3/*",
            "arn:aws:s3:::dashboard-data.aws_caller_identity.current.account_id-.vpc_name-gen3"],
            "Action": ["s3:PutObject", "s3:GetObject", "s3:DeleteObject"], "Effect":
            "Allow", "Resource": "arn:aws:s3:::dashboard-data.aws_caller_identity.current.account_id-.vpc_name-gen3/*"])'
      - hatchery-role-policy:
          name: hatchery-role-policy
          role: aws_iam_role.hatchery-role[0].id
          policy: 'jsonencode("Version": "2012-10-17", "Statement": ["Action":
            ["sts:AssumeRole"], "Effect": "Allow", "Resource": ["arn:aws:iam::*:role/csoc_adminvm*"],
            "Action": ["ec2:*"], "Effect": "Allow", "Resource": "*"])'
      - manifestservice-role-policy:
          name: manifestservice-role-policy
          role: aws_iam_role.manifestservice-role[0].id
          policy: 'jsonencode("Version": "2012-10-17", "Statement": ["Action":
            ["s3:List*", "s3:Get*"], "Effect": "Allow", "Resource": ["arn:aws:s3:::manifestservice-.vpc_name-.namespace/*",
            "arn:aws:s3:::manifestservice-.vpc_name-.namespace"], "Action":
            ["s3:PutObject", "s3:GetObject", "s3:DeleteObject"], "Effect": "Allow",
            "Resource": "arn:aws:s3:::manifestservice-.vpc_name-.namespace/*"])'
      - aws-load-balancer-role-policy:
          count: '.namespace == "default" ? 1 : 0'
          name: aws-load-balancer-controller-role-policy
          role: aws_iam_role.aws-load-balancer-controller-role[0].id
          policy: 'jsonencode("Version": "2012-10-17", "Statement": ["Action":
            "iam:createServiceLinkedRole", "Effect": "Allow", "Resource": "*", "Condition":
            "StringEquals": "iam:AWSServiceName": "elasticloadbalancing.amazonaws.com",
            "Action": ["ec2:DescribeAccountAttributes", "ec2:DescribeAddresses",
            "ec2:DescribeAvailabilityZones", "ec2:DescribeInternetGateways", "ec2:DescribeVpcs",
            "ec2:DescribeVpcPeeringConnections", "ec2:DescribeSubnets", "ec2:DescribeSecurityGroups",
            "ec2:DescribeInstances", "ec2:DescribeNetworkInterfaces", "ec2:DescribeTags",
            "ec2:GetCoipPoolUsage", "ec2:GetSecurityGroupsForVpc", "ec2:DescribeCoipPools",
            "elasticloadbalancing:DescribeLoadBalancers", "elasticloadbalancing:DescribeLoadBalancerAttributes",
            "elasticloadbalancing:DescribeListeners", "elasticloadbalancing:DescribeListenerAttributes",
            "elasticloadbalancing:DescribeListenerCertificates", "elasticloadbalancing:DescribeSSLPolicies",
            "elasticloadbalancing:DescribeRules", "elasticloadbalancing:DescribeTargetGroups",
            "elasticloadbalancing:DescribeTargetGroupAttributes", "elasticloadbalancing:DescribeTargetHealth",
            "elasticloadbalancing:DescribeTags", "elasticloadbalancing:AddTags"],
            "Effect": "Allow", "Resource": "*", "Action": ["cognito-idp:DescribeUserPoolClient",
            "acm:ListCertificates", "acm:DescribeCertificate", "iam:ListServerCertificates",
            "iam:GetServerCertificate", "waf-regional:GetWebACL", "waf-regional:GetWebACLForResource",
            "waf-regional:AssociateWebACL", "waf-regional:DisassociateWebACL", "wafv2:GetWebACL",
            "wafv2:GetWebACLForResource", "wafv2:AssociateWebACL", "wafv2:DisassociateWebACL",
            "shield:GetSubscriptionState", "shield:DescribeProtection", "shield:CreateProtection",
            "shield:DeleteProtection"], "Effect": "Allow", "Resource": "*", "Action":
            ["ec2:AuthorizeSecurityGroupIngress", "ec2:RevokeSecurityGroupIngress"],
            "Effect": "Allow", "Resource": "*", "Action": ["ec2:CreateSecurityGroup"],
            "Effect": "Allow", "Resource": "*", "Action": ["ec2:CreateTags"], "Effect":
            "Allow", "Resource": "arn:aws:ec2:*:*:security-group/*", "Condition":
            "StringEquals": "ec2:CreateAction": "CreateSecurityGroup", "Null":
            "aws:RequestTag/elbv2.k8s.aws/cluster": "false", "Action": ["ec2:CreateTags",
            "ec2:DeleteTags"], "Effect": "Allow", "Resource": "arn:aws:ec2:*:*:security-group/*",
            "Condition": "Null": "aws:RequestTag/elbv2.k8s.aws/cluster": "true",
            "aws:ResourceTag/elbv2.k8s.aws/cluster": "false", "Action": ["ec2:AuthorizeSecurityGroupIngress",
            "ec2:RevokeSecurityGroupIngress", "ec2:DeleteSecurityGroup"], "Effect":
            "Allow", "Resource": "*", "Condition": "Null": "aws:ResourceTag/elbv2.k8s.aws/cluster":
            "false", "Action": ["elasticloadbalancing:CreateLoadBalancer", "elasticloadbalancing:CreateTargetGroup"],
            "Effect": "Allow", "Resource": "*", "Condition": "Null": "aws:RequestTag/elbv2.k8s.aws/cluster":
            "false", "Action": ["elasticloadbalancing:CreateListener", "elasticloadbalancing:DeleteListener",
            "elasticloadbalancing:CreateRule", "elasticloadbalancing:DeleteRule"],
            "Effect": "Allow", "Resource": "*", "Action": ["elasticloadbalancing:AddTags",
            "elasticloadbalancing:RemoveTags"], "Effect": "Allow", "Resource": ["arn:aws:elasticloadbalancing:*:*:targetgroup/*/*",
            "arn:aws:elasticloadbalancing:*:*:loadbalancer/net/*/*", "arn:aws:elasticloadbalancing:*:*:loadbalancer/app/*/*"],
            "Condition": "Null": "aws:RequestTag/elbv2.k8s.aws/cluster": "true",
            "aws:ResourceTag/elbv2.k8s.aws/cluster": "false", "Action": ["elasticloadbalancing:AddTags",
            "elasticloadbalancing:RemoveTags"], "Effect": "Allow", "Resource": ["arn:aws:elasticloadbalancing:*:*:listener/net/*/*/*",
            "arn:aws:elasticloadbalancing:*:*:listener/app/*/*/*", "arn:aws:elasticloadbalancing:*:*:listener-rule/net/*/*/*",
            "arn:aws:elasticloadbalancing:*:*:listener-rule/app/*/*/*"], "Action":
            ["elasticloadbalancing:ModifyLoadBalancerAttributes", "elasticloadbalancing:SetIpAddressType",
            "elasticloadbalancing:SetSecurityGroups", "elasticloadbalancing:SetSubnets",
            "elasticloadbalancing:DeleteLoadBalancer", "elasticloadbalancing:ModifyTargetGroup",
            "elasticloadbalancing:ModifyTargetGroupAttributes", "elasticloadbalancing:DeleteTargetGroup"],
            "Effect": "Allow", "Resource": "*", "Condition": "Null": "aws:ResourceTag/elbv2.k8s.aws/cluster":
            "false", "Action": ["elasticloadbalancing:RegisterTargets", "elasticloadbalancing:DeregisterTargets"],
            "Effect": "Allow", "Resource": "arn:aws:elasticloadbalancing:*:*:targetgroup/*/*",
            "Action": ["elasticloadbalancing:SetWebAcl", "elasticloadbalancing:ModifyListener",
            "elasticloadbalancing:AddListenerCertificates", "elasticloadbalancing:RemoveListenerCertificates",
            "elasticloadbalancing:ModifyRule"], "Effect": "Allow", "Resource": "*"])'
      - external-secrets-role-policy:
          count: '.namespace == "default" || var.deploy_external_secrets ? 1
            : 0'
          name: external-secrets-role-policy
          role: aws_iam_role.external-secrets-role[0].id
          policy: 'jsonencode("Version": "2012-10-17", "Statement": ["Action":
            ["secretsmanager:ListSecrets", "secretsmanager:GetSecretValue"], "Effect":
            "Allow", "Resource": "*"])'
      - s3-mountpoint-role-policy:
          count: '.namespace == "default" || var.deploy_s3_mountpoint ? 1 : 0'
          name: s3-mountpoint-role-policy
          role: aws_iam_role.s3-mountpoint-role[0].id
          policy: 'jsonencode("Version": "2012-10-17", "Statement": ["Action":
            ["s3:ListBucket"], "Effect": "Allow", "Resource": "*", "Action": ["s3:GetObject",
            "s3:PutObject", "s3:AbortMultipartUpload", "s3:DeleteObject"], "Effect":
            "Allow", "Resource": "*"])'
      - grafana-role-policy:
          count: '.namespace == "default" && var.deploy_grafana ? 1 : 0'
          name: grafana-role-policy
          role: aws_iam_role.grafana-role[0].id
          policy: 'jsonencode("Version": "2012-10-17", "Statement": ["Action":
            ["s3:AbortMultipartUpload", "s3:DeleteObject", "s3:GetObject", "s3:ListBucket",
            "s3:PutObject", "s3:DeleteObjectVersion", "s3:GetObjectVersion", "s3:PutObjectAcl",
            "s3:GetObjectAcl", "s3:ListBucketMultipartUploads", "s3:ListBucketVersions"],
            "Effect": "Allow", "Resource": ["arn:aws:s3:::.vpc_name-observability-bucket",
            "arn:aws:s3:::.vpc_name-observability-bucket/*"]])'
      aws_iam_role_policy_attachment:
        hatchery-role-policy-attachment:
          count: '.hatchery_enabled ? 1 : 0'
          role: aws_iam_role.hatchery-role[0].name
          policy_arn: arn:aws:iam::aws:policy/AWSResourceAccessManagerFullAccess
    variable:
      ambassador_enabled:
        description: Enable ambassador
        type: bool
        default: true
      arborist_enabled:
        description: Enable arborist
        type: bool
        default: true
      argo_enabled:
        description: Enable argo
        type: bool
        default: true
      audit_enabled:
        description: Enable audit
        type: bool
        default: true
      aurora_username:
        description: aurora username
        default: ''
      aurora_hostname:
        description: aurora hostname
        default: ''
      aurora_password:
        description: aurora password
        default: ''
      aws-es-proxy_enabled:
        description: Enable aws-es-proxy
        type: bool
        default: true
      dbgap_enabled:
        description: Enable dbgap sync in the usersync job
        type: bool
        default: false
      dd_enabled:
        description: Enable datadog
        type: bool
        default: false
      deploy_external_secrets:
        description: Deploy external secrets
        type: bool
        default: false
      deploy_grafana:
        description: Deploy grafana
        type: bool
        default: false
      deploy_s3_mountpoint:
        description: Deploy s3 mountpoints
        type: bool
        default: false
      dictionary_url:
        description: URL to the data dictionary
        default: ''
      dispatcher_job_number:
        description: Number of dispatcher jobs
        default: 10
      dicom-server_enabled:
        description: Enable dicom
        type: bool
        default: false
      dicom-viewer_enabled:
        description: Enable dicom server
        type: bool
        default: false
      es_endpoint:
        description: Elasticsearch endpoint
        default: ''
      es_user_key:
        description: Elasticsearch user access key
        default: ''
      es_user_secret:
        description: Elasticsearch user secret key
        default: ''
      fence_enabled:
        description: Enable fence
        type: bool
        default: true
      gen3ff_enabled:
        description: Enable gen3ff
        type: bool
        default: false
      gen3ff_repo:
        description: Gen3ff repo
        default: quay.io/cdis/frontend-framework
      gen3ff_tag:
        description: Gen3ff tag
        default: main
      guppy_enabled:
        description: Enable guppy
        type: bool
        default: true
      hatchery_enabled:
        description: Enable hatchery
        type: bool
        default: true
      hostname:
        description: hostname of the commons
        default: ''
      indexd_enabled:
        description: Enable indexd
        type: bool
        default: true
      indexd_prefix:
        description: Indexd prefix
        default: dg.XXXX/
      ingress_enabled:
        description: Create ALB ingress
        type: bool
        default: true
      manifestservice_enabled:
        description: Enable manfiestservice
        type: bool
        default: true
      metadata_enabled:
        description: Enable metadata
        type: bool
        default: true
      netpolicy_enabled:
        description: Enable network policy security rules
        type: bool
        default: false
      peregrine_enabled:
        description: Enable perergrine
        type: bool
        default: true
      pidgin_enabled:
        description: Enable pidgin
        type: bool
        default: false
      portal_enabled:
        description: Enable portal
        type: bool
        default: true
      public_datasets:
        description: whether the datasets are public
        type: bool
        default: false
      requestor_enabled:
        description: Enable requestor
        type: bool
        default: false
      revproxy_arn:
        description: ARN for the revproxy cert in ACM
        default: ''
      revproxy_enabled:
        description: Enable revproxy
        type: bool
        default: true
      sheepdog_enabled:
        description: Enable sheepdog
        type: bool
        default: true
      slack_send_dbgap:
        description: Enable slack message for usersync job
        type: bool
        default: false
      slack_webhook:
        description: Slack webhook
        default: ''
      ssjdispatcher_enabled:
        description: Enable ssjdispatcher
        type: bool
        default: true
      sower_enabled:
        description: Enable sower
        type: bool
        default: true
      tier_access_level:
        description: Tier access level for guppy
        default: private
      tier_access_limit:
        description: value for tier access limit
        default: '100'
      usersync_enabled:
        description: Enable usersync cronjob
        type: bool
        default: true
      usersync_schedule:
        description: Cronjob schedule for usersync
        default: '*/30 * * * *'
      useryaml_s3_path:
        description: S3 path to the user.yaml file
        default: s3://cdis-gen3-users/dev/user.yaml
      vpc_name:
        description: Name of the VPC
        default: ''
      wts_enabled:
        description: Enable wts
        type: bool
        default: true
      cluster_endpoint:
        default: ''
      cluster_ca_cert:
        default: ''
      cluster_name:
        default: ''
      oidc_provider_arn:
        default: ''
      useryaml_path:
        default: ''
      gitops_path:
        default: ''
      fence_config_path:
        default: ''
      google_client_id:
        default: ''
      google_client_secret:
        default: ''
      fence_access_key:
        default: ''
      fence_secret_key:
        default: ''
      upload_bucket:
        default: ''
      waf_arn:
        default: ''
      namespace:
        default: default
      deploy_gen3:
        default: false
data:
  aws_availability_zones:
    available: 
  aws_caller_identity:
    current: 
  aws_iam_policy_document:
    configbucket_reader:
      statement:
        actions: s3:Get* s3:List*
        effect: Allow
        resources: arn:aws:s3:::.users_bucket_name arn:aws:s3:::.users_bucket_name/.config_folder/*
          arn:aws:s3:::qualys-agentpackage arn:aws:s3:::qualys-agentpackage/*
locals:
  db_fence_address: '.deploy_aurora ? module.aurora[0].aurora_cluster_writer_endpoint
    : var.deploy_fence_db && var.deploy_rds ? aws_db_instance.db_fence[0].address
    : ""'
  db_indexd_address: '.deploy_aurora ? module.aurora[0].aurora_cluster_writer_endpoint
    : var.deploy_indexd_db && var.deploy_rds ? aws_db_instance.db_indexd[0].address
    : ""'
  db_sheepdog_address: '.deploy_aurora ? module.aurora[0].aurora_cluster_writer_endpoint
    : var.deploy_sheepdog_db && var.deploy_rds ? aws_db_instance.db_sheepdog[0].address
    : ""'
  db_peregrine_address: '.deploy_aurora ? module.aurora[0].aurora_cluster_writer_endpoint
    : var.deploy_sheepdog_db && var.deploy_rds ? aws_db_instance.db_sheepdog[0].address
    : ""'
  pg_family_version: replace(var.engine_version, "/\\.[0-9]/", "")
  cluster_name: replace(var.vpc_name, "-", "")
output:
  aws_region:
    value: .aws_region
  vpc_name:
    value: .vpc_name
  vpc_cidr_block:
    value: module.cdis_vpc.vpc_cidr_block
  indexd_rds_id:
    value: aws_db_instance.db_indexd.*.identifier
  fence_rds_id:
    value: aws_db_instance.db_fence.*.identifier
  sheepdog_rds_id:
    value: aws_db_instance.db_sheepdog.*.identifier
  fence-bot_user_secret:
    value: module.cdis_vpc.fence-bot_secret
    sensitive: true
  fence-bot_user_id:
    value: module.cdis_vpc.fence-bot_id
  data-bucket_name:
    value: module.cdis_vpc.data-bucket_name
  kubeconfig:
    value: module.eks[0].kubeconfig
    sensitive: true
  config_map_aws_auth:
    value: module.eks[0].config_map_aws_auth
    sensitive: true
  eks_cluster_name:
    value: module.eks[0].cluster_name
  eks_cluster_endpoint:
    value: module.eks[0].cluster_endpoint
    sensitive: true
  eks_cluster_ca_cert:
    value: module.eks[0].cluster_certificate_authority_data
    sensitive: true
  eks_oidc_arn:
    value: module.eks[0].oidc_provider_arn
  cluster_oidc_provider_url:
    value: module.eks[0].cluster_oidc_provider_url
  cluster_oidc_provider_arn:
    value: module.eks[0].cluster_oidc_provider_arn
  opensearch_cluster_arn:
    value: '.deploy_es ? module.commons_vpc_es[0].es_arn : null'
  aurora_cluster_writer_endpoint:
    description: Aurora cluster writer instance endpoint
    value: one(module.aurora[*].aurora_cluster_writer_endpoint)
  aurora_cluster_reader_endpoint:
    description: Aurora cluster reader endpoint
    value: one(module.aurora[*].aurora_cluster_reader_endpoint)
  aurora_cluster_master_username:
    description: Aurora cluster master username
    value: one(module.aurora[*].aurora_cluster_master_username)
  aurora_cluster_master_password:
    description: Aurora cluster master user's password
    value: one(module.aurora[*].aurora_cluster_master_password)
    sensitive: true
  es_endpoint:
    value: '.deploy_es ? module.commons_vpc_es[0].es_endpoint : null'
  waf_arn:
    description: WAF arn - annotate the cluster ingress
    value: '.deploy_waf ? module.aws_waf[0].waf_arn : null'
provider:
  kubernetes:
    host: module.eks[0].cluster_endpoint
    cluster_ca_certificate: base64decode(module.eks[0].cluster_certificate_authority_data)
    exec:
      api_version: client.authentication.k8s.io/v1beta1
      command: aws
      args: eks get-token --cluster-name module.eks[0].cluster_name --role-arn
        arn:aws:iam::data.aws_caller_identity.current.account_id:role/.iam_role_name
  helm:
    kubernetes:
      host: module.eks[0].cluster_endpoint
      cluster_ca_certificate: base64decode(module.eks[0].cluster_certificate_authority_data)
      exec:
        api_version: client.authentication.k8s.io/v1beta1
        command: aws
        args: eks get-token --cluster-name module.eks[0].cluster_name --role-arn
          arn:aws:iam::data.aws_caller_identity.current.account_id:role/.iam_role_name
  kubectl:
    apply_retry_count: 5
    host: module.eks[0].cluster_endpoint
    cluster_ca_certificate: base64decode(module.eks[0].cluster_certificate_authority_data)
    load_config_file: false
    exec:
      api_version: client.authentication.k8s.io/v1beta1
      command: aws
      args: eks get-token --cluster-name module.eks[0].cluster_name --role-arn
        arn:aws:iam::data.aws_caller_identity.current.account_id:role/.iam_role_name
  aws:
  - 
  - alias: csoc
resource:
  aws_route_table:
    private_kube:
      vpc_id: module.cdis_vpc.vpc_id
      tags:
        Name: private_kube
        Environment: .vpc_name
        Organization: .organization_name
  aws_route:
    for_peering:
      count: '.csoc_managed ? 1 : 0'
      route_table_id: aws_route_table.private_kube.id
      destination_cidr_block: .peering_cidr
      vpc_peering_connection_id: module.cdis_vpc.vpc_peering_id
      depends_on: aws_route_table.private_kube
  aws_route_table_association:
    private_kube:
      subnet_id: aws_subnet.private_kube.id
      route_table_id: aws_route_table.private_kube.id
  aws_subnet:
  - private_kube:
      vpc_id: module.cdis_vpc.vpc_id
      cidr_block: '.network_expansion ? cidrsubnet(var.vpc_cidr_block, 5, 0)
        : cidrsubnet(var.vpc_cidr_block, 4, 2)'
      map_public_ip_on_launch: false
      availability_zone: data.aws_availability_zones.available.names[0]
      tags: 'tomap("Name": "int_services", "Organization": ".organization_name",
        "Environment": ".vpc_name")'
      lifecycle:
        ignore_changes: tags availability_zone
  - private_db_alt:
      vpc_id: module.cdis_vpc.vpc_id
      cidr_block: '.network_expansion ? cidrsubnet(var.vpc_cidr_block, 5, 1)
        : cidrsubnet(var.vpc_cidr_block, 4, 3)'
      availability_zone: data.aws_availability_zones.available.names[1]
      map_public_ip_on_launch: false
      tags:
        Name: private_db_alt
        Environment: .vpc_name
        Organization: .organization_name
      lifecycle:
        ignore_changes: tags availability_zone
  aws_db_subnet_group:
    private_group:
      name: .vpc_name_private_group
      subnet_ids: aws_subnet.private_kube.id aws_subnet.private_db_alt.id
      description: Private subnet group
      tags:
        Name: Private subnet group
        Environment: .vpc_name
        Organization: .organization_name
  aws_iam_role:
    esproxy-role:
      count: '.deploy_es_role ? 1 : 0'
      name: .vpc_name-esproxy-sa
      description: Role for ES proxy service account for .vpc_name
      assume_role_policy: "\n    \"Version\": \"2012-10-17\",\n    \"Statement\"\
        : [\n        \n            \"Effect\": \"Allow\",\n            \"Principal\"\
        : \n                \"Service\": \"ec2.amazonaws.com\"\n            ,\n\
        \            \"Action\": \"sts:AssumeRole\"\n        ,\n        \n     \
        \       \"Sid\": \"\",\n            \"Effect\": \"Allow\",\n            \"\
        Principal\": \n                \"Federated\": \"module.eks[0].cluster_oidc_provider_arn\"\
        \n            ,\n            \"Action\": \"sts:AssumeRoleWithWebIdentity\"\
        ,\n            \"Condition\": \n                \"ForAllValues:StringLike\"\
        : \n                    \"module.eks[0].oidc_provider_arn:sub\": [\n \
        \                       \"system:serviceaccount:*:esproxy-sa\"\n         \
        \           ],\n                    \"module.eks[0].oidc_provider_arn:aud\"\
        : \"sts.amazonaws.com\"\n                \n            \n        \n   \
        \ ]\n"
      path: /gen3-service/
  helm_release:
  - argocd:
      count: '.k8s_bootstrap_resources && var.deploy_argocd ? 1 : 0'
      name: argocd
      chart: argo-cd
      repository: https://argoproj.github.io/argo-helm
      version: .argocd_version
      namespace: argocd
      create_namespace: true
      values: 'server.basehref: "/argocd/"'
  - external-secrets:
      count: '.k8s_bootstrap_resources && var.deploy_external_secrets_operator
        ? 1 : 0'
      name: external-secrets
      chart: external-secrets
      repository: https://charts.external-secrets.io
      version: .external_secrets_operator_version
      namespace: external-secrets
      create_namespace: true
      values: "serviceAccount:\n  create: true\n  name: external-secrets\nsyncPolicy:\n\
        \  automated:\n    prune: true\n    selfHeal: true"
  aws_db_instance:
  - db_fence:
      count: '.deploy_fence_db && var.deploy_rds ? 1 : 0'
      allocated_storage: .fence_db_size
      identifier: .vpc_name-fencedb
      storage_type: gp2
      engine: postgres
      engine_version: .engine_version
      parameter_group_name: aws_db_parameter_group.rds-cdis-pg.name
      instance_class: .fence_db_instance
      db_name: .fence_database_name
      username: .fence_db_username
      password: '.db_password_fence != "" ? var.db_password_fence : random_password.fence_password.result'
      snapshot_identifier: .fence_snapshot
      db_subnet_group_name: aws_db_subnet_group.private_group.id
      vpc_security_group_ids: module.cdis_vpc.security_group_local_id
      allow_major_version_upgrade: .fence_allow_major_version_upgrade
      final_snapshot_identifier: replace(var.vpc_name, "_", "-")-fencedb
      maintenance_window: .fence_maintenance_window
      backup_retention_period: .fence_backup_retention_period
      backup_window: .fence_backup_window
      multi_az: .fence_ha
      auto_minor_version_upgrade: .fence_auto_minor_version_upgrade
      storage_encrypted: .rds_instance_storage_encrypted
      max_allocated_storage: .fence_max_allocated_storage
      tags:
        Environment: .vpc_name
        Organization: .organization_name
      lifecycle:
        prevent_destroy: true
        ignore_changes: all
  - db_sheepdog:
      count: '.deploy_sheepdog_db && var.deploy_rds ? 1 : 0'
      allocated_storage: .sheepdog_db_size
      identifier: .vpc_name-sheepdog
      storage_type: gp2
      engine: postgres
      engine_version: .engine_version
      parameter_group_name: aws_db_parameter_group.rds-cdis-pg.name
      instance_class: .sheepdog_db_instance
      db_name: .sheepdog_database_name
      username: .sheepdog_db_username
      password: '.db_password_sheepdog != "" ? var.db_password_sheepdog : random_password.sheepdog_password.result'
      snapshot_identifier: .sheepdog_snapshot
      db_subnet_group_name: aws_db_subnet_group.private_group.id
      vpc_security_group_ids: module.cdis_vpc.security_group_local_id
      allow_major_version_upgrade: .sheepdog_allow_major_version_upgrade
      final_snapshot_identifier: replace(var.vpc_name, "_", "-")-sheepdogdb
      maintenance_window: .sheepdog_maintenance_window
      backup_retention_period: .sheepdog_backup_retention_period
      backup_window: .sheepdog_backup_window
      multi_az: .sheepdog_ha
      auto_minor_version_upgrade: .sheepdog_auto_minor_version_upgrade
      storage_encrypted: .rds_instance_storage_encrypted
      max_allocated_storage: .sheepdog_max_allocated_storage
      tags:
        Environment: .vpc_name
        Organization: .organization_name
      lifecycle:
        prevent_destroy: true
        ignore_changes: all
  - db_indexd:
      count: '.deploy_indexd_db && var.deploy_rds ? 1 : 0'
      allocated_storage: .indexd_db_size
      identifier: .vpc_name-indexddb
      storage_type: gp2
      engine: postgres
      engine_version: .engine_version
      parameter_group_name: aws_db_parameter_group.rds-cdis-pg.name
      instance_class: .indexd_db_instance
      db_name: .indexd_database_name
      username: .indexd_db_username
      password: '.db_password_indexd != "" ? var.db_password_indexd : random_password.indexd_password.result'
      snapshot_identifier: .indexd_snapshot
      db_subnet_group_name: aws_db_subnet_group.private_group.id
      vpc_security_group_ids: module.cdis_vpc.security_group_local_id
      allow_major_version_upgrade: .indexd_allow_major_version_upgrade
      final_snapshot_identifier: replace(var.vpc_name, "_", "-")-indexddb
      maintenance_window: .indexd_maintenance_window
      backup_retention_period: .indexd_backup_retention_period
      backup_window: .indexd_backup_window
      multi_az: .indexd_ha
      auto_minor_version_upgrade: .indexd_auto_minor_version_upgrade
      storage_encrypted: .rds_instance_storage_encrypted
      max_allocated_storage: .indexd_max_allocated_storage
      tags:
        Environment: .vpc_name
        Organization: .organization_name
      lifecycle:
        prevent_destroy: true
        ignore_changes: all
  aws_db_parameter_group:
    rds-cdis-pg:
      name: .vpc_name-rds-cdis-pg
      family: postgreslocal.pg_family_version
      parameter:
        name:
        - cpu_index_tuple_cost
        - cpu_tuple_cost
        - log_duration
        - log_min_duration_statement
        - random_page_cost
        - password_encryption
        value:
        - '0.000005'
        - '0.7'
        - '1'
        - '0'
        - '0.7'
        - scram-sha-256
      lifecycle:
        ignore_changes: all
  aws_kms_key:
    kube_key:
      description: encryption/decryption key for kubernete
      enable_key_rotation: true
      tags:
        Environment: .vpc_name
        Organization: .organization_name
  aws_kms_alias:
    kube_key:
      name: alias/.vpc_name-k8s
      target_key_id: aws_kms_key.kube_key.key_id
  aws_key_pair:
    automation_dev:
      key_name: .vpc_name_automation_dev
      public_key: .kube_ssh_key
  aws_iam_policy:
    configbucket_reader:
      name: bucket_reader_cdis-gen3-users_.vpc_name
      description: Read cdis-gen3-users/.config_folder
      policy: data.aws_iam_policy_document.configbucket_reader.json
      lifecycle:
        ignore_changes: policy
  random_password:
  - fence_password:
      length: .password_length
      special: false
  - sheepdog_password:
      length: .password_length
      special: false
  - peregrine_password:
      length: .password_length
      special: false
  - indexd_password:
      length: .password_length
      special: false
  - hmac_encryption_key:
      length: 32
      special: false
  - sheepdog_secret_key:
      length: 50
      special: false
  - sheepdog_indexd_password:
      length: 32
      special: false
terraform:
  required_providers:
    kubectl:
      source: gavinbunney/kubectl
variable:
  vpc_name:
    default: Commons1
  iam_role_name:
    default: csoc_adminvm
  vpc_cidr_block:
    default: 172.24.17.0/20
  vpc_flow_logs:
    default: false
  vpc_flow_traffic:
    default: ALL
  aws_region:
    default: us-east-1
  aws_cert_name:
    default: AWS-CERTIFICATE-NAME
  csoc_account_id:
    default: '433568766270'
  peering_cidr:
    default: 10.128.0.0/20
  fence_db_size:
    default: 10
  sheepdog_db_size:
    default: 10
  indexd_db_size:
    default: 10
  db_password_fence:
    default: ''
    sensitive: true
  db_password_peregrine:
    default: ''
    sensitive: true
  db_password_sheepdog:
    default: ''
    sensitive: true
  db_password_indexd:
    default: ''
    sensitive: true
  portal_app:
    default: dev
  fence_snapshot:
    default: ''
  peregrine_snapshot:
    default: ''
  sheepdog_snapshot:
    default: ''
  indexd_snapshot:
    default: ''
  fence_db_instance:
    default: db.t3.small
  sheepdog_db_instance:
    default: db.t3.small
  indexd_db_instance:
    default: db.t3.small
  hostname:
    default: dev.bionimbus.org
  kube_ssh_key:
    default: ''
  kube_additional_keys:
    default: ''
  hmac_encryption_key:
    default: ''
    sensitive: true
  sheepdog_secret_key:
    default: ''
    sensitive: true
  sheepdog_indexd_password:
    default: ''
    sensitive: true
  sheepdog_oauth2_client_id:
    default: ''
    sensitive: true
  config_folder:
    default: dev
  sheepdog_oauth2_client_secret:
    default: ''
    sensitive: true
  ami_account_id:
    default: '137112412989'
  squid_image_search_criteria:
    description: Search criteria for squid AMI look up
    default: al2023-ami-*
  peering_vpc_id:
    default: vpc-e2b51d99
  squid-nlb-endpointservice-name:
    default: com.amazonaws.vpce.us-east-1.vpce-svc-0ce2261f708539011
  slack_webhook:
    default: ''
    sensitive: true
  secondary_slack_webhook:
    default: ''
    sensitive: true
  alarm_threshold:
    default: '85'
  csoc_managed:
    default: false
  csoc_peering:
    default: false
  mailgun_api_key:
    default: ''
  mailgun_smtp_host:
    default: smtp.mailgun.org
  mailgun_api_url:
    default: https://api.mailgun.net/v3/
  fence_ha:
    default: false
  sheepdog_ha:
    default: false
  indexd_ha:
    default: false
  fence_maintenance_window:
    default: SAT:09:00-SAT:09:59
  sheepdog_maintenance_window:
    default: SAT:10:00-SAT:10:59
  indexd_maintenance_window:
    default: SAT:11:00-SAT:11:59
  fence_backup_retention_period:
    default: '4'
  sheepdog_backup_retention_period:
    default: '4'
  indexd_backup_retention_period:
    default: '4'
  fence_backup_window:
    default: 06:00-06:59
  sheepdog_backup_window:
    default: 07:00-07:59
  indexd_backup_window:
    default: 08:00-08:59
  engine_version:
    default: '13'
  fence_auto_minor_version_upgrade:
    default: 'true'
  indexd_auto_minor_version_upgrade:
    default: 'true'
  sheepdog_auto_minor_version_upgrade:
    default: 'true'
  users_bucket_name:
    default: cdis-gen3-users
  fence_database_name:
    default: fence
  sheepdog_database_name:
    default: gdcapi
  indexd_database_name:
    default: indexd
  fence_db_username:
    default: fence_user
  sheepdog_db_username:
    default: sheepdog
  indexd_db_username:
    default: indexd_user
  fence_allow_major_version_upgrade:
    default: 'true'
  sheepdog_allow_major_version_upgrade:
    default: 'true'
  indexd_allow_major_version_upgrade:
    default: 'true'
  ha-squid_instance_type:
    description: Instance type for HA squid
    default: t3.medium
  ha-squid_instance_drive_size:
    description: Volume size for HA squid instances
    default: 25
  deploy_single_proxy:
    description: Single instance plus HA
    default: true
  ha-squid_bootstrap_script:
    description: Bootstrapt script for ha-squid instances
    default: squid_running_on_docker.sh
  ha-squid_extra_vars:
    type: list(string)
    description: additional variables to pass along with the bootstrapscript
    default: squid_image=master
  branch:
    description: For testing purposes, when something else than the master
    default: master
  fence-bot_bucket_access_arns:
    description: When fence bot has to access another bucket that wasn't created by
      the VPC module
    default: []
  deploy_ha_squid:
    description: Should you want to deploy HA-squid
    default: false
  ha-squid_cluster_desired_capasity:
    description: If ha squid is enabled and you want to set your own capasity
    default: 2
  ha-squid_cluster_min_size:
    description: If ha squid is enabled and you want to set your own min size
    default: 1
  ha-squid_cluster_max_size:
    description: If ha squid is enabled and you want to set your own max size
    default: 3
  deploy_sheepdog_db:
    description: Whether or not to deploy the database instance
    default: true
  deploy_fence_db:
    description: Whether or not to deploy the database instance
    default: true
  deploy_indexd_db:
    description: Whether or not to deploy the database instance
    default: true
  single_squid_instance_type:
    description: Instance type for the single proxy instance
    default: t2.micro
  network_expansion:
    description: Let k8s workers be on a /22 subnet per AZ
    default: false
  rds_instance_storage_encrypted:
    default: true
  fence_max_allocated_storage:
    description: Maximum allocated storage for autosacaling
    default: 0
  sheepdog_max_allocated_storage:
    description: Maximum allocated storage for autosacaling
    default: 0
  indexd_max_allocated_storage:
    description: Maximum allocated storage for autosacaling
    default: 0
  activation_id:
    default: ''
    sensitive: true
  customer_id:
    default: ''
    sensitive: true
  fips:
    default: false
  ignore_fence_changes:
    default: engine_version storage_encrypted identifier
  ignore_sheepdog_changes:
    default: engine_version storage_encrypted identifier
  ignore_indexd_changes:
    default: engine_version storage_encrypted identifier
  prevent_fence_destroy:
    default: true
  prevent_sheepdog_destroy:
    default: true
  prevent_indexd_destroy:
    default: true
  deploy_alarms:
    default: true
  deploy_argocd:
    default: true
  argocd_version:
    default: 7.8.2
  deploy_external_secrets_operator:
    default: true
  external_secrets_operator_version:
    default: 0.14.0
  ec2_keyname:
    default: someone@uchicago.edu
  instance_type:
    default: t3.large
  jupyter_instance_type:
    default: t3.large
  workflow_instance_type:
    default: t3.2xlarge
  secondary_cidr_block:
    default: ''
  users_policy: 
  worker_drive_size:
    default: 30
  eks_version:
    default: '1.31'
  workers_subnet_size:
    default: 24
  bootstrap_script:
    default: bootstrap-with-security-updates.sh
  jupyter_bootstrap_script:
    default: bootstrap-with-security-updates.sh
  kernel:
    default: N/A
  jupyter_worker_drive_size:
    default: 30
  workflow_bootstrap_script:
    default: bootstrap.sh
  workflow_worker_drive_size:
    default: 30
  cidrs_to_route_to_gw:
    default: []
  organization_name:
    default: Basic Services
  jupyter_asg_desired_capacity:
    default: 0
  jupyter_asg_max_size:
    default: 10
  jupyter_asg_min_size:
    default: 0
  workflow_asg_desired_capacity:
    default: 0
  workflow_asg_max_size:
    default: 50
  workflow_asg_min_size:
    default: 0
  iam-serviceaccount:
    default: true
  domain_test:
    description: url for the lambda function to check for the proxy
    default: www.google.com
  deploy_workflow:
    description: Deploy workflow nodepool?
    default: false
  secondary_availability_zones:
    description: AZ to be used by EKS nodes in the secondary subnet
    default: us-east-1a us-east-1b us-east-1c us-east-1d
  deploy_jupyter:
    description: Deploy workflow nodepool?
    default: true
  dual_proxy:
    description: Single instance and HA
    default: false
  single_az_for_jupyter:
    description: Jupyter notebooks on a single AZ
    default: false
  oidc_eks_thumbprint:
    description: Thumbprint for the AWS OIDC identity provider
    default: 9e99a48a9960b14926bb7f3b02e22da2b0ab7280
  sns_topic_arn:
    description: SNS topic ARN for alerts
    default: arn:aws:sns:us-east-1:433568766270:planx-csoc-alerts-topic
  fips_ami_kms:
    default: arn:aws:kms:us-east-1:707767160287:key/mrk-697897f040ef45b0aa3cebf38a916f99
  fips_enabled_ami:
    default: ami-0de87e3680dcb13ec
  availability_zones:
    description: AZ to be used by EKS nodes
    default: us-east-1a us-east-1c us-east-1d
  deploy_eks:
    default: true
  deploy_es:
    default: true
  es_name:
    default: ''
  ebs_volume_size_gb:
    default: 20
  encryption:
    default: 'true'
  es_instance_type:
    default: m4.large.elasticsearch
  es_instance_count:
    default: 3
  es_version:
    description: What version to use when deploying ES
    default: '6.8'
  es_linked_role:
    description: Whether or no to deploy a linked roll for ES
    default: true
  spot_linked_role:
    default: false
  cluster_identifier:
    description: Cluster Identifier
    type: string
    default: aurora-cluster
  cluster_instance_identifier:
    description: Cluster Instance Identifier
    type: string
    default: aurora-cluster-instance
  cluster_instance_class:
    description: Cluster Instance Class
    type: string
    default: db.serverless
  cluster_engine:
    description: Aurora database engine type
    type: string
    default: aurora-postgresql
  cluster_engine_version:
    description: Aurora database engine version.
    type: string
    default: '13.7'
  master_username:
    description: Master DB username
    type: string
    default: postgres
  storage_encrypted:
    description: Specifies whether storage encryption is enabled
    type: bool
    default: true
  apply_immediate:
    description: Instruct the service to apply the change immediately. This can result
      in a brief downtime as the server reboots. See the AWS Docs on RDS Maintenance
      for more information
    type: bool
    default: true
  engine_mode:
    type: string
    description: use provisioned for Serverless v2 RDS cluster
    default: provisioned
  serverlessv2_scaling_min_capacity:
    type: string
    description: Serverless v2 RDS cluster minimum scaling capacity in ACUs
    default: '0.5'
  serverlessv2_scaling_max_capacity:
    type: string
    description: Serverless v2 RDS cluster maximum scaling capacity in ACUs
    default: '10.0'
  skip_final_snapshot:
    description: Determines whether a final DB snapshot is created before the DB cluster
      is deleted
    type: bool
    default: false
  final_snapshot_identifier:
    type: string
    description: The name of your final DB snapshot when this DB cluster is deleted
    default: aurora-cluster-snapshot-final
  backup_retention_period:
    type: number
    description: The days to retain backups for
    default: 10
  preferred_backup_window:
    description: The daily time range during which automated backups are created if
      automated backups are enabled using the BackupRetentionPeriod parameter
    type: string
    default: 02:00-03:00
  password_length:
    type: number
    description: The length of the password string
    default: 32
  db_kms_key_id:
    default: ''
  deploy_aurora:
    default: false
  deploy_rds:
    default: true
  use_asg:
    default: true
  use_karpenter:
    default: false
  deploy_karpenter_in_k8s:
    default: false
    description: Allows you to enable the Karpenter Helm chart and associated resources
      without deploying the other parts of karpenter (i.e. the roles, permissions,
      and SQS queue)
  karpenter_version:
    default: v0.32.9
  deploy_cloud_trail:
    default: true
  send_logs_to_csoc:
    default: true
  route_table_name:
    default: eks_private
  eks_public_access:
    default: 'true'
  deploy_gen3:
    default: false
  ambassador_enabled:
    description: Enable ambassador
    type: bool
    default: true
  arborist_enabled:
    description: Enable arborist
    type: bool
    default: true
  argo_enabled:
    description: Enable argo
    type: bool
    default: true
  audit_enabled:
    description: Enable audit
    type: bool
    default: true
  aws-es-proxy_enabled:
    description: Enable aws-es-proxy
    type: bool
    default: true
  dbgap_enabled:
    description: Enable dbgap sync in the usersync job
    type: bool
    default: false
  dd_enabled:
    description: Enable datadog
    type: bool
    default: false
  dictionary_url:
    description: URL to the data dictionary
    default: ''
  dispatcher_job_number:
    description: Number of dispatcher jobs
    default: 10
  fence_enabled:
    description: Enable fence
    type: bool
    default: true
  guppy_enabled:
    description: Enable guppy
    type: bool
    default: true
  hatchery_enabled:
    description: Enable hatchery
    type: bool
    default: true
  indexd_enabled:
    description: Enable indexd
    type: bool
    default: true
  indexd_prefix:
    description: Indexd prefix
    default: dg.XXXX/
  ingress_enabled:
    description: Create ALB ingress
    type: bool
    default: true
  manifestservice_enabled:
    description: Enable manfiestservice
    type: bool
    default: true
  metadata_enabled:
    description: Enable metadata
    type: bool
    default: true
  netpolicy_enabled:
    description: Enable network policy security rules
    type: bool
    default: false
  peregrine_enabled:
    description: Enable perergrine
    type: bool
    default: true
  pidgin_enabled:
    description: Enable pidgin
    type: bool
    default: true
  portal_enabled:
    description: Enable portal
    type: bool
    default: true
  public_datasets:
    description: whether the datasets are public
    type: bool
    default: false
  requestor_enabled:
    description: Enable requestor
    type: bool
    default: true
  revproxy_arn:
    description: ARN for the revproxy cert in ACM
    default: ''
  revproxy_enabled:
    description: Enable revproxy
    type: bool
    default: true
  sheepdog_enabled:
    description: Enable sheepdog
    type: bool
    default: true
  slack_send_dbgap:
    description: Enable slack message for usersync job
    type: bool
    default: false
  ssjdispatcher_enabled:
    description: Enable ssjdispatcher
    type: bool
    default: true
  tier_access_level:
    description: Tier access level for guppy
    default: private
  tier_access_limit:
    description: value for tier access limit
    default: '100'
  usersync_enabled:
    description: Enable usersync cronjob
    type: bool
    default: true
  usersync_schedule:
    description: Cronjob schedule for usersync
    default: '*/30 * * * *'
  useryaml_s3_path:
    description: S3 path to the user.yaml file
    default: https://s3.amazonaws.com/dictionary-artifacts/datadictionary/develop/schema.json
  wts_enabled:
    description: Enable wts
    type: bool
    default: true
  fence_config_path:
    default: ''
  useryaml_path:
    default: ''
  gitops_path:
    default: https://github.com/uc-cdis/cdis-manifest.git
  google_client_id:
    default: ''
    sensitive: true
  google_client_secret:
    default: ''
    sensitive: true
  fence_access_key:
    default: ''
    sensitive: true
  fence_secret_key:
    default: ''
    sensitive: true
  upload_bucket:
    default: ''
  namespace:
    default: default
  secrets_manager_enabled:
    default: false
  ci_run:
    default: false
  commons_log_retention:
    description: value in days for the cloudwatch log retention period
    default: '3653'
  enable_vpc_endpoints:
    default: true
  deploy_es_role:
    default: false
  deploy_waf:
    default: false
  k8s_bootstrap_resources:
    default: true
    description: If set to true, creates resources for bootstrapping a kubernetes
      cluster (such as karpenter configs and helm releases)
  base_rules:
    description: Base AWS Managed Rules
    type: 'list(object("managed_rule_group_name": "string", "priority": "number",
      "override_to_count": "list(string)"))'
    default:
      managed_rule_group_name:
      - AWSManagedRulesAmazonIpReputationList
      - AWSManagedRulesPHPRuleSet
      - AWSManagedRulesWordPressRuleSet
      priority:
      - 0
      - 1
      - 2
      override_to_count:
      - AWSManagedReconnaissanceList
      - PHPHighRiskMethodsVariables_HEADER PHPHighRiskMethodsVariables_QUERYSTRING
        PHPHighRiskMethodsVariables_BODY
      - WordPressExploitableCommands_QUERYSTRING WordPressExploitablePaths_URIPATH
  additional_rules:
    description: Additional AWS Managed Rules
    type: 'list(object("managed_rule_group_name": "string", "priority": "number",
      "override_to_count": "list(string)"))'
    default: []
